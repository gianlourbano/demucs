# PyTorch ONNX Conversion Report

```
❌ Obtain model graph with `torch.export.export`
✅ Obtain model graph with `torch.export.export(..., strict=False)`
⚪ Obtain model graph with `torch.jit.trace`
✅ Translate the graph into ONNX
✅ Run `onnx.checker` on the ONNX model
✅ Execute the model with ONNX Runtime
❌ Validate model output accuracy
```

## Error messages

```pytb
# ⚠️ Errors from strategy 'TorchExportStrategy': -----------------------

Traceback (most recent call last):

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_capture_strategies.py", line 90, in __call__
    exported_program = self._capture(model, args, kwargs, dynamic_shapes)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/onnx/_internal/exporter/_capture_strategies.py", line 123, in _capture
    return torch.export.export(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/__init__.py", line 172, in export
    return _export(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/_trace.py", line 1013, in wrapper
    raise e

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/_trace.py", line 986, in wrapper
    ep = fn(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/exported_program.py", line 97, in wrapper
    return fn(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/_trace.py", line 1921, in _export
    export_artifact = export_func(  # type: ignore[operator]

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/_trace.py", line 1220, in _strict_export
    return _strict_export_lower_to_aten_ir(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/_trace.py", line 1248, in _strict_export_lower_to_aten_ir
    gm_torch_level = _export_to_torch_ir(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/export/_trace.py", line 556, in _export_to_torch_ir
    gm_torch_level, _ = torch._dynamo.export(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 1432, in inner
    result_traced = opt_f(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 465, in _fn
    return fn(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 1243, in __call__
    return self._torchdynamo_orig_callable(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 516, in __call__
    return _compile(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 907, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 655, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_utils_internal.py", line 87, in wrapper_function
    return function(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 688, in _compile_inner
    out_code = transform_code_object(code, transform)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py", line 1322, in transform_code_object
    transformations(instructions, code_options)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 210, in _fn
    return fn(*args, **kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py", line 624, in transform
    tracer.run()

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 2797, in run
    super().run()

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 582, in wrapper
    return inner_fn(self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1603, in CALL_FUNCTION
    self.call_function(fn, args, {})

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 830, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/nn_module.py", line 442, in call_function
    return tx.inline_user_function_return(

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 836, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3012, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3140, in inline_call_
    tracer.run()

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 582, in wrapper
    return inner_fn(self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1681, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 830, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 383, in call_function
    return super().call_function(tx, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 322, in call_function
    return super().call_function(tx, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 106, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 836, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3012, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3140, in inline_call_
    tracer.run()

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 582, in wrapper
    return inner_fn(self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1603, in CALL_FUNCTION
    self.call_function(fn, args, {})

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 830, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 383, in call_function
    return super().call_function(tx, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 322, in call_function
    return super().call_function(tx, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/functions.py", line 106, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 836, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3012, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 3140, in inline_call_
    tracer.run()

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 983, in run
    while self.step():

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 895, in step
    self.dispatch_table[inst.opcode](self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 582, in wrapper
    return inner_fn(self, inst)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 1603, in CALL_FUNCTION
    self.call_function(fn, args, {})

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py", line 830, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/user_defined.py", line 818, in call_function
    return call_random_fn(tx, self.value, args, kwargs)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/user_defined.py", line 572, in call_random_fn
    return VariableBuilder(tx, source).wrap_unspecialized_primitive(example_value)

  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/_dynamo/variables/builder.py", line 1892, in wrap_unspecialized_primitive
    raise AssertionError(

AssertionError: Dynamo attempts to add additional input during export: value=0, source=RandomValueSource(random_call_index=0)

from user code:
   File "/home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py", line 484, in forward
    x, xt = self.crosstransformer(x, xt)
  File "/home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py", line 660, in forward
    pos_emb = self._get_pos_embedding(T2, B, C, x.device)
  File "/home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py", line 680, in _get_pos_embedding
    shift = random.randrange(self.sin_random_shift + 1)

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information



```

## Exported program

```python
ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, p_encoder_0_conv_weight: "f32[48, 4, 8, 1]", p_encoder_0_conv_bias: "f32[48]", p_encoder_0_rewrite_weight: "f32[96, 48, 1, 1]", p_encoder_0_rewrite_bias: "f32[96]", p_encoder_0_dconv_layers_0_0_weight: "f32[6, 48, 3]", p_encoder_0_dconv_layers_0_0_bias: "f32[6]", p_encoder_0_dconv_layers_0_1_weight: "f32[6]", p_encoder_0_dconv_layers_0_1_bias: "f32[6]", p_encoder_0_dconv_layers_0_3_weight: "f32[96, 6, 1]", p_encoder_0_dconv_layers_0_3_bias: "f32[96]", p_encoder_0_dconv_layers_0_4_weight: "f32[96]", p_encoder_0_dconv_layers_0_4_bias: "f32[96]", p_encoder_0_dconv_layers_0_6_scale: "f32[48]", p_encoder_0_dconv_layers_1_0_weight: "f32[6, 48, 3]", p_encoder_0_dconv_layers_1_0_bias: "f32[6]", p_encoder_0_dconv_layers_1_1_weight: "f32[6]", p_encoder_0_dconv_layers_1_1_bias: "f32[6]", p_encoder_0_dconv_layers_1_3_weight: "f32[96, 6, 1]", p_encoder_0_dconv_layers_1_3_bias: "f32[96]", p_encoder_0_dconv_layers_1_4_weight: "f32[96]", p_encoder_0_dconv_layers_1_4_bias: "f32[96]", p_encoder_0_dconv_layers_1_6_scale: "f32[48]", p_encoder_1_conv_weight: "f32[96, 48, 8, 1]", p_encoder_1_conv_bias: "f32[96]", p_encoder_1_rewrite_weight: "f32[192, 96, 1, 1]", p_encoder_1_rewrite_bias: "f32[192]", p_encoder_1_dconv_layers_0_0_weight: "f32[12, 96, 3]", p_encoder_1_dconv_layers_0_0_bias: "f32[12]", p_encoder_1_dconv_layers_0_1_weight: "f32[12]", p_encoder_1_dconv_layers_0_1_bias: "f32[12]", p_encoder_1_dconv_layers_0_3_weight: "f32[192, 12, 1]", p_encoder_1_dconv_layers_0_3_bias: "f32[192]", p_encoder_1_dconv_layers_0_4_weight: "f32[192]", p_encoder_1_dconv_layers_0_4_bias: "f32[192]", p_encoder_1_dconv_layers_0_6_scale: "f32[96]", p_encoder_1_dconv_layers_1_0_weight: "f32[12, 96, 3]", p_encoder_1_dconv_layers_1_0_bias: "f32[12]", p_encoder_1_dconv_layers_1_1_weight: "f32[12]", p_encoder_1_dconv_layers_1_1_bias: "f32[12]", p_encoder_1_dconv_layers_1_3_weight: "f32[192, 12, 1]", p_encoder_1_dconv_layers_1_3_bias: "f32[192]", p_encoder_1_dconv_layers_1_4_weight: "f32[192]", p_encoder_1_dconv_layers_1_4_bias: "f32[192]", p_encoder_1_dconv_layers_1_6_scale: "f32[96]", p_encoder_2_conv_weight: "f32[192, 96, 8, 1]", p_encoder_2_conv_bias: "f32[192]", p_encoder_2_rewrite_weight: "f32[384, 192, 1, 1]", p_encoder_2_rewrite_bias: "f32[384]", p_encoder_2_dconv_layers_0_0_weight: "f32[24, 192, 3]", p_encoder_2_dconv_layers_0_0_bias: "f32[24]", p_encoder_2_dconv_layers_0_1_weight: "f32[24]", p_encoder_2_dconv_layers_0_1_bias: "f32[24]", p_encoder_2_dconv_layers_0_3_weight: "f32[384, 24, 1]", p_encoder_2_dconv_layers_0_3_bias: "f32[384]", p_encoder_2_dconv_layers_0_4_weight: "f32[384]", p_encoder_2_dconv_layers_0_4_bias: "f32[384]", p_encoder_2_dconv_layers_0_6_scale: "f32[192]", p_encoder_2_dconv_layers_1_0_weight: "f32[24, 192, 3]", p_encoder_2_dconv_layers_1_0_bias: "f32[24]", p_encoder_2_dconv_layers_1_1_weight: "f32[24]", p_encoder_2_dconv_layers_1_1_bias: "f32[24]", p_encoder_2_dconv_layers_1_3_weight: "f32[384, 24, 1]", p_encoder_2_dconv_layers_1_3_bias: "f32[384]", p_encoder_2_dconv_layers_1_4_weight: "f32[384]", p_encoder_2_dconv_layers_1_4_bias: "f32[384]", p_encoder_2_dconv_layers_1_6_scale: "f32[192]", p_encoder_3_conv_weight: "f32[384, 192, 8, 1]", p_encoder_3_conv_bias: "f32[384]", p_encoder_3_rewrite_weight: "f32[768, 384, 1, 1]", p_encoder_3_rewrite_bias: "f32[768]", p_encoder_3_dconv_layers_0_0_weight: "f32[48, 384, 3]", p_encoder_3_dconv_layers_0_0_bias: "f32[48]", p_encoder_3_dconv_layers_0_1_weight: "f32[48]", p_encoder_3_dconv_layers_0_1_bias: "f32[48]", p_encoder_3_dconv_layers_0_3_weight: "f32[768, 48, 1]", p_encoder_3_dconv_layers_0_3_bias: "f32[768]", p_encoder_3_dconv_layers_0_4_weight: "f32[768]", p_encoder_3_dconv_layers_0_4_bias: "f32[768]", p_encoder_3_dconv_layers_0_6_scale: "f32[384]", p_encoder_3_dconv_layers_1_0_weight: "f32[48, 384, 3]", p_encoder_3_dconv_layers_1_0_bias: "f32[48]", p_encoder_3_dconv_layers_1_1_weight: "f32[48]", p_encoder_3_dconv_layers_1_1_bias: "f32[48]", p_encoder_3_dconv_layers_1_3_weight: "f32[768, 48, 1]", p_encoder_3_dconv_layers_1_3_bias: "f32[768]", p_encoder_3_dconv_layers_1_4_weight: "f32[768]", p_encoder_3_dconv_layers_1_4_bias: "f32[768]", p_encoder_3_dconv_layers_1_6_scale: "f32[384]", p_decoder_0_conv_tr_weight: "f32[384, 192, 8, 1]", p_decoder_0_conv_tr_bias: "f32[192]", p_decoder_0_rewrite_weight: "f32[768, 384, 3, 3]", p_decoder_0_rewrite_bias: "f32[768]", p_decoder_0_dconv_layers_0_0_weight: "f32[48, 384, 3]", p_decoder_0_dconv_layers_0_0_bias: "f32[48]", p_decoder_0_dconv_layers_0_1_weight: "f32[48]", p_decoder_0_dconv_layers_0_1_bias: "f32[48]", p_decoder_0_dconv_layers_0_3_weight: "f32[768, 48, 1]", p_decoder_0_dconv_layers_0_3_bias: "f32[768]", p_decoder_0_dconv_layers_0_4_weight: "f32[768]", p_decoder_0_dconv_layers_0_4_bias: "f32[768]", p_decoder_0_dconv_layers_0_6_scale: "f32[384]", p_decoder_0_dconv_layers_1_0_weight: "f32[48, 384, 3]", p_decoder_0_dconv_layers_1_0_bias: "f32[48]", p_decoder_0_dconv_layers_1_1_weight: "f32[48]", p_decoder_0_dconv_layers_1_1_bias: "f32[48]", p_decoder_0_dconv_layers_1_3_weight: "f32[768, 48, 1]", p_decoder_0_dconv_layers_1_3_bias: "f32[768]", p_decoder_0_dconv_layers_1_4_weight: "f32[768]", p_decoder_0_dconv_layers_1_4_bias: "f32[768]", p_decoder_0_dconv_layers_1_6_scale: "f32[384]", p_decoder_1_conv_tr_weight: "f32[192, 96, 8, 1]", p_decoder_1_conv_tr_bias: "f32[96]", p_decoder_1_rewrite_weight: "f32[384, 192, 3, 3]", p_decoder_1_rewrite_bias: "f32[384]", p_decoder_1_dconv_layers_0_0_weight: "f32[24, 192, 3]", p_decoder_1_dconv_layers_0_0_bias: "f32[24]", p_decoder_1_dconv_layers_0_1_weight: "f32[24]", p_decoder_1_dconv_layers_0_1_bias: "f32[24]", p_decoder_1_dconv_layers_0_3_weight: "f32[384, 24, 1]", p_decoder_1_dconv_layers_0_3_bias: "f32[384]", p_decoder_1_dconv_layers_0_4_weight: "f32[384]", p_decoder_1_dconv_layers_0_4_bias: "f32[384]", p_decoder_1_dconv_layers_0_6_scale: "f32[192]", p_decoder_1_dconv_layers_1_0_weight: "f32[24, 192, 3]", p_decoder_1_dconv_layers_1_0_bias: "f32[24]", p_decoder_1_dconv_layers_1_1_weight: "f32[24]", p_decoder_1_dconv_layers_1_1_bias: "f32[24]", p_decoder_1_dconv_layers_1_3_weight: "f32[384, 24, 1]", p_decoder_1_dconv_layers_1_3_bias: "f32[384]", p_decoder_1_dconv_layers_1_4_weight: "f32[384]", p_decoder_1_dconv_layers_1_4_bias: "f32[384]", p_decoder_1_dconv_layers_1_6_scale: "f32[192]", p_decoder_2_conv_tr_weight: "f32[96, 48, 8, 1]", p_decoder_2_conv_tr_bias: "f32[48]", p_decoder_2_rewrite_weight: "f32[192, 96, 3, 3]", p_decoder_2_rewrite_bias: "f32[192]", p_decoder_2_dconv_layers_0_0_weight: "f32[12, 96, 3]", p_decoder_2_dconv_layers_0_0_bias: "f32[12]", p_decoder_2_dconv_layers_0_1_weight: "f32[12]", p_decoder_2_dconv_layers_0_1_bias: "f32[12]", p_decoder_2_dconv_layers_0_3_weight: "f32[192, 12, 1]", p_decoder_2_dconv_layers_0_3_bias: "f32[192]", p_decoder_2_dconv_layers_0_4_weight: "f32[192]", p_decoder_2_dconv_layers_0_4_bias: "f32[192]", p_decoder_2_dconv_layers_0_6_scale: "f32[96]", p_decoder_2_dconv_layers_1_0_weight: "f32[12, 96, 3]", p_decoder_2_dconv_layers_1_0_bias: "f32[12]", p_decoder_2_dconv_layers_1_1_weight: "f32[12]", p_decoder_2_dconv_layers_1_1_bias: "f32[12]", p_decoder_2_dconv_layers_1_3_weight: "f32[192, 12, 1]", p_decoder_2_dconv_layers_1_3_bias: "f32[192]", p_decoder_2_dconv_layers_1_4_weight: "f32[192]", p_decoder_2_dconv_layers_1_4_bias: "f32[192]", p_decoder_2_dconv_layers_1_6_scale: "f32[96]", p_decoder_3_conv_tr_weight: "f32[48, 16, 8, 1]", p_decoder_3_conv_tr_bias: "f32[16]", p_decoder_3_rewrite_weight: "f32[96, 48, 3, 3]", p_decoder_3_rewrite_bias: "f32[96]", p_decoder_3_dconv_layers_0_0_weight: "f32[6, 48, 3]", p_decoder_3_dconv_layers_0_0_bias: "f32[6]", p_decoder_3_dconv_layers_0_1_weight: "f32[6]", p_decoder_3_dconv_layers_0_1_bias: "f32[6]", p_decoder_3_dconv_layers_0_3_weight: "f32[96, 6, 1]", p_decoder_3_dconv_layers_0_3_bias: "f32[96]", p_decoder_3_dconv_layers_0_4_weight: "f32[96]", p_decoder_3_dconv_layers_0_4_bias: "f32[96]", p_decoder_3_dconv_layers_0_6_scale: "f32[48]", p_decoder_3_dconv_layers_1_0_weight: "f32[6, 48, 3]", p_decoder_3_dconv_layers_1_0_bias: "f32[6]", p_decoder_3_dconv_layers_1_1_weight: "f32[6]", p_decoder_3_dconv_layers_1_1_bias: "f32[6]", p_decoder_3_dconv_layers_1_3_weight: "f32[96, 6, 1]", p_decoder_3_dconv_layers_1_3_bias: "f32[96]", p_decoder_3_dconv_layers_1_4_weight: "f32[96]", p_decoder_3_dconv_layers_1_4_bias: "f32[96]", p_decoder_3_dconv_layers_1_6_scale: "f32[48]", p_tencoder_0_conv_weight: "f32[48, 2, 8]", p_tencoder_0_conv_bias: "f32[48]", p_tencoder_0_rewrite_weight: "f32[96, 48, 1]", p_tencoder_0_rewrite_bias: "f32[96]", p_tencoder_0_dconv_layers_0_0_weight: "f32[6, 48, 3]", p_tencoder_0_dconv_layers_0_0_bias: "f32[6]", p_tencoder_0_dconv_layers_0_1_weight: "f32[6]", p_tencoder_0_dconv_layers_0_1_bias: "f32[6]", p_tencoder_0_dconv_layers_0_3_weight: "f32[96, 6, 1]", p_tencoder_0_dconv_layers_0_3_bias: "f32[96]", p_tencoder_0_dconv_layers_0_4_weight: "f32[96]", p_tencoder_0_dconv_layers_0_4_bias: "f32[96]", p_tencoder_0_dconv_layers_0_6_scale: "f32[48]", p_tencoder_0_dconv_layers_1_0_weight: "f32[6, 48, 3]", p_tencoder_0_dconv_layers_1_0_bias: "f32[6]", p_tencoder_0_dconv_layers_1_1_weight: "f32[6]", p_tencoder_0_dconv_layers_1_1_bias: "f32[6]", p_tencoder_0_dconv_layers_1_3_weight: "f32[96, 6, 1]", p_tencoder_0_dconv_layers_1_3_bias: "f32[96]", p_tencoder_0_dconv_layers_1_4_weight: "f32[96]", p_tencoder_0_dconv_layers_1_4_bias: "f32[96]", p_tencoder_0_dconv_layers_1_6_scale: "f32[48]", p_tencoder_1_conv_weight: "f32[96, 48, 8]", p_tencoder_1_conv_bias: "f32[96]", p_tencoder_1_rewrite_weight: "f32[192, 96, 1]", p_tencoder_1_rewrite_bias: "f32[192]", p_tencoder_1_dconv_layers_0_0_weight: "f32[12, 96, 3]", p_tencoder_1_dconv_layers_0_0_bias: "f32[12]", p_tencoder_1_dconv_layers_0_1_weight: "f32[12]", p_tencoder_1_dconv_layers_0_1_bias: "f32[12]", p_tencoder_1_dconv_layers_0_3_weight: "f32[192, 12, 1]", p_tencoder_1_dconv_layers_0_3_bias: "f32[192]", p_tencoder_1_dconv_layers_0_4_weight: "f32[192]", p_tencoder_1_dconv_layers_0_4_bias: "f32[192]", p_tencoder_1_dconv_layers_0_6_scale: "f32[96]", p_tencoder_1_dconv_layers_1_0_weight: "f32[12, 96, 3]", p_tencoder_1_dconv_layers_1_0_bias: "f32[12]", p_tencoder_1_dconv_layers_1_1_weight: "f32[12]", p_tencoder_1_dconv_layers_1_1_bias: "f32[12]", p_tencoder_1_dconv_layers_1_3_weight: "f32[192, 12, 1]", p_tencoder_1_dconv_layers_1_3_bias: "f32[192]", p_tencoder_1_dconv_layers_1_4_weight: "f32[192]", p_tencoder_1_dconv_layers_1_4_bias: "f32[192]", p_tencoder_1_dconv_layers_1_6_scale: "f32[96]", p_tencoder_2_conv_weight: "f32[192, 96, 8]", p_tencoder_2_conv_bias: "f32[192]", p_tencoder_2_rewrite_weight: "f32[384, 192, 1]", p_tencoder_2_rewrite_bias: "f32[384]", p_tencoder_2_dconv_layers_0_0_weight: "f32[24, 192, 3]", p_tencoder_2_dconv_layers_0_0_bias: "f32[24]", p_tencoder_2_dconv_layers_0_1_weight: "f32[24]", p_tencoder_2_dconv_layers_0_1_bias: "f32[24]", p_tencoder_2_dconv_layers_0_3_weight: "f32[384, 24, 1]", p_tencoder_2_dconv_layers_0_3_bias: "f32[384]", p_tencoder_2_dconv_layers_0_4_weight: "f32[384]", p_tencoder_2_dconv_layers_0_4_bias: "f32[384]", p_tencoder_2_dconv_layers_0_6_scale: "f32[192]", p_tencoder_2_dconv_layers_1_0_weight: "f32[24, 192, 3]", p_tencoder_2_dconv_layers_1_0_bias: "f32[24]", p_tencoder_2_dconv_layers_1_1_weight: "f32[24]", p_tencoder_2_dconv_layers_1_1_bias: "f32[24]", p_tencoder_2_dconv_layers_1_3_weight: "f32[384, 24, 1]", p_tencoder_2_dconv_layers_1_3_bias: "f32[384]", p_tencoder_2_dconv_layers_1_4_weight: "f32[384]", p_tencoder_2_dconv_layers_1_4_bias: "f32[384]", p_tencoder_2_dconv_layers_1_6_scale: "f32[192]", p_tencoder_3_conv_weight: "f32[384, 192, 8]", p_tencoder_3_conv_bias: "f32[384]", p_tencoder_3_rewrite_weight: "f32[768, 384, 1]", p_tencoder_3_rewrite_bias: "f32[768]", p_tencoder_3_dconv_layers_0_0_weight: "f32[48, 384, 3]", p_tencoder_3_dconv_layers_0_0_bias: "f32[48]", p_tencoder_3_dconv_layers_0_1_weight: "f32[48]", p_tencoder_3_dconv_layers_0_1_bias: "f32[48]", p_tencoder_3_dconv_layers_0_3_weight: "f32[768, 48, 1]", p_tencoder_3_dconv_layers_0_3_bias: "f32[768]", p_tencoder_3_dconv_layers_0_4_weight: "f32[768]", p_tencoder_3_dconv_layers_0_4_bias: "f32[768]", p_tencoder_3_dconv_layers_0_6_scale: "f32[384]", p_tencoder_3_dconv_layers_1_0_weight: "f32[48, 384, 3]", p_tencoder_3_dconv_layers_1_0_bias: "f32[48]", p_tencoder_3_dconv_layers_1_1_weight: "f32[48]", p_tencoder_3_dconv_layers_1_1_bias: "f32[48]", p_tencoder_3_dconv_layers_1_3_weight: "f32[768, 48, 1]", p_tencoder_3_dconv_layers_1_3_bias: "f32[768]", p_tencoder_3_dconv_layers_1_4_weight: "f32[768]", p_tencoder_3_dconv_layers_1_4_bias: "f32[768]", p_tencoder_3_dconv_layers_1_6_scale: "f32[384]", p_tdecoder_0_conv_tr_weight: "f32[384, 192, 8]", p_tdecoder_0_conv_tr_bias: "f32[192]", p_tdecoder_0_rewrite_weight: "f32[768, 384, 3]", p_tdecoder_0_rewrite_bias: "f32[768]", p_tdecoder_0_dconv_layers_0_0_weight: "f32[48, 384, 3]", p_tdecoder_0_dconv_layers_0_0_bias: "f32[48]", p_tdecoder_0_dconv_layers_0_1_weight: "f32[48]", p_tdecoder_0_dconv_layers_0_1_bias: "f32[48]", p_tdecoder_0_dconv_layers_0_3_weight: "f32[768, 48, 1]", p_tdecoder_0_dconv_layers_0_3_bias: "f32[768]", p_tdecoder_0_dconv_layers_0_4_weight: "f32[768]", p_tdecoder_0_dconv_layers_0_4_bias: "f32[768]", p_tdecoder_0_dconv_layers_0_6_scale: "f32[384]", p_tdecoder_0_dconv_layers_1_0_weight: "f32[48, 384, 3]", p_tdecoder_0_dconv_layers_1_0_bias: "f32[48]", p_tdecoder_0_dconv_layers_1_1_weight: "f32[48]", p_tdecoder_0_dconv_layers_1_1_bias: "f32[48]", p_tdecoder_0_dconv_layers_1_3_weight: "f32[768, 48, 1]", p_tdecoder_0_dconv_layers_1_3_bias: "f32[768]", p_tdecoder_0_dconv_layers_1_4_weight: "f32[768]", p_tdecoder_0_dconv_layers_1_4_bias: "f32[768]", p_tdecoder_0_dconv_layers_1_6_scale: "f32[384]", p_tdecoder_1_conv_tr_weight: "f32[192, 96, 8]", p_tdecoder_1_conv_tr_bias: "f32[96]", p_tdecoder_1_rewrite_weight: "f32[384, 192, 3]", p_tdecoder_1_rewrite_bias: "f32[384]", p_tdecoder_1_dconv_layers_0_0_weight: "f32[24, 192, 3]", p_tdecoder_1_dconv_layers_0_0_bias: "f32[24]", p_tdecoder_1_dconv_layers_0_1_weight: "f32[24]", p_tdecoder_1_dconv_layers_0_1_bias: "f32[24]", p_tdecoder_1_dconv_layers_0_3_weight: "f32[384, 24, 1]", p_tdecoder_1_dconv_layers_0_3_bias: "f32[384]", p_tdecoder_1_dconv_layers_0_4_weight: "f32[384]", p_tdecoder_1_dconv_layers_0_4_bias: "f32[384]", p_tdecoder_1_dconv_layers_0_6_scale: "f32[192]", p_tdecoder_1_dconv_layers_1_0_weight: "f32[24, 192, 3]", p_tdecoder_1_dconv_layers_1_0_bias: "f32[24]", p_tdecoder_1_dconv_layers_1_1_weight: "f32[24]", p_tdecoder_1_dconv_layers_1_1_bias: "f32[24]", p_tdecoder_1_dconv_layers_1_3_weight: "f32[384, 24, 1]", p_tdecoder_1_dconv_layers_1_3_bias: "f32[384]", p_tdecoder_1_dconv_layers_1_4_weight: "f32[384]", p_tdecoder_1_dconv_layers_1_4_bias: "f32[384]", p_tdecoder_1_dconv_layers_1_6_scale: "f32[192]", p_tdecoder_2_conv_tr_weight: "f32[96, 48, 8]", p_tdecoder_2_conv_tr_bias: "f32[48]", p_tdecoder_2_rewrite_weight: "f32[192, 96, 3]", p_tdecoder_2_rewrite_bias: "f32[192]", p_tdecoder_2_dconv_layers_0_0_weight: "f32[12, 96, 3]", p_tdecoder_2_dconv_layers_0_0_bias: "f32[12]", p_tdecoder_2_dconv_layers_0_1_weight: "f32[12]", p_tdecoder_2_dconv_layers_0_1_bias: "f32[12]", p_tdecoder_2_dconv_layers_0_3_weight: "f32[192, 12, 1]", p_tdecoder_2_dconv_layers_0_3_bias: "f32[192]", p_tdecoder_2_dconv_layers_0_4_weight: "f32[192]", p_tdecoder_2_dconv_layers_0_4_bias: "f32[192]", p_tdecoder_2_dconv_layers_0_6_scale: "f32[96]", p_tdecoder_2_dconv_layers_1_0_weight: "f32[12, 96, 3]", p_tdecoder_2_dconv_layers_1_0_bias: "f32[12]", p_tdecoder_2_dconv_layers_1_1_weight: "f32[12]", p_tdecoder_2_dconv_layers_1_1_bias: "f32[12]", p_tdecoder_2_dconv_layers_1_3_weight: "f32[192, 12, 1]", p_tdecoder_2_dconv_layers_1_3_bias: "f32[192]", p_tdecoder_2_dconv_layers_1_4_weight: "f32[192]", p_tdecoder_2_dconv_layers_1_4_bias: "f32[192]", p_tdecoder_2_dconv_layers_1_6_scale: "f32[96]", p_tdecoder_3_conv_tr_weight: "f32[48, 8, 8]", p_tdecoder_3_conv_tr_bias: "f32[8]", p_tdecoder_3_rewrite_weight: "f32[96, 48, 3]", p_tdecoder_3_rewrite_bias: "f32[96]", p_tdecoder_3_dconv_layers_0_0_weight: "f32[6, 48, 3]", p_tdecoder_3_dconv_layers_0_0_bias: "f32[6]", p_tdecoder_3_dconv_layers_0_1_weight: "f32[6]", p_tdecoder_3_dconv_layers_0_1_bias: "f32[6]", p_tdecoder_3_dconv_layers_0_3_weight: "f32[96, 6, 1]", p_tdecoder_3_dconv_layers_0_3_bias: "f32[96]", p_tdecoder_3_dconv_layers_0_4_weight: "f32[96]", p_tdecoder_3_dconv_layers_0_4_bias: "f32[96]", p_tdecoder_3_dconv_layers_0_6_scale: "f32[48]", p_tdecoder_3_dconv_layers_1_0_weight: "f32[6, 48, 3]", p_tdecoder_3_dconv_layers_1_0_bias: "f32[6]", p_tdecoder_3_dconv_layers_1_1_weight: "f32[6]", p_tdecoder_3_dconv_layers_1_1_bias: "f32[6]", p_tdecoder_3_dconv_layers_1_3_weight: "f32[96, 6, 1]", p_tdecoder_3_dconv_layers_1_3_bias: "f32[96]", p_tdecoder_3_dconv_layers_1_4_weight: "f32[96]", p_tdecoder_3_dconv_layers_1_4_bias: "f32[96]", p_tdecoder_3_dconv_layers_1_6_scale: "f32[48]", p_freq_emb_embedding_weight: "f32[512, 48]", p_channel_upsampler_weight: "f32[512, 384, 1]", p_channel_upsampler_bias: "f32[512]", p_channel_downsampler_weight: "f32[384, 512, 1]", p_channel_downsampler_bias: "f32[384]", p_channel_upsampler_t_weight: "f32[512, 384, 1]", p_channel_upsampler_t_bias: "f32[512]", p_channel_downsampler_t_weight: "f32[384, 512, 1]", p_channel_downsampler_t_bias: "f32[384]", p_crosstransformer_norm_in_weight: "f32[512]", p_crosstransformer_norm_in_bias: "f32[512]", p_crosstransformer_norm_in_t_weight: "f32[512]", p_crosstransformer_norm_in_t_bias: "f32[512]", p_crosstransformer_layers_0_self_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_0_self_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_0_self_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_0_self_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_0_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_0_linear1_bias: "f32[2048]", p_crosstransformer_layers_0_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_0_linear2_bias: "f32[512]", p_crosstransformer_layers_0_norm1_weight: "f32[512]", p_crosstransformer_layers_0_norm1_bias: "f32[512]", p_crosstransformer_layers_0_norm2_weight: "f32[512]", p_crosstransformer_layers_0_norm2_bias: "f32[512]", p_crosstransformer_layers_0_norm_out_weight: "f32[512]", p_crosstransformer_layers_0_norm_out_bias: "f32[512]", p_crosstransformer_layers_0_gamma_1_scale: "f32[512]", p_crosstransformer_layers_0_gamma_2_scale: "f32[512]", p_crosstransformer_layers_1_cross_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_1_cross_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_1_cross_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_1_cross_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_1_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_1_linear1_bias: "f32[2048]", p_crosstransformer_layers_1_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_1_linear2_bias: "f32[512]", p_crosstransformer_layers_1_norm1_weight: "f32[512]", p_crosstransformer_layers_1_norm1_bias: "f32[512]", p_crosstransformer_layers_1_norm2_weight: "f32[512]", p_crosstransformer_layers_1_norm2_bias: "f32[512]", p_crosstransformer_layers_1_norm3_weight: "f32[512]", p_crosstransformer_layers_1_norm3_bias: "f32[512]", p_crosstransformer_layers_1_norm_out_weight: "f32[512]", p_crosstransformer_layers_1_norm_out_bias: "f32[512]", p_crosstransformer_layers_1_gamma_1_scale: "f32[512]", p_crosstransformer_layers_1_gamma_2_scale: "f32[512]", p_crosstransformer_layers_2_self_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_2_self_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_2_self_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_2_self_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_2_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_2_linear1_bias: "f32[2048]", p_crosstransformer_layers_2_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_2_linear2_bias: "f32[512]", p_crosstransformer_layers_2_norm1_weight: "f32[512]", p_crosstransformer_layers_2_norm1_bias: "f32[512]", p_crosstransformer_layers_2_norm2_weight: "f32[512]", p_crosstransformer_layers_2_norm2_bias: "f32[512]", p_crosstransformer_layers_2_norm_out_weight: "f32[512]", p_crosstransformer_layers_2_norm_out_bias: "f32[512]", p_crosstransformer_layers_2_gamma_1_scale: "f32[512]", p_crosstransformer_layers_2_gamma_2_scale: "f32[512]", p_crosstransformer_layers_3_cross_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_3_cross_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_3_cross_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_3_cross_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_3_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_3_linear1_bias: "f32[2048]", p_crosstransformer_layers_3_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_3_linear2_bias: "f32[512]", p_crosstransformer_layers_3_norm1_weight: "f32[512]", p_crosstransformer_layers_3_norm1_bias: "f32[512]", p_crosstransformer_layers_3_norm2_weight: "f32[512]", p_crosstransformer_layers_3_norm2_bias: "f32[512]", p_crosstransformer_layers_3_norm3_weight: "f32[512]", p_crosstransformer_layers_3_norm3_bias: "f32[512]", p_crosstransformer_layers_3_norm_out_weight: "f32[512]", p_crosstransformer_layers_3_norm_out_bias: "f32[512]", p_crosstransformer_layers_3_gamma_1_scale: "f32[512]", p_crosstransformer_layers_3_gamma_2_scale: "f32[512]", p_crosstransformer_layers_4_self_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_4_self_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_4_self_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_4_self_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_4_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_4_linear1_bias: "f32[2048]", p_crosstransformer_layers_4_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_4_linear2_bias: "f32[512]", p_crosstransformer_layers_4_norm1_weight: "f32[512]", p_crosstransformer_layers_4_norm1_bias: "f32[512]", p_crosstransformer_layers_4_norm2_weight: "f32[512]", p_crosstransformer_layers_4_norm2_bias: "f32[512]", p_crosstransformer_layers_4_norm_out_weight: "f32[512]", p_crosstransformer_layers_4_norm_out_bias: "f32[512]", p_crosstransformer_layers_4_gamma_1_scale: "f32[512]", p_crosstransformer_layers_4_gamma_2_scale: "f32[512]", p_crosstransformer_layers_t_0_self_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_t_0_self_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_t_0_self_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_t_0_self_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_t_0_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_t_0_linear1_bias: "f32[2048]", p_crosstransformer_layers_t_0_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_t_0_linear2_bias: "f32[512]", p_crosstransformer_layers_t_0_norm1_weight: "f32[512]", p_crosstransformer_layers_t_0_norm1_bias: "f32[512]", p_crosstransformer_layers_t_0_norm2_weight: "f32[512]", p_crosstransformer_layers_t_0_norm2_bias: "f32[512]", p_crosstransformer_layers_t_0_norm_out_weight: "f32[512]", p_crosstransformer_layers_t_0_norm_out_bias: "f32[512]", p_crosstransformer_layers_t_0_gamma_1_scale: "f32[512]", p_crosstransformer_layers_t_0_gamma_2_scale: "f32[512]", p_crosstransformer_layers_t_1_cross_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_t_1_cross_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_t_1_cross_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_t_1_cross_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_t_1_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_t_1_linear1_bias: "f32[2048]", p_crosstransformer_layers_t_1_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_t_1_linear2_bias: "f32[512]", p_crosstransformer_layers_t_1_norm1_weight: "f32[512]", p_crosstransformer_layers_t_1_norm1_bias: "f32[512]", p_crosstransformer_layers_t_1_norm2_weight: "f32[512]", p_crosstransformer_layers_t_1_norm2_bias: "f32[512]", p_crosstransformer_layers_t_1_norm3_weight: "f32[512]", p_crosstransformer_layers_t_1_norm3_bias: "f32[512]", p_crosstransformer_layers_t_1_norm_out_weight: "f32[512]", p_crosstransformer_layers_t_1_norm_out_bias: "f32[512]", p_crosstransformer_layers_t_1_gamma_1_scale: "f32[512]", p_crosstransformer_layers_t_1_gamma_2_scale: "f32[512]", p_crosstransformer_layers_t_2_self_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_t_2_self_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_t_2_self_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_t_2_self_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_t_2_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_t_2_linear1_bias: "f32[2048]", p_crosstransformer_layers_t_2_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_t_2_linear2_bias: "f32[512]", p_crosstransformer_layers_t_2_norm1_weight: "f32[512]", p_crosstransformer_layers_t_2_norm1_bias: "f32[512]", p_crosstransformer_layers_t_2_norm2_weight: "f32[512]", p_crosstransformer_layers_t_2_norm2_bias: "f32[512]", p_crosstransformer_layers_t_2_norm_out_weight: "f32[512]", p_crosstransformer_layers_t_2_norm_out_bias: "f32[512]", p_crosstransformer_layers_t_2_gamma_1_scale: "f32[512]", p_crosstransformer_layers_t_2_gamma_2_scale: "f32[512]", p_crosstransformer_layers_t_3_cross_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_t_3_cross_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_t_3_cross_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_t_3_cross_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_t_3_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_t_3_linear1_bias: "f32[2048]", p_crosstransformer_layers_t_3_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_t_3_linear2_bias: "f32[512]", p_crosstransformer_layers_t_3_norm1_weight: "f32[512]", p_crosstransformer_layers_t_3_norm1_bias: "f32[512]", p_crosstransformer_layers_t_3_norm2_weight: "f32[512]", p_crosstransformer_layers_t_3_norm2_bias: "f32[512]", p_crosstransformer_layers_t_3_norm3_weight: "f32[512]", p_crosstransformer_layers_t_3_norm3_bias: "f32[512]", p_crosstransformer_layers_t_3_norm_out_weight: "f32[512]", p_crosstransformer_layers_t_3_norm_out_bias: "f32[512]", p_crosstransformer_layers_t_3_gamma_1_scale: "f32[512]", p_crosstransformer_layers_t_3_gamma_2_scale: "f32[512]", p_crosstransformer_layers_t_4_self_attn_in_proj_weight: "f32[1536, 512]", p_crosstransformer_layers_t_4_self_attn_in_proj_bias: "f32[1536]", p_crosstransformer_layers_t_4_self_attn_out_proj_weight: "f32[512, 512]", p_crosstransformer_layers_t_4_self_attn_out_proj_bias: "f32[512]", p_crosstransformer_layers_t_4_linear1_weight: "f32[2048, 512]", p_crosstransformer_layers_t_4_linear1_bias: "f32[2048]", p_crosstransformer_layers_t_4_linear2_weight: "f32[512, 2048]", p_crosstransformer_layers_t_4_linear2_bias: "f32[512]", p_crosstransformer_layers_t_4_norm1_weight: "f32[512]", p_crosstransformer_layers_t_4_norm1_bias: "f32[512]", p_crosstransformer_layers_t_4_norm2_weight: "f32[512]", p_crosstransformer_layers_t_4_norm2_bias: "f32[512]", p_crosstransformer_layers_t_4_norm_out_weight: "f32[512]", p_crosstransformer_layers_t_4_norm_out_bias: "f32[512]", p_crosstransformer_layers_t_4_gamma_1_scale: "f32[512]", p_crosstransformer_layers_t_4_gamma_2_scale: "f32[512]", mix: "f32[1, 2, 441000]", spec: "c64[1, 2, 2048, 431]"):
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:430 in forward, code: mag = self._magnitude(z)
            view_as_real: "f32[1, 2, 2048, 431, 2]" = torch.ops.aten.view_as_real.default(spec);  spec = None
            permute: "f32[1, 2, 2, 2048, 431]" = torch.ops.aten.permute.default(view_as_real, [0, 1, 4, 2, 3]);  view_as_real = None
            clone: "f32[1, 2, 2, 2048, 431]" = torch.ops.aten.clone.default(permute, memory_format = torch.contiguous_format);  permute = None
            _unsafe_view: "f32[1, 4, 2048, 431]" = torch.ops.aten._unsafe_view.default(clone, [1, 4, 2048, 431]);  clone = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:436 in forward, code: mean = x.mean(dim=(1, 2, 3), keepdim=True)
            mean: "f32[1, 1, 1, 1]" = torch.ops.aten.mean.dim(_unsafe_view, [1, 2, 3], True)
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:437 in forward, code: std = x.std(dim=(1, 2, 3), keepdim=True)
            var: "f32[1, 1, 1, 1]" = torch.ops.aten.var.correction(_unsafe_view, [1, 2, 3], correction = 1.0, keepdim = True)
            sqrt: "f32[1, 1, 1, 1]" = torch.ops.aten.sqrt.default(var);  var = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:438 in forward, code: x = (x - mean) / (1e-5 + std)
            sub: "f32[1, 4, 2048, 431]" = torch.ops.aten.sub.Tensor(_unsafe_view, mean);  _unsafe_view = None
            add_10: "f32[1, 1, 1, 1]" = torch.ops.aten.add.Tensor(sqrt, 1e-05)
            div: "f32[1, 4, 2048, 431]" = torch.ops.aten.div.Tensor(sub, add_10);  sub = add_10 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:443 in forward, code: meant = xt.mean(dim=(1, 2), keepdim=True)
            mean_1: "f32[1, 1, 1]" = torch.ops.aten.mean.dim(mix, [1, 2], True)
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:444 in forward, code: stdt = xt.std(dim=(1, 2), keepdim=True)
            var_1: "f32[1, 1, 1]" = torch.ops.aten.var.correction(mix, [1, 2], correction = 1.0, keepdim = True)
            sqrt_1: "f32[1, 1, 1]" = torch.ops.aten.sqrt.default(var_1);  var_1 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:445 in forward, code: xt = (xt - meant) / (1e-5 + stdt)
            sub_1: "f32[1, 2, 441000]" = torch.ops.aten.sub.Tensor(mix, mean_1);  mix = None
            add_11: "f32[1, 1, 1]" = torch.ops.aten.add.Tensor(sqrt_1, 1e-05)
            div_1: "f32[1, 2, 441000]" = torch.ops.aten.div.Tensor(sub_1, add_11);  sub_1 = add_11 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution: "f32[1, 48, 110250]" = torch.ops.aten.convolution.default(div_1, p_tencoder_0_conv_weight, p_tencoder_0_conv_bias, [4], [2], [1], False, [0], 1);  div_1 = p_tencoder_0_conv_weight = p_tencoder_0_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu: "f32[1, 48, 110250]" = torch.ops.aten.gelu.default(convolution);  convolution = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_1: "f32[1, 6, 110250]" = torch.ops.aten.convolution.default(gelu, p_tencoder_0_dconv_layers_0_0_weight, p_tencoder_0_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tencoder_0_dconv_layers_0_0_weight = p_tencoder_0_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm: "f32[1, 6, 110250]" = torch.ops.aten.group_norm.default(convolution_1, 1, p_tencoder_0_dconv_layers_0_1_weight, p_tencoder_0_dconv_layers_0_1_bias);  convolution_1 = p_tencoder_0_dconv_layers_0_1_weight = p_tencoder_0_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_1: "f32[1, 6, 110250]" = torch.ops.aten.gelu.default(group_norm);  group_norm = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_2: "f32[1, 96, 110250]" = torch.ops.aten.convolution.default(gelu_1, p_tencoder_0_dconv_layers_0_3_weight, p_tencoder_0_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_1 = p_tencoder_0_dconv_layers_0_3_weight = p_tencoder_0_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_1: "f32[1, 96, 110250]" = torch.ops.aten.group_norm.default(convolution_2, 1, p_tencoder_0_dconv_layers_0_4_weight, p_tencoder_0_dconv_layers_0_4_bias);  convolution_2 = p_tencoder_0_dconv_layers_0_4_weight = p_tencoder_0_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu: "f32[1, 48, 110250]" = torch.ops.aten.glu.default(group_norm_1, 1);  group_norm_1 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_1: "f32[48]" = torch.ops.aten.slice.Tensor(p_tencoder_0_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tencoder_0_dconv_layers_0_6_scale = None
            unsqueeze: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
            mul_6: "f32[1, 48, 110250]" = torch.ops.aten.mul.Tensor(unsqueeze, glu);  unsqueeze = glu = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_12: "f32[1, 48, 110250]" = torch.ops.aten.add.Tensor(gelu, mul_6);  gelu = mul_6 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_3: "f32[1, 6, 110250]" = torch.ops.aten.convolution.default(add_12, p_tencoder_0_dconv_layers_1_0_weight, p_tencoder_0_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tencoder_0_dconv_layers_1_0_weight = p_tencoder_0_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_2: "f32[1, 6, 110250]" = torch.ops.aten.group_norm.default(convolution_3, 1, p_tencoder_0_dconv_layers_1_1_weight, p_tencoder_0_dconv_layers_1_1_bias);  convolution_3 = p_tencoder_0_dconv_layers_1_1_weight = p_tencoder_0_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_2: "f32[1, 6, 110250]" = torch.ops.aten.gelu.default(group_norm_2);  group_norm_2 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_4: "f32[1, 96, 110250]" = torch.ops.aten.convolution.default(gelu_2, p_tencoder_0_dconv_layers_1_3_weight, p_tencoder_0_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_2 = p_tencoder_0_dconv_layers_1_3_weight = p_tencoder_0_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_3: "f32[1, 96, 110250]" = torch.ops.aten.group_norm.default(convolution_4, 1, p_tencoder_0_dconv_layers_1_4_weight, p_tencoder_0_dconv_layers_1_4_bias);  convolution_4 = p_tencoder_0_dconv_layers_1_4_weight = p_tencoder_0_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_1: "f32[1, 48, 110250]" = torch.ops.aten.glu.default(group_norm_3, 1);  group_norm_3 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_2: "f32[48]" = torch.ops.aten.slice.Tensor(p_tencoder_0_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tencoder_0_dconv_layers_1_6_scale = None
            unsqueeze_1: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_2, 1);  slice_2 = None
            mul_7: "f32[1, 48, 110250]" = torch.ops.aten.mul.Tensor(unsqueeze_1, glu_1);  unsqueeze_1 = glu_1 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_13: "f32[1, 48, 110250]" = torch.ops.aten.add.Tensor(add_12, mul_7);  add_12 = mul_7 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_5: "f32[1, 96, 110250]" = torch.ops.aten.convolution.default(add_13, p_tencoder_0_rewrite_weight, p_tencoder_0_rewrite_bias, [1], [0], [1], False, [0], 1);  add_13 = p_tencoder_0_rewrite_weight = p_tencoder_0_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_2: "f32[1, 48, 110250]" = torch.ops.aten.glu.default(convolution_5, 1);  convolution_5 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_6: "f32[1, 48, 512, 431]" = torch.ops.aten.convolution.default(div, p_encoder_0_conv_weight, p_encoder_0_conv_bias, [4, 1], [2, 0], [1, 1], False, [0, 0], 1);  div = p_encoder_0_conv_weight = p_encoder_0_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_3: "f32[1, 48, 512, 431]" = torch.ops.aten.gelu.default(convolution_6);  convolution_6 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:148 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_1: "f32[1, 512, 48, 431]" = torch.ops.aten.permute.default(gelu_3, [0, 2, 1, 3]);  gelu_3 = None
            view: "f32[512, 48, 431]" = torch.ops.aten.view.default(permute_1, [512, 48, 431]);  permute_1 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_7: "f32[512, 6, 431]" = torch.ops.aten.convolution.default(view, p_encoder_0_dconv_layers_0_0_weight, p_encoder_0_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_encoder_0_dconv_layers_0_0_weight = p_encoder_0_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_4: "f32[512, 6, 431]" = torch.ops.aten.group_norm.default(convolution_7, 1, p_encoder_0_dconv_layers_0_1_weight, p_encoder_0_dconv_layers_0_1_bias);  convolution_7 = p_encoder_0_dconv_layers_0_1_weight = p_encoder_0_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_4: "f32[512, 6, 431]" = torch.ops.aten.gelu.default(group_norm_4);  group_norm_4 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_8: "f32[512, 96, 431]" = torch.ops.aten.convolution.default(gelu_4, p_encoder_0_dconv_layers_0_3_weight, p_encoder_0_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_4 = p_encoder_0_dconv_layers_0_3_weight = p_encoder_0_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_5: "f32[512, 96, 431]" = torch.ops.aten.group_norm.default(convolution_8, 1, p_encoder_0_dconv_layers_0_4_weight, p_encoder_0_dconv_layers_0_4_bias);  convolution_8 = p_encoder_0_dconv_layers_0_4_weight = p_encoder_0_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_3: "f32[512, 48, 431]" = torch.ops.aten.glu.default(group_norm_5, 1);  group_norm_5 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_3: "f32[48]" = torch.ops.aten.slice.Tensor(p_encoder_0_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_encoder_0_dconv_layers_0_6_scale = None
            unsqueeze_2: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_3, 1);  slice_3 = None
            mul_8: "f32[512, 48, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_2, glu_3);  unsqueeze_2 = glu_3 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_14: "f32[512, 48, 431]" = torch.ops.aten.add.Tensor(view, mul_8);  view = mul_8 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_9: "f32[512, 6, 431]" = torch.ops.aten.convolution.default(add_14, p_encoder_0_dconv_layers_1_0_weight, p_encoder_0_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_encoder_0_dconv_layers_1_0_weight = p_encoder_0_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_6: "f32[512, 6, 431]" = torch.ops.aten.group_norm.default(convolution_9, 1, p_encoder_0_dconv_layers_1_1_weight, p_encoder_0_dconv_layers_1_1_bias);  convolution_9 = p_encoder_0_dconv_layers_1_1_weight = p_encoder_0_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_5: "f32[512, 6, 431]" = torch.ops.aten.gelu.default(group_norm_6);  group_norm_6 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_10: "f32[512, 96, 431]" = torch.ops.aten.convolution.default(gelu_5, p_encoder_0_dconv_layers_1_3_weight, p_encoder_0_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_5 = p_encoder_0_dconv_layers_1_3_weight = p_encoder_0_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_7: "f32[512, 96, 431]" = torch.ops.aten.group_norm.default(convolution_10, 1, p_encoder_0_dconv_layers_1_4_weight, p_encoder_0_dconv_layers_1_4_bias);  convolution_10 = p_encoder_0_dconv_layers_1_4_weight = p_encoder_0_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_4: "f32[512, 48, 431]" = torch.ops.aten.glu.default(group_norm_7, 1);  group_norm_7 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_4: "f32[48]" = torch.ops.aten.slice.Tensor(p_encoder_0_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_encoder_0_dconv_layers_1_6_scale = None
            unsqueeze_3: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
            mul_9: "f32[512, 48, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_3, glu_4);  unsqueeze_3 = glu_4 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_15: "f32[512, 48, 431]" = torch.ops.aten.add.Tensor(add_14, mul_9);  add_14 = mul_9 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:151 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_1: "f32[1, 512, 48, 431]" = torch.ops.aten.view.default(add_15, [1, 512, 48, 431]);  add_15 = None
            permute_2: "f32[1, 48, 512, 431]" = torch.ops.aten.permute.default(view_1, [0, 2, 1, 3]);  view_1 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_11: "f32[1, 96, 512, 431]" = torch.ops.aten.convolution.default(permute_2, p_encoder_0_rewrite_weight, p_encoder_0_rewrite_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  permute_2 = p_encoder_0_rewrite_weight = p_encoder_0_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_5: "f32[1, 48, 512, 431]" = torch.ops.aten.glu.default(convolution_11, 1);  convolution_11 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:471 in forward, code: frs = torch.arange(x.shape[-2])
            arange: "i64[512]" = torch.ops.aten.arange.default(512, device = device(type='cpu'), pin_memory = False)
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190 in forward, code: return F.embedding(
            embedding: "f32[512, 48]" = torch.ops.aten.embedding.default(p_freq_emb_embedding_weight, arange);  p_freq_emb_embedding_weight = arange = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:65 in forward, code: out = self.embedding(x) * self.scale
            scalar_tensor_default: "f32[]" = torch.ops.aten.scalar_tensor.default(10, dtype = torch.float32)
            mul_10: "f32[512, 48]" = torch.ops.aten.mul.Tensor(embedding, scalar_tensor_default);  embedding = scalar_tensor_default = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:472 in forward, code: emb = self.freq_emb(frs).t()[None, :, :, None].expand_as(x)
            t: "f32[48, 512]" = torch.ops.aten.t.default(mul_10);  mul_10 = None
            unsqueeze_4: "f32[1, 48, 512]" = torch.ops.aten.unsqueeze.default(t, 0);  t = None
            slice_5: "f32[1, 48, 512]" = torch.ops.aten.slice.Tensor(unsqueeze_4, 1, 0, 9223372036854775807);  unsqueeze_4 = None
            slice_6: "f32[1, 48, 512]" = torch.ops.aten.slice.Tensor(slice_5, 2, 0, 9223372036854775807);  slice_5 = None
            unsqueeze_5: "f32[1, 48, 512, 1]" = torch.ops.aten.unsqueeze.default(slice_6, 3);  slice_6 = None
            expand: "f32[1, 48, 512, 431]" = torch.ops.aten.expand.default(unsqueeze_5, [1, 48, 512, 431]);  unsqueeze_5 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:473 in forward, code: x = x + self.freq_emb_scale * emb
            mul_11: "f32[1, 48, 512, 431]" = torch.ops.aten.mul.Tensor(expand, 0.2);  expand = None
            add_16: "f32[1, 48, 512, 431]" = torch.ops.aten.add.Tensor(glu_5, mul_11);  glu_5 = mul_11 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:135 in forward, code: x = F.pad(x, (0, self.stride - (le % self.stride)))
            constant_pad_nd: "f32[1, 48, 110252]" = torch.ops.aten.constant_pad_nd.default(glu_2, [0, 2], 0.0)
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_12: "f32[1, 96, 27563]" = torch.ops.aten.convolution.default(constant_pad_nd, p_tencoder_1_conv_weight, p_tencoder_1_conv_bias, [4], [2], [1], False, [0], 1);  constant_pad_nd = p_tencoder_1_conv_weight = p_tencoder_1_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_6: "f32[1, 96, 27563]" = torch.ops.aten.gelu.default(convolution_12);  convolution_12 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_13: "f32[1, 12, 27563]" = torch.ops.aten.convolution.default(gelu_6, p_tencoder_1_dconv_layers_0_0_weight, p_tencoder_1_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tencoder_1_dconv_layers_0_0_weight = p_tencoder_1_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_8: "f32[1, 12, 27563]" = torch.ops.aten.group_norm.default(convolution_13, 1, p_tencoder_1_dconv_layers_0_1_weight, p_tencoder_1_dconv_layers_0_1_bias);  convolution_13 = p_tencoder_1_dconv_layers_0_1_weight = p_tencoder_1_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_7: "f32[1, 12, 27563]" = torch.ops.aten.gelu.default(group_norm_8);  group_norm_8 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_14: "f32[1, 192, 27563]" = torch.ops.aten.convolution.default(gelu_7, p_tencoder_1_dconv_layers_0_3_weight, p_tencoder_1_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_7 = p_tencoder_1_dconv_layers_0_3_weight = p_tencoder_1_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_9: "f32[1, 192, 27563]" = torch.ops.aten.group_norm.default(convolution_14, 1, p_tencoder_1_dconv_layers_0_4_weight, p_tencoder_1_dconv_layers_0_4_bias);  convolution_14 = p_tencoder_1_dconv_layers_0_4_weight = p_tencoder_1_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_6: "f32[1, 96, 27563]" = torch.ops.aten.glu.default(group_norm_9, 1);  group_norm_9 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_7: "f32[96]" = torch.ops.aten.slice.Tensor(p_tencoder_1_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tencoder_1_dconv_layers_0_6_scale = None
            unsqueeze_6: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_7, 1);  slice_7 = None
            mul_12: "f32[1, 96, 27563]" = torch.ops.aten.mul.Tensor(unsqueeze_6, glu_6);  unsqueeze_6 = glu_6 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_17: "f32[1, 96, 27563]" = torch.ops.aten.add.Tensor(gelu_6, mul_12);  gelu_6 = mul_12 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_15: "f32[1, 12, 27563]" = torch.ops.aten.convolution.default(add_17, p_tencoder_1_dconv_layers_1_0_weight, p_tencoder_1_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tencoder_1_dconv_layers_1_0_weight = p_tencoder_1_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_10: "f32[1, 12, 27563]" = torch.ops.aten.group_norm.default(convolution_15, 1, p_tencoder_1_dconv_layers_1_1_weight, p_tencoder_1_dconv_layers_1_1_bias);  convolution_15 = p_tencoder_1_dconv_layers_1_1_weight = p_tencoder_1_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_8: "f32[1, 12, 27563]" = torch.ops.aten.gelu.default(group_norm_10);  group_norm_10 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_16: "f32[1, 192, 27563]" = torch.ops.aten.convolution.default(gelu_8, p_tencoder_1_dconv_layers_1_3_weight, p_tencoder_1_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_8 = p_tencoder_1_dconv_layers_1_3_weight = p_tencoder_1_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_11: "f32[1, 192, 27563]" = torch.ops.aten.group_norm.default(convolution_16, 1, p_tencoder_1_dconv_layers_1_4_weight, p_tencoder_1_dconv_layers_1_4_bias);  convolution_16 = p_tencoder_1_dconv_layers_1_4_weight = p_tencoder_1_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_7: "f32[1, 96, 27563]" = torch.ops.aten.glu.default(group_norm_11, 1);  group_norm_11 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_8: "f32[96]" = torch.ops.aten.slice.Tensor(p_tencoder_1_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tencoder_1_dconv_layers_1_6_scale = None
            unsqueeze_7: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_8, 1);  slice_8 = None
            mul_13: "f32[1, 96, 27563]" = torch.ops.aten.mul.Tensor(unsqueeze_7, glu_7);  unsqueeze_7 = glu_7 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_18: "f32[1, 96, 27563]" = torch.ops.aten.add.Tensor(add_17, mul_13);  add_17 = mul_13 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_17: "f32[1, 192, 27563]" = torch.ops.aten.convolution.default(add_18, p_tencoder_1_rewrite_weight, p_tencoder_1_rewrite_bias, [1], [0], [1], False, [0], 1);  add_18 = p_tencoder_1_rewrite_weight = p_tencoder_1_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_8: "f32[1, 96, 27563]" = torch.ops.aten.glu.default(convolution_17, 1);  convolution_17 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_18: "f32[1, 96, 128, 431]" = torch.ops.aten.convolution.default(add_16, p_encoder_1_conv_weight, p_encoder_1_conv_bias, [4, 1], [2, 0], [1, 1], False, [0, 0], 1);  p_encoder_1_conv_weight = p_encoder_1_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_9: "f32[1, 96, 128, 431]" = torch.ops.aten.gelu.default(convolution_18);  convolution_18 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:148 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_3: "f32[1, 128, 96, 431]" = torch.ops.aten.permute.default(gelu_9, [0, 2, 1, 3]);  gelu_9 = None
            view_2: "f32[128, 96, 431]" = torch.ops.aten.view.default(permute_3, [128, 96, 431]);  permute_3 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_19: "f32[128, 12, 431]" = torch.ops.aten.convolution.default(view_2, p_encoder_1_dconv_layers_0_0_weight, p_encoder_1_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_encoder_1_dconv_layers_0_0_weight = p_encoder_1_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_12: "f32[128, 12, 431]" = torch.ops.aten.group_norm.default(convolution_19, 1, p_encoder_1_dconv_layers_0_1_weight, p_encoder_1_dconv_layers_0_1_bias);  convolution_19 = p_encoder_1_dconv_layers_0_1_weight = p_encoder_1_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_10: "f32[128, 12, 431]" = torch.ops.aten.gelu.default(group_norm_12);  group_norm_12 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_20: "f32[128, 192, 431]" = torch.ops.aten.convolution.default(gelu_10, p_encoder_1_dconv_layers_0_3_weight, p_encoder_1_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_10 = p_encoder_1_dconv_layers_0_3_weight = p_encoder_1_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_13: "f32[128, 192, 431]" = torch.ops.aten.group_norm.default(convolution_20, 1, p_encoder_1_dconv_layers_0_4_weight, p_encoder_1_dconv_layers_0_4_bias);  convolution_20 = p_encoder_1_dconv_layers_0_4_weight = p_encoder_1_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_9: "f32[128, 96, 431]" = torch.ops.aten.glu.default(group_norm_13, 1);  group_norm_13 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_9: "f32[96]" = torch.ops.aten.slice.Tensor(p_encoder_1_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_encoder_1_dconv_layers_0_6_scale = None
            unsqueeze_8: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_9, 1);  slice_9 = None
            mul_14: "f32[128, 96, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_8, glu_9);  unsqueeze_8 = glu_9 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_19: "f32[128, 96, 431]" = torch.ops.aten.add.Tensor(view_2, mul_14);  view_2 = mul_14 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_21: "f32[128, 12, 431]" = torch.ops.aten.convolution.default(add_19, p_encoder_1_dconv_layers_1_0_weight, p_encoder_1_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_encoder_1_dconv_layers_1_0_weight = p_encoder_1_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_14: "f32[128, 12, 431]" = torch.ops.aten.group_norm.default(convolution_21, 1, p_encoder_1_dconv_layers_1_1_weight, p_encoder_1_dconv_layers_1_1_bias);  convolution_21 = p_encoder_1_dconv_layers_1_1_weight = p_encoder_1_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_11: "f32[128, 12, 431]" = torch.ops.aten.gelu.default(group_norm_14);  group_norm_14 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_22: "f32[128, 192, 431]" = torch.ops.aten.convolution.default(gelu_11, p_encoder_1_dconv_layers_1_3_weight, p_encoder_1_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_11 = p_encoder_1_dconv_layers_1_3_weight = p_encoder_1_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_15: "f32[128, 192, 431]" = torch.ops.aten.group_norm.default(convolution_22, 1, p_encoder_1_dconv_layers_1_4_weight, p_encoder_1_dconv_layers_1_4_bias);  convolution_22 = p_encoder_1_dconv_layers_1_4_weight = p_encoder_1_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_10: "f32[128, 96, 431]" = torch.ops.aten.glu.default(group_norm_15, 1);  group_norm_15 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_10: "f32[96]" = torch.ops.aten.slice.Tensor(p_encoder_1_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_encoder_1_dconv_layers_1_6_scale = None
            unsqueeze_9: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_10, 1);  slice_10 = None
            mul_15: "f32[128, 96, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_9, glu_10);  unsqueeze_9 = glu_10 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_20: "f32[128, 96, 431]" = torch.ops.aten.add.Tensor(add_19, mul_15);  add_19 = mul_15 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:151 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_3: "f32[1, 128, 96, 431]" = torch.ops.aten.view.default(add_20, [1, 128, 96, 431]);  add_20 = None
            permute_4: "f32[1, 96, 128, 431]" = torch.ops.aten.permute.default(view_3, [0, 2, 1, 3]);  view_3 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_23: "f32[1, 192, 128, 431]" = torch.ops.aten.convolution.default(permute_4, p_encoder_1_rewrite_weight, p_encoder_1_rewrite_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  permute_4 = p_encoder_1_rewrite_weight = p_encoder_1_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_11: "f32[1, 96, 128, 431]" = torch.ops.aten.glu.default(convolution_23, 1);  convolution_23 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:135 in forward, code: x = F.pad(x, (0, self.stride - (le % self.stride)))
            constant_pad_nd_1: "f32[1, 96, 27564]" = torch.ops.aten.constant_pad_nd.default(glu_8, [0, 1], 0.0)
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_24: "f32[1, 192, 6891]" = torch.ops.aten.convolution.default(constant_pad_nd_1, p_tencoder_2_conv_weight, p_tencoder_2_conv_bias, [4], [2], [1], False, [0], 1);  constant_pad_nd_1 = p_tencoder_2_conv_weight = p_tencoder_2_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_12: "f32[1, 192, 6891]" = torch.ops.aten.gelu.default(convolution_24);  convolution_24 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_25: "f32[1, 24, 6891]" = torch.ops.aten.convolution.default(gelu_12, p_tencoder_2_dconv_layers_0_0_weight, p_tencoder_2_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tencoder_2_dconv_layers_0_0_weight = p_tencoder_2_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_16: "f32[1, 24, 6891]" = torch.ops.aten.group_norm.default(convolution_25, 1, p_tencoder_2_dconv_layers_0_1_weight, p_tencoder_2_dconv_layers_0_1_bias);  convolution_25 = p_tencoder_2_dconv_layers_0_1_weight = p_tencoder_2_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_13: "f32[1, 24, 6891]" = torch.ops.aten.gelu.default(group_norm_16);  group_norm_16 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_26: "f32[1, 384, 6891]" = torch.ops.aten.convolution.default(gelu_13, p_tencoder_2_dconv_layers_0_3_weight, p_tencoder_2_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_13 = p_tencoder_2_dconv_layers_0_3_weight = p_tencoder_2_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_17: "f32[1, 384, 6891]" = torch.ops.aten.group_norm.default(convolution_26, 1, p_tencoder_2_dconv_layers_0_4_weight, p_tencoder_2_dconv_layers_0_4_bias);  convolution_26 = p_tencoder_2_dconv_layers_0_4_weight = p_tencoder_2_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_12: "f32[1, 192, 6891]" = torch.ops.aten.glu.default(group_norm_17, 1);  group_norm_17 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_11: "f32[192]" = torch.ops.aten.slice.Tensor(p_tencoder_2_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tencoder_2_dconv_layers_0_6_scale = None
            unsqueeze_10: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_11, 1);  slice_11 = None
            mul_16: "f32[1, 192, 6891]" = torch.ops.aten.mul.Tensor(unsqueeze_10, glu_12);  unsqueeze_10 = glu_12 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_21: "f32[1, 192, 6891]" = torch.ops.aten.add.Tensor(gelu_12, mul_16);  gelu_12 = mul_16 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_27: "f32[1, 24, 6891]" = torch.ops.aten.convolution.default(add_21, p_tencoder_2_dconv_layers_1_0_weight, p_tencoder_2_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tencoder_2_dconv_layers_1_0_weight = p_tencoder_2_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_18: "f32[1, 24, 6891]" = torch.ops.aten.group_norm.default(convolution_27, 1, p_tencoder_2_dconv_layers_1_1_weight, p_tencoder_2_dconv_layers_1_1_bias);  convolution_27 = p_tencoder_2_dconv_layers_1_1_weight = p_tencoder_2_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_14: "f32[1, 24, 6891]" = torch.ops.aten.gelu.default(group_norm_18);  group_norm_18 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_28: "f32[1, 384, 6891]" = torch.ops.aten.convolution.default(gelu_14, p_tencoder_2_dconv_layers_1_3_weight, p_tencoder_2_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_14 = p_tencoder_2_dconv_layers_1_3_weight = p_tencoder_2_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_19: "f32[1, 384, 6891]" = torch.ops.aten.group_norm.default(convolution_28, 1, p_tencoder_2_dconv_layers_1_4_weight, p_tencoder_2_dconv_layers_1_4_bias);  convolution_28 = p_tencoder_2_dconv_layers_1_4_weight = p_tencoder_2_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_13: "f32[1, 192, 6891]" = torch.ops.aten.glu.default(group_norm_19, 1);  group_norm_19 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_12: "f32[192]" = torch.ops.aten.slice.Tensor(p_tencoder_2_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tencoder_2_dconv_layers_1_6_scale = None
            unsqueeze_11: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_12, 1);  slice_12 = None
            mul_17: "f32[1, 192, 6891]" = torch.ops.aten.mul.Tensor(unsqueeze_11, glu_13);  unsqueeze_11 = glu_13 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_22: "f32[1, 192, 6891]" = torch.ops.aten.add.Tensor(add_21, mul_17);  add_21 = mul_17 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_29: "f32[1, 384, 6891]" = torch.ops.aten.convolution.default(add_22, p_tencoder_2_rewrite_weight, p_tencoder_2_rewrite_bias, [1], [0], [1], False, [0], 1);  add_22 = p_tencoder_2_rewrite_weight = p_tencoder_2_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_14: "f32[1, 192, 6891]" = torch.ops.aten.glu.default(convolution_29, 1);  convolution_29 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_30: "f32[1, 192, 32, 431]" = torch.ops.aten.convolution.default(glu_11, p_encoder_2_conv_weight, p_encoder_2_conv_bias, [4, 1], [2, 0], [1, 1], False, [0, 0], 1);  p_encoder_2_conv_weight = p_encoder_2_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_15: "f32[1, 192, 32, 431]" = torch.ops.aten.gelu.default(convolution_30);  convolution_30 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:148 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_5: "f32[1, 32, 192, 431]" = torch.ops.aten.permute.default(gelu_15, [0, 2, 1, 3]);  gelu_15 = None
            view_4: "f32[32, 192, 431]" = torch.ops.aten.view.default(permute_5, [32, 192, 431]);  permute_5 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_31: "f32[32, 24, 431]" = torch.ops.aten.convolution.default(view_4, p_encoder_2_dconv_layers_0_0_weight, p_encoder_2_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_encoder_2_dconv_layers_0_0_weight = p_encoder_2_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_20: "f32[32, 24, 431]" = torch.ops.aten.group_norm.default(convolution_31, 1, p_encoder_2_dconv_layers_0_1_weight, p_encoder_2_dconv_layers_0_1_bias);  convolution_31 = p_encoder_2_dconv_layers_0_1_weight = p_encoder_2_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_16: "f32[32, 24, 431]" = torch.ops.aten.gelu.default(group_norm_20);  group_norm_20 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_32: "f32[32, 384, 431]" = torch.ops.aten.convolution.default(gelu_16, p_encoder_2_dconv_layers_0_3_weight, p_encoder_2_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_16 = p_encoder_2_dconv_layers_0_3_weight = p_encoder_2_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_21: "f32[32, 384, 431]" = torch.ops.aten.group_norm.default(convolution_32, 1, p_encoder_2_dconv_layers_0_4_weight, p_encoder_2_dconv_layers_0_4_bias);  convolution_32 = p_encoder_2_dconv_layers_0_4_weight = p_encoder_2_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_15: "f32[32, 192, 431]" = torch.ops.aten.glu.default(group_norm_21, 1);  group_norm_21 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_13: "f32[192]" = torch.ops.aten.slice.Tensor(p_encoder_2_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_encoder_2_dconv_layers_0_6_scale = None
            unsqueeze_12: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_13, 1);  slice_13 = None
            mul_18: "f32[32, 192, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_12, glu_15);  unsqueeze_12 = glu_15 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_23: "f32[32, 192, 431]" = torch.ops.aten.add.Tensor(view_4, mul_18);  view_4 = mul_18 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_33: "f32[32, 24, 431]" = torch.ops.aten.convolution.default(add_23, p_encoder_2_dconv_layers_1_0_weight, p_encoder_2_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_encoder_2_dconv_layers_1_0_weight = p_encoder_2_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_22: "f32[32, 24, 431]" = torch.ops.aten.group_norm.default(convolution_33, 1, p_encoder_2_dconv_layers_1_1_weight, p_encoder_2_dconv_layers_1_1_bias);  convolution_33 = p_encoder_2_dconv_layers_1_1_weight = p_encoder_2_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_17: "f32[32, 24, 431]" = torch.ops.aten.gelu.default(group_norm_22);  group_norm_22 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_34: "f32[32, 384, 431]" = torch.ops.aten.convolution.default(gelu_17, p_encoder_2_dconv_layers_1_3_weight, p_encoder_2_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_17 = p_encoder_2_dconv_layers_1_3_weight = p_encoder_2_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_23: "f32[32, 384, 431]" = torch.ops.aten.group_norm.default(convolution_34, 1, p_encoder_2_dconv_layers_1_4_weight, p_encoder_2_dconv_layers_1_4_bias);  convolution_34 = p_encoder_2_dconv_layers_1_4_weight = p_encoder_2_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_16: "f32[32, 192, 431]" = torch.ops.aten.glu.default(group_norm_23, 1);  group_norm_23 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_14: "f32[192]" = torch.ops.aten.slice.Tensor(p_encoder_2_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_encoder_2_dconv_layers_1_6_scale = None
            unsqueeze_13: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_14, 1);  slice_14 = None
            mul_19: "f32[32, 192, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_13, glu_16);  unsqueeze_13 = glu_16 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_24: "f32[32, 192, 431]" = torch.ops.aten.add.Tensor(add_23, mul_19);  add_23 = mul_19 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:151 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_5: "f32[1, 32, 192, 431]" = torch.ops.aten.view.default(add_24, [1, 32, 192, 431]);  add_24 = None
            permute_6: "f32[1, 192, 32, 431]" = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_35: "f32[1, 384, 32, 431]" = torch.ops.aten.convolution.default(permute_6, p_encoder_2_rewrite_weight, p_encoder_2_rewrite_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  permute_6 = p_encoder_2_rewrite_weight = p_encoder_2_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_17: "f32[1, 192, 32, 431]" = torch.ops.aten.glu.default(convolution_35, 1);  convolution_35 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:135 in forward, code: x = F.pad(x, (0, self.stride - (le % self.stride)))
            constant_pad_nd_2: "f32[1, 192, 6892]" = torch.ops.aten.constant_pad_nd.default(glu_14, [0, 1], 0.0)
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_36: "f32[1, 384, 1723]" = torch.ops.aten.convolution.default(constant_pad_nd_2, p_tencoder_3_conv_weight, p_tencoder_3_conv_bias, [4], [2], [1], False, [0], 1);  constant_pad_nd_2 = p_tencoder_3_conv_weight = p_tencoder_3_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_18: "f32[1, 384, 1723]" = torch.ops.aten.gelu.default(convolution_36);  convolution_36 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_37: "f32[1, 48, 1723]" = torch.ops.aten.convolution.default(gelu_18, p_tencoder_3_dconv_layers_0_0_weight, p_tencoder_3_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tencoder_3_dconv_layers_0_0_weight = p_tencoder_3_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_24: "f32[1, 48, 1723]" = torch.ops.aten.group_norm.default(convolution_37, 1, p_tencoder_3_dconv_layers_0_1_weight, p_tencoder_3_dconv_layers_0_1_bias);  convolution_37 = p_tencoder_3_dconv_layers_0_1_weight = p_tencoder_3_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_19: "f32[1, 48, 1723]" = torch.ops.aten.gelu.default(group_norm_24);  group_norm_24 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_38: "f32[1, 768, 1723]" = torch.ops.aten.convolution.default(gelu_19, p_tencoder_3_dconv_layers_0_3_weight, p_tencoder_3_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_19 = p_tencoder_3_dconv_layers_0_3_weight = p_tencoder_3_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_25: "f32[1, 768, 1723]" = torch.ops.aten.group_norm.default(convolution_38, 1, p_tencoder_3_dconv_layers_0_4_weight, p_tencoder_3_dconv_layers_0_4_bias);  convolution_38 = p_tencoder_3_dconv_layers_0_4_weight = p_tencoder_3_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_18: "f32[1, 384, 1723]" = torch.ops.aten.glu.default(group_norm_25, 1);  group_norm_25 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_15: "f32[384]" = torch.ops.aten.slice.Tensor(p_tencoder_3_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tencoder_3_dconv_layers_0_6_scale = None
            unsqueeze_14: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_15, 1);  slice_15 = None
            mul_20: "f32[1, 384, 1723]" = torch.ops.aten.mul.Tensor(unsqueeze_14, glu_18);  unsqueeze_14 = glu_18 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_25: "f32[1, 384, 1723]" = torch.ops.aten.add.Tensor(gelu_18, mul_20);  gelu_18 = mul_20 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_39: "f32[1, 48, 1723]" = torch.ops.aten.convolution.default(add_25, p_tencoder_3_dconv_layers_1_0_weight, p_tencoder_3_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tencoder_3_dconv_layers_1_0_weight = p_tencoder_3_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_26: "f32[1, 48, 1723]" = torch.ops.aten.group_norm.default(convolution_39, 1, p_tencoder_3_dconv_layers_1_1_weight, p_tencoder_3_dconv_layers_1_1_bias);  convolution_39 = p_tencoder_3_dconv_layers_1_1_weight = p_tencoder_3_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_20: "f32[1, 48, 1723]" = torch.ops.aten.gelu.default(group_norm_26);  group_norm_26 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_40: "f32[1, 768, 1723]" = torch.ops.aten.convolution.default(gelu_20, p_tencoder_3_dconv_layers_1_3_weight, p_tencoder_3_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_20 = p_tencoder_3_dconv_layers_1_3_weight = p_tencoder_3_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_27: "f32[1, 768, 1723]" = torch.ops.aten.group_norm.default(convolution_40, 1, p_tencoder_3_dconv_layers_1_4_weight, p_tencoder_3_dconv_layers_1_4_bias);  convolution_40 = p_tencoder_3_dconv_layers_1_4_weight = p_tencoder_3_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_19: "f32[1, 384, 1723]" = torch.ops.aten.glu.default(group_norm_27, 1);  group_norm_27 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_16: "f32[384]" = torch.ops.aten.slice.Tensor(p_tencoder_3_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tencoder_3_dconv_layers_1_6_scale = None
            unsqueeze_15: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_16, 1);  slice_16 = None
            mul_21: "f32[1, 384, 1723]" = torch.ops.aten.mul.Tensor(unsqueeze_15, glu_19);  unsqueeze_15 = glu_19 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_26: "f32[1, 384, 1723]" = torch.ops.aten.add.Tensor(add_25, mul_21);  add_25 = mul_21 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_41: "f32[1, 768, 1723]" = torch.ops.aten.convolution.default(add_26, p_tencoder_3_rewrite_weight, p_tencoder_3_rewrite_bias, [1], [0], [1], False, [0], 1);  add_26 = p_tencoder_3_rewrite_weight = p_tencoder_3_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_20: "f32[1, 384, 1723]" = torch.ops.aten.glu.default(convolution_41, 1);  convolution_41 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_42: "f32[1, 384, 8, 431]" = torch.ops.aten.convolution.default(glu_17, p_encoder_3_conv_weight, p_encoder_3_conv_bias, [4, 1], [2, 0], [1, 1], False, [0, 0], 1);  p_encoder_3_conv_weight = p_encoder_3_conv_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:144 in forward, code: y = F.gelu(self.norm1(y))
            gelu_21: "f32[1, 384, 8, 431]" = torch.ops.aten.gelu.default(convolution_42);  convolution_42 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:148 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_7: "f32[1, 8, 384, 431]" = torch.ops.aten.permute.default(gelu_21, [0, 2, 1, 3]);  gelu_21 = None
            view_6: "f32[8, 384, 431]" = torch.ops.aten.view.default(permute_7, [8, 384, 431]);  permute_7 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_43: "f32[8, 48, 431]" = torch.ops.aten.convolution.default(view_6, p_encoder_3_dconv_layers_0_0_weight, p_encoder_3_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_encoder_3_dconv_layers_0_0_weight = p_encoder_3_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_28: "f32[8, 48, 431]" = torch.ops.aten.group_norm.default(convolution_43, 1, p_encoder_3_dconv_layers_0_1_weight, p_encoder_3_dconv_layers_0_1_bias);  convolution_43 = p_encoder_3_dconv_layers_0_1_weight = p_encoder_3_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_22: "f32[8, 48, 431]" = torch.ops.aten.gelu.default(group_norm_28);  group_norm_28 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_44: "f32[8, 768, 431]" = torch.ops.aten.convolution.default(gelu_22, p_encoder_3_dconv_layers_0_3_weight, p_encoder_3_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_22 = p_encoder_3_dconv_layers_0_3_weight = p_encoder_3_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_29: "f32[8, 768, 431]" = torch.ops.aten.group_norm.default(convolution_44, 1, p_encoder_3_dconv_layers_0_4_weight, p_encoder_3_dconv_layers_0_4_bias);  convolution_44 = p_encoder_3_dconv_layers_0_4_weight = p_encoder_3_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_21: "f32[8, 384, 431]" = torch.ops.aten.glu.default(group_norm_29, 1);  group_norm_29 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_17: "f32[384]" = torch.ops.aten.slice.Tensor(p_encoder_3_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_encoder_3_dconv_layers_0_6_scale = None
            unsqueeze_16: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_17, 1);  slice_17 = None
            mul_22: "f32[8, 384, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_16, glu_21);  unsqueeze_16 = glu_21 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_27: "f32[8, 384, 431]" = torch.ops.aten.add.Tensor(view_6, mul_22);  view_6 = mul_22 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_45: "f32[8, 48, 431]" = torch.ops.aten.convolution.default(add_27, p_encoder_3_dconv_layers_1_0_weight, p_encoder_3_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_encoder_3_dconv_layers_1_0_weight = p_encoder_3_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_30: "f32[8, 48, 431]" = torch.ops.aten.group_norm.default(convolution_45, 1, p_encoder_3_dconv_layers_1_1_weight, p_encoder_3_dconv_layers_1_1_bias);  convolution_45 = p_encoder_3_dconv_layers_1_1_weight = p_encoder_3_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_23: "f32[8, 48, 431]" = torch.ops.aten.gelu.default(group_norm_30);  group_norm_30 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_46: "f32[8, 768, 431]" = torch.ops.aten.convolution.default(gelu_23, p_encoder_3_dconv_layers_1_3_weight, p_encoder_3_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_23 = p_encoder_3_dconv_layers_1_3_weight = p_encoder_3_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_31: "f32[8, 768, 431]" = torch.ops.aten.group_norm.default(convolution_46, 1, p_encoder_3_dconv_layers_1_4_weight, p_encoder_3_dconv_layers_1_4_bias);  convolution_46 = p_encoder_3_dconv_layers_1_4_weight = p_encoder_3_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_22: "f32[8, 384, 431]" = torch.ops.aten.glu.default(group_norm_31, 1);  group_norm_31 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_18: "f32[384]" = torch.ops.aten.slice.Tensor(p_encoder_3_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_encoder_3_dconv_layers_1_6_scale = None
            unsqueeze_17: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_18, 1);  slice_18 = None
            mul_23: "f32[8, 384, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_17, glu_22);  unsqueeze_17 = glu_22 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_28: "f32[8, 384, 431]" = torch.ops.aten.add.Tensor(add_27, mul_23);  add_27 = mul_23 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:151 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_7: "f32[1, 8, 384, 431]" = torch.ops.aten.view.default(add_28, [1, 8, 384, 431]);  add_28 = None
            permute_8: "f32[1, 384, 8, 431]" = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_47: "f32[1, 768, 8, 431]" = torch.ops.aten.convolution.default(permute_8, p_encoder_3_rewrite_weight, p_encoder_3_rewrite_bias, [1, 1], [0, 0], [1, 1], False, [0, 0], 1);  permute_8 = p_encoder_3_rewrite_weight = p_encoder_3_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:154 in forward, code: z = F.glu(z, dim=1)
            glu_23: "f32[1, 384, 8, 431]" = torch.ops.aten.glu.default(convolution_47, 1);  convolution_47 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:479 in forward, code: x = rearrange(x, "b c f t-> b c (f t)")
            view_8: "f32[1, 384, 3448]" = torch.ops.aten.view.default(glu_23, [1, 384, 3448])
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_48: "f32[1, 512, 3448]" = torch.ops.aten.convolution.default(view_8, p_channel_upsampler_weight, p_channel_upsampler_bias, [1], [0], [1], False, [0], 1);  view_8 = p_channel_upsampler_weight = p_channel_upsampler_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:481 in forward, code: x = rearrange(x, "b c (f t)-> b c f t", f=f)
            view_9: "f32[1, 512, 8, 431]" = torch.ops.aten.view.default(convolution_48, [1, 512, 8, 431]);  convolution_48 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_49: "f32[1, 512, 1723]" = torch.ops.aten.convolution.default(glu_20, p_channel_upsampler_t_weight, p_channel_upsampler_t_bias, [1], [0], [1], False, [0], 1);  p_channel_upsampler_t_weight = p_channel_upsampler_t_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:650 in forward, code: pos_emb_2d = create_2d_sin_embedding(
            zeros: "f32[512, 8, 431]" = torch.ops.aten.zeros.default([512, 8, 431], device = device(type='cpu'), pin_memory = False)
            arange_1: "f32[128]" = torch.ops.aten.arange.start_step(0.0, 256, 2, device = device(type='cpu'), pin_memory = False)
            mul_24: "f32[128]" = torch.ops.aten.mul.Tensor(arange_1, -0.03597789207803197);  arange_1 = None
            exp: "f32[128]" = torch.ops.aten.exp.default(mul_24);  mul_24 = None
            arange_2: "f32[431]" = torch.ops.aten.arange.start(0.0, 431, device = device(type='cpu'), pin_memory = False)
            unsqueeze_18: "f32[431, 1]" = torch.ops.aten.unsqueeze.default(arange_2, 1);  arange_2 = None
            arange_3: "f32[8]" = torch.ops.aten.arange.start(0.0, 8, device = device(type='cpu'), pin_memory = False)
            unsqueeze_19: "f32[8, 1]" = torch.ops.aten.unsqueeze.default(arange_3, 1);  arange_3 = None
            mul_25: "f32[431, 128]" = torch.ops.aten.mul.Tensor(unsqueeze_18, exp)
            sin: "f32[431, 128]" = torch.ops.aten.sin.default(mul_25);  mul_25 = None
            transpose: "f32[128, 431]" = torch.ops.aten.transpose.int(sin, 0, 1);  sin = None
            unsqueeze_20: "f32[128, 1, 431]" = torch.ops.aten.unsqueeze.default(transpose, 1);  transpose = None
            repeat: "f32[128, 8, 431]" = torch.ops.aten.repeat.default(unsqueeze_20, [1, 8, 1]);  unsqueeze_20 = None
            slice_19: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(zeros, 0, 0, 256, 2)
            slice_20: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_19, 1, 0, 9223372036854775807);  slice_19 = None
            slice_21: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_20, 2, 0, 9223372036854775807);  slice_20 = None
            copy: "f32[128, 8, 431]" = torch.ops.aten.copy.default(slice_21, repeat);  slice_21 = repeat = None
            slice_22: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(zeros, 0, 0, 256, 2)
            slice_23: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_22, 1, 0, 9223372036854775807)
            slice_scatter: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_23, copy, 2, 0, 9223372036854775807);  slice_23 = copy = None
            slice_scatter_1: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_22, slice_scatter, 1, 0, 9223372036854775807);  slice_22 = slice_scatter = None
            slice_scatter_2: "f32[512, 8, 431]" = torch.ops.aten.slice_scatter.default(zeros, slice_scatter_1, 0, 0, 256, 2);  zeros = slice_scatter_1 = None
            mul_26: "f32[431, 128]" = torch.ops.aten.mul.Tensor(unsqueeze_18, exp);  unsqueeze_18 = None
            cos: "f32[431, 128]" = torch.ops.aten.cos.default(mul_26);  mul_26 = None
            transpose_1: "f32[128, 431]" = torch.ops.aten.transpose.int(cos, 0, 1);  cos = None
            unsqueeze_21: "f32[128, 1, 431]" = torch.ops.aten.unsqueeze.default(transpose_1, 1);  transpose_1 = None
            repeat_1: "f32[128, 8, 431]" = torch.ops.aten.repeat.default(unsqueeze_21, [1, 8, 1]);  unsqueeze_21 = None
            slice_24: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_scatter_2, 0, 1, 256, 2)
            slice_25: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_24, 1, 0, 9223372036854775807);  slice_24 = None
            slice_26: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_25, 2, 0, 9223372036854775807);  slice_25 = None
            copy_1: "f32[128, 8, 431]" = torch.ops.aten.copy.default(slice_26, repeat_1);  slice_26 = repeat_1 = None
            slice_27: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_scatter_2, 0, 1, 256, 2)
            slice_28: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_27, 1, 0, 9223372036854775807)
            slice_scatter_3: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_28, copy_1, 2, 0, 9223372036854775807);  slice_28 = copy_1 = None
            slice_scatter_4: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_27, slice_scatter_3, 1, 0, 9223372036854775807);  slice_27 = slice_scatter_3 = None
            slice_scatter_5: "f32[512, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_scatter_2, slice_scatter_4, 0, 1, 256, 2);  slice_scatter_2 = slice_scatter_4 = None
            mul_27: "f32[8, 128]" = torch.ops.aten.mul.Tensor(unsqueeze_19, exp)
            sin_1: "f32[8, 128]" = torch.ops.aten.sin.default(mul_27);  mul_27 = None
            transpose_2: "f32[128, 8]" = torch.ops.aten.transpose.int(sin_1, 0, 1);  sin_1 = None
            unsqueeze_22: "f32[128, 8, 1]" = torch.ops.aten.unsqueeze.default(transpose_2, 2);  transpose_2 = None
            repeat_2: "f32[128, 8, 431]" = torch.ops.aten.repeat.default(unsqueeze_22, [1, 1, 431]);  unsqueeze_22 = None
            slice_29: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_scatter_5, 0, 256, 9223372036854775807, 2)
            slice_30: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_29, 1, 0, 9223372036854775807);  slice_29 = None
            slice_31: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_30, 2, 0, 9223372036854775807);  slice_30 = None
            copy_2: "f32[128, 8, 431]" = torch.ops.aten.copy.default(slice_31, repeat_2);  slice_31 = repeat_2 = None
            slice_32: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_scatter_5, 0, 256, 9223372036854775807, 2)
            slice_33: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_32, 1, 0, 9223372036854775807)
            slice_scatter_6: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_33, copy_2, 2, 0, 9223372036854775807);  slice_33 = copy_2 = None
            slice_scatter_7: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_32, slice_scatter_6, 1, 0, 9223372036854775807);  slice_32 = slice_scatter_6 = None
            slice_scatter_8: "f32[512, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_scatter_5, slice_scatter_7, 0, 256, 9223372036854775807, 2);  slice_scatter_5 = slice_scatter_7 = None
            mul_28: "f32[8, 128]" = torch.ops.aten.mul.Tensor(unsqueeze_19, exp);  unsqueeze_19 = exp = None
            cos_1: "f32[8, 128]" = torch.ops.aten.cos.default(mul_28);  mul_28 = None
            transpose_3: "f32[128, 8]" = torch.ops.aten.transpose.int(cos_1, 0, 1);  cos_1 = None
            unsqueeze_23: "f32[128, 8, 1]" = torch.ops.aten.unsqueeze.default(transpose_3, 2);  transpose_3 = None
            repeat_3: "f32[128, 8, 431]" = torch.ops.aten.repeat.default(unsqueeze_23, [1, 1, 431]);  unsqueeze_23 = None
            slice_34: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_scatter_8, 0, 257, 9223372036854775807, 2)
            slice_35: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_34, 1, 0, 9223372036854775807);  slice_34 = None
            slice_36: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_35, 2, 0, 9223372036854775807);  slice_35 = None
            copy_3: "f32[128, 8, 431]" = torch.ops.aten.copy.default(slice_36, repeat_3);  slice_36 = repeat_3 = None
            slice_37: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_scatter_8, 0, 257, 9223372036854775807, 2)
            slice_38: "f32[128, 8, 431]" = torch.ops.aten.slice.Tensor(slice_37, 1, 0, 9223372036854775807)
            slice_scatter_9: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_38, copy_3, 2, 0, 9223372036854775807);  slice_38 = copy_3 = None
            slice_scatter_10: "f32[128, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_37, slice_scatter_9, 1, 0, 9223372036854775807);  slice_37 = slice_scatter_9 = None
            slice_scatter_11: "f32[512, 8, 431]" = torch.ops.aten.slice_scatter.default(slice_scatter_8, slice_scatter_10, 0, 257, 9223372036854775807, 2);  slice_scatter_8 = slice_scatter_10 = None
            unsqueeze_24: "f32[1, 512, 8, 431]" = torch.ops.aten.unsqueeze.default(slice_scatter_11, 0);  slice_scatter_11 = None
            slice_39: "f32[1, 512, 8, 431]" = torch.ops.aten.slice.Tensor(unsqueeze_24, 1, 0, 9223372036854775807);  unsqueeze_24 = None
            _to_copy: "f32[1, 512, 8, 431]" = torch.ops.aten._to_copy.default(slice_39, dtype = torch.float32, layout = torch.strided, device = device(type='cpu'));  slice_39 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:653 in forward, code: pos_emb_2d = rearrange(pos_emb_2d, "b c fr t1 -> b (t1 fr) c")
            permute_9: "f32[1, 431, 8, 512]" = torch.ops.aten.permute.default(_to_copy, [0, 3, 2, 1]);  _to_copy = None
            clone_1: "f32[1, 431, 8, 512]" = torch.ops.aten.clone.default(permute_9, memory_format = torch.contiguous_format);  permute_9 = None
            _unsafe_view_1: "f32[1, 3448, 512]" = torch.ops.aten._unsafe_view.default(clone_1, [1, 3448, 512]);  clone_1 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:654 in forward, code: x = rearrange(x, "b c fr t1 -> b (t1 fr) c")
            permute_10: "f32[1, 431, 8, 512]" = torch.ops.aten.permute.default(view_9, [0, 3, 2, 1]);  view_9 = None
            clone_2: "f32[1, 431, 8, 512]" = torch.ops.aten.clone.default(permute_10, memory_format = torch.contiguous_format);  permute_10 = None
            _unsafe_view_2: "f32[1, 3448, 512]" = torch.ops.aten._unsafe_view.default(clone_2, [1, 3448, 512]);  clone_2 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm = torch.ops.aten.native_layer_norm.default(_unsafe_view_2, [512], p_crosstransformer_norm_in_weight, p_crosstransformer_norm_in_bias, 1e-05);  _unsafe_view_2 = p_crosstransformer_norm_in_weight = p_crosstransformer_norm_in_bias = None
            getitem: "f32[1, 3448, 512]" = native_layer_norm[0];  native_layer_norm = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:656 in forward, code: x = x + self.weight_pos_embed * pos_emb_2d
            mul_29: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(_unsafe_view_1, 1.0);  _unsafe_view_1 = None
            add_29: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(getitem, mul_29);  getitem = mul_29 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:659 in forward, code: xt = rearrange(xt, "b c t2 -> b t2 c")  # now T2, B, C
            permute_11: "f32[1, 1723, 512]" = torch.ops.aten.permute.default(convolution_49, [0, 2, 1]);  convolution_49 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:660 in forward, code: pos_emb = self._get_pos_embedding(T2, B, C, x.device)
            arange_4: "i64[1723]" = torch.ops.aten.arange.default(1723, device = device(type='cpu'), pin_memory = False)
            view_10: "i64[1723, 1, 1]" = torch.ops.aten.view.default(arange_4, [-1, 1, 1]);  arange_4 = None
            add_30: "i64[1723, 1, 1]" = torch.ops.aten.add.Tensor(view_10, 0);  view_10 = None
            arange_5: "i64[256]" = torch.ops.aten.arange.default(256, device = device(type='cpu'), pin_memory = False)
            view_11: "i64[1, 1, 256]" = torch.ops.aten.view.default(arange_5, [1, 1, -1]);  arange_5 = None
            convert_element_type_default: "f32[1, 1, 256]" = torch.ops.prims.convert_element_type.default(view_11, dtype = torch.float32);  view_11 = None
            scalar_tensor_default_1: "f32[]" = torch.ops.aten.scalar_tensor.default(255, dtype = torch.float32)
            div_2: "f32[1, 1, 256]" = torch.ops.aten.div.Tensor(convert_element_type_default, scalar_tensor_default_1);  convert_element_type_default = scalar_tensor_default_1 = None
            pow_1: "f32[1, 1, 256]" = torch.ops.aten.pow.Scalar(10000.0, div_2);  div_2 = None
            convert_element_type_default_1: "f32[1723, 1, 1]" = torch.ops.prims.convert_element_type.default(add_30, dtype = torch.float32);  add_30 = None
            div_3: "f32[1723, 1, 256]" = torch.ops.aten.div.Tensor(convert_element_type_default_1, pow_1);  convert_element_type_default_1 = pow_1 = None
            cos_2: "f32[1723, 1, 256]" = torch.ops.aten.cos.default(div_3)
            sin_2: "f32[1723, 1, 256]" = torch.ops.aten.sin.default(div_3);  div_3 = None
            cat: "f32[1723, 1, 512]" = torch.ops.aten.cat.default([cos_2, sin_2], -1);  cos_2 = sin_2 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:661 in forward, code: pos_emb = rearrange(pos_emb, "t2 b c -> b t2 c")
            permute_12: "f32[1, 1723, 512]" = torch.ops.aten.permute.default(cat, [1, 0, 2]);  cat = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_1 = torch.ops.aten.native_layer_norm.default(permute_11, [512], p_crosstransformer_norm_in_t_weight, p_crosstransformer_norm_in_t_bias, 1e-05);  permute_11 = p_crosstransformer_norm_in_t_weight = p_crosstransformer_norm_in_t_bias = None
            getitem_3: "f32[1, 1723, 512]" = native_layer_norm_1[0];  native_layer_norm_1 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:663 in forward, code: xt = xt + self.weight_pos_embed * pos_emb
            mul_30: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(permute_12, 1.0);  permute_12 = None
            add_31: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(getitem_3, mul_30);  getitem_3 = mul_30 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_2 = torch.ops.aten.native_layer_norm.default(add_29, [512], p_crosstransformer_layers_0_norm1_weight, p_crosstransformer_layers_0_norm1_bias, 1e-05);  p_crosstransformer_layers_0_norm1_weight = p_crosstransformer_layers_0_norm1_bias = None
            getitem_6: "f32[1, 3448, 512]" = native_layer_norm_2[0];  native_layer_norm_2 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1334 in forward, code: query = key = value = query.transpose(1, 0)
            transpose_4: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_6, 1, 0);  getitem_6 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            view_12: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_4, [3448, 512]);  transpose_4 = None
            t_1: "f32[512, 1536]" = torch.ops.aten.t.default(p_crosstransformer_layers_0_self_attn_in_proj_weight);  p_crosstransformer_layers_0_self_attn_in_proj_weight = None
            addmm: "f32[3448, 1536]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_0_self_attn_in_proj_bias, view_12, t_1);  p_crosstransformer_layers_0_self_attn_in_proj_bias = view_12 = t_1 = None
            view_13: "f32[3448, 1, 1536]" = torch.ops.aten.view.default(addmm, [3448, 1, 1536]);  addmm = None
            view_14: "f32[3448, 1, 3, 512]" = torch.ops.aten.view.default(view_13, [3448, 1, 3, 512]);  view_13 = None
            unsqueeze_25: "f32[1, 3448, 1, 3, 512]" = torch.ops.aten.unsqueeze.default(view_14, 0);  view_14 = None
            transpose_5: "f32[3, 3448, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_25, 0, -2);  unsqueeze_25 = None
            squeeze: "f32[3, 3448, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_5, -2);  transpose_5 = None
            clone_3: "f32[3, 3448, 1, 512]" = torch.ops.aten.clone.default(squeeze, memory_format = torch.contiguous_format);  squeeze = None
            select: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_3, 0, 0)
            select_1: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_3, 0, 1)
            select_2: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_3, 0, 2);  clone_3 = None
            view_15: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select, [3448, 8, 64]);  select = None
            transpose_6: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_15, 0, 1);  view_15 = None
            view_16: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_1, [3448, 8, 64]);  select_1 = None
            transpose_7: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_16, 0, 1);  view_16 = None
            view_17: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_2, [3448, 8, 64]);  select_2 = None
            transpose_8: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_17, 0, 1);  view_17 = None
            view_18: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_6, [1, 8, 3448, 64]);  transpose_6 = None
            view_19: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_7, [1, 8, 3448, 64]);  transpose_7 = None
            view_20: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_8, [1, 8, 3448, 64]);  transpose_8 = None
            _scaled_dot_product_flash_attention_for_cpu = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_18, view_19, view_20);  view_18 = view_19 = view_20 = None
            getitem_9: "f32[1, 8, 3448, 64]" = _scaled_dot_product_flash_attention_for_cpu[0];  _scaled_dot_product_flash_attention_for_cpu = None
            permute_13: "f32[3448, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_9, [2, 0, 1, 3]);  getitem_9 = None
            view_21: "f32[3448, 512]" = torch.ops.aten.view.default(permute_13, [3448, 512]);  permute_13 = None
            t_2: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_0_self_attn_out_proj_weight);  p_crosstransformer_layers_0_self_attn_out_proj_weight = None
            addmm_1: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_0_self_attn_out_proj_bias, view_21, t_2);  p_crosstransformer_layers_0_self_attn_out_proj_bias = view_21 = t_2 = None
            view_22: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_1, [3448, 1, 512]);  addmm_1 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_9: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(view_22, 1, 0);  view_22 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_4: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(transpose_9);  transpose_9 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_31: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_0_gamma_1_scale, clone_4);  p_crosstransformer_layers_0_gamma_1_scale = clone_4 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:364 in forward, code: x = x + self.gamma_1(
            add_32: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(add_29, mul_31);  add_29 = mul_31 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_3 = torch.ops.aten.native_layer_norm.default(add_32, [512], p_crosstransformer_layers_0_norm2_weight, p_crosstransformer_layers_0_norm2_bias, 1e-05);  p_crosstransformer_layers_0_norm2_weight = p_crosstransformer_layers_0_norm2_bias = None
            getitem_11: "f32[1, 3448, 512]" = native_layer_norm_3[0];  native_layer_norm_3 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_23: "f32[3448, 512]" = torch.ops.aten.view.default(getitem_11, [3448, 512]);  getitem_11 = None
            t_3: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_0_linear1_weight);  p_crosstransformer_layers_0_linear1_weight = None
            addmm_2: "f32[3448, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_0_linear1_bias, view_23, t_3);  p_crosstransformer_layers_0_linear1_bias = view_23 = t_3 = None
            view_24: "f32[1, 3448, 2048]" = torch.ops.aten.view.default(addmm_2, [1, 3448, 2048]);  addmm_2 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            gelu_24: "f32[1, 3448, 2048]" = torch.ops.aten.gelu.default(view_24);  view_24 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_5: "f32[1, 3448, 2048]" = torch.ops.aten.clone.default(gelu_24);  gelu_24 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_25: "f32[3448, 2048]" = torch.ops.aten.view.default(clone_5, [3448, 2048]);  clone_5 = None
            t_4: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_0_linear2_weight);  p_crosstransformer_layers_0_linear2_weight = None
            addmm_3: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_0_linear2_bias, view_25, t_4);  p_crosstransformer_layers_0_linear2_bias = view_25 = t_4 = None
            view_26: "f32[1, 3448, 512]" = torch.ops.aten.view.default(addmm_3, [1, 3448, 512]);  addmm_3 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_6: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(view_26);  view_26 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_32: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_0_gamma_2_scale, clone_6);  p_crosstransformer_layers_0_gamma_2_scale = clone_6 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            add_33: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(add_32, mul_32);  add_32 = mul_32 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_10: "f32[1, 512, 3448]" = torch.ops.aten.transpose.int(add_33, 1, 2);  add_33 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_32: "f32[1, 512, 3448]" = torch.ops.aten.group_norm.default(transpose_10, 1, p_crosstransformer_layers_0_norm_out_weight, p_crosstransformer_layers_0_norm_out_bias);  transpose_10 = p_crosstransformer_layers_0_norm_out_weight = p_crosstransformer_layers_0_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_11: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(group_norm_32, 1, 2);  group_norm_32 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_4 = torch.ops.aten.native_layer_norm.default(add_31, [512], p_crosstransformer_layers_t_0_norm1_weight, p_crosstransformer_layers_t_0_norm1_bias, 1e-05);  p_crosstransformer_layers_t_0_norm1_weight = p_crosstransformer_layers_t_0_norm1_bias = None
            getitem_14: "f32[1, 1723, 512]" = native_layer_norm_4[0];  native_layer_norm_4 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1334 in forward, code: query = key = value = query.transpose(1, 0)
            transpose_12: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_14, 1, 0);  getitem_14 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            view_27: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_12, [1723, 512]);  transpose_12 = None
            t_5: "f32[512, 1536]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_0_self_attn_in_proj_weight);  p_crosstransformer_layers_t_0_self_attn_in_proj_weight = None
            addmm_4: "f32[1723, 1536]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_0_self_attn_in_proj_bias, view_27, t_5);  p_crosstransformer_layers_t_0_self_attn_in_proj_bias = view_27 = t_5 = None
            view_28: "f32[1723, 1, 1536]" = torch.ops.aten.view.default(addmm_4, [1723, 1, 1536]);  addmm_4 = None
            view_29: "f32[1723, 1, 3, 512]" = torch.ops.aten.view.default(view_28, [1723, 1, 3, 512]);  view_28 = None
            unsqueeze_26: "f32[1, 1723, 1, 3, 512]" = torch.ops.aten.unsqueeze.default(view_29, 0);  view_29 = None
            transpose_13: "f32[3, 1723, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_26, 0, -2);  unsqueeze_26 = None
            squeeze_1: "f32[3, 1723, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_13, -2);  transpose_13 = None
            clone_7: "f32[3, 1723, 1, 512]" = torch.ops.aten.clone.default(squeeze_1, memory_format = torch.contiguous_format);  squeeze_1 = None
            select_3: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_7, 0, 0)
            select_4: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_7, 0, 1)
            select_5: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_7, 0, 2);  clone_7 = None
            view_30: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_3, [1723, 8, 64]);  select_3 = None
            transpose_14: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_30, 0, 1);  view_30 = None
            view_31: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_4, [1723, 8, 64]);  select_4 = None
            transpose_15: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_31, 0, 1);  view_31 = None
            view_32: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_5, [1723, 8, 64]);  select_5 = None
            transpose_16: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_32, 0, 1);  view_32 = None
            view_33: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_14, [1, 8, 1723, 64]);  transpose_14 = None
            view_34: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_15, [1, 8, 1723, 64]);  transpose_15 = None
            view_35: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_16, [1, 8, 1723, 64]);  transpose_16 = None
            _scaled_dot_product_flash_attention_for_cpu_1 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_33, view_34, view_35);  view_33 = view_34 = view_35 = None
            getitem_17: "f32[1, 8, 1723, 64]" = _scaled_dot_product_flash_attention_for_cpu_1[0];  _scaled_dot_product_flash_attention_for_cpu_1 = None
            permute_14: "f32[1723, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_17, [2, 0, 1, 3]);  getitem_17 = None
            view_36: "f32[1723, 512]" = torch.ops.aten.view.default(permute_14, [1723, 512]);  permute_14 = None
            t_6: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_0_self_attn_out_proj_weight);  p_crosstransformer_layers_t_0_self_attn_out_proj_weight = None
            addmm_5: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_0_self_attn_out_proj_bias, view_36, t_6);  p_crosstransformer_layers_t_0_self_attn_out_proj_bias = view_36 = t_6 = None
            view_37: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_5, [1723, 1, 512]);  addmm_5 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_17: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(view_37, 1, 0);  view_37 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_8: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(transpose_17);  transpose_17 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_33: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_0_gamma_1_scale, clone_8);  p_crosstransformer_layers_t_0_gamma_1_scale = clone_8 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:364 in forward, code: x = x + self.gamma_1(
            add_34: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(add_31, mul_33);  add_31 = mul_33 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_5 = torch.ops.aten.native_layer_norm.default(add_34, [512], p_crosstransformer_layers_t_0_norm2_weight, p_crosstransformer_layers_t_0_norm2_bias, 1e-05);  p_crosstransformer_layers_t_0_norm2_weight = p_crosstransformer_layers_t_0_norm2_bias = None
            getitem_19: "f32[1, 1723, 512]" = native_layer_norm_5[0];  native_layer_norm_5 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_38: "f32[1723, 512]" = torch.ops.aten.view.default(getitem_19, [1723, 512]);  getitem_19 = None
            t_7: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_0_linear1_weight);  p_crosstransformer_layers_t_0_linear1_weight = None
            addmm_6: "f32[1723, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_0_linear1_bias, view_38, t_7);  p_crosstransformer_layers_t_0_linear1_bias = view_38 = t_7 = None
            view_39: "f32[1, 1723, 2048]" = torch.ops.aten.view.default(addmm_6, [1, 1723, 2048]);  addmm_6 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            gelu_25: "f32[1, 1723, 2048]" = torch.ops.aten.gelu.default(view_39);  view_39 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_9: "f32[1, 1723, 2048]" = torch.ops.aten.clone.default(gelu_25);  gelu_25 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_40: "f32[1723, 2048]" = torch.ops.aten.view.default(clone_9, [1723, 2048]);  clone_9 = None
            t_8: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_0_linear2_weight);  p_crosstransformer_layers_t_0_linear2_weight = None
            addmm_7: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_0_linear2_bias, view_40, t_8);  p_crosstransformer_layers_t_0_linear2_bias = view_40 = t_8 = None
            view_41: "f32[1, 1723, 512]" = torch.ops.aten.view.default(addmm_7, [1, 1723, 512]);  addmm_7 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_10: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(view_41);  view_41 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_34: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_0_gamma_2_scale, clone_10);  p_crosstransformer_layers_t_0_gamma_2_scale = clone_10 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            add_35: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(add_34, mul_34);  add_34 = mul_34 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_18: "f32[1, 512, 1723]" = torch.ops.aten.transpose.int(add_35, 1, 2);  add_35 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_33: "f32[1, 512, 1723]" = torch.ops.aten.group_norm.default(transpose_18, 1, p_crosstransformer_layers_t_0_norm_out_weight, p_crosstransformer_layers_t_0_norm_out_bias);  transpose_18 = p_crosstransformer_layers_t_0_norm_out_weight = p_crosstransformer_layers_t_0_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_19: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(group_norm_33, 1, 2);  group_norm_33 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_6 = torch.ops.aten.native_layer_norm.default(transpose_11, [512], p_crosstransformer_layers_1_norm1_weight, p_crosstransformer_layers_1_norm1_bias, 1e-05);  p_crosstransformer_layers_1_norm1_weight = p_crosstransformer_layers_1_norm1_bias = None
            getitem_22: "f32[1, 3448, 512]" = native_layer_norm_6[0];  native_layer_norm_6 = None
            native_layer_norm_7 = torch.ops.aten.native_layer_norm.default(transpose_19, [512], p_crosstransformer_layers_1_norm2_weight, p_crosstransformer_layers_1_norm2_bias, 1e-05);  p_crosstransformer_layers_1_norm2_weight = p_crosstransformer_layers_1_norm2_bias = None
            getitem_25: "f32[1, 1723, 512]" = native_layer_norm_7[0];  native_layer_norm_7 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1336 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))
            transpose_20: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_22, 1, 0);  getitem_22 = None
            transpose_21: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_25, 1, 0);  getitem_25 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            split_with_sizes = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_1_cross_attn_in_proj_weight, [512, 1024]);  p_crosstransformer_layers_1_cross_attn_in_proj_weight = None
            getitem_28: "f32[512, 512]" = split_with_sizes[0]
            getitem_29: "f32[1024, 512]" = split_with_sizes[1];  split_with_sizes = None
            split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_1_cross_attn_in_proj_bias, [512, 1024]);  p_crosstransformer_layers_1_cross_attn_in_proj_bias = None
            getitem_30: "f32[512]" = split_with_sizes_1[0]
            getitem_31: "f32[1024]" = split_with_sizes_1[1];  split_with_sizes_1 = None
            view_42: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_20, [3448, 512]);  transpose_20 = None
            t_9: "f32[512, 512]" = torch.ops.aten.t.default(getitem_28);  getitem_28 = None
            addmm_8: "f32[3448, 512]" = torch.ops.aten.addmm.default(getitem_30, view_42, t_9);  getitem_30 = view_42 = t_9 = None
            view_43: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_8, [3448, 1, 512]);  addmm_8 = None
            view_44: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_21, [1723, 512]);  transpose_21 = None
            t_10: "f32[512, 1024]" = torch.ops.aten.t.default(getitem_29);  getitem_29 = None
            addmm_9: "f32[1723, 1024]" = torch.ops.aten.addmm.default(getitem_31, view_44, t_10);  getitem_31 = view_44 = t_10 = None
            view_45: "f32[1723, 1, 1024]" = torch.ops.aten.view.default(addmm_9, [1723, 1, 1024]);  addmm_9 = None
            view_46: "f32[1723, 1, 2, 512]" = torch.ops.aten.view.default(view_45, [1723, 1, 2, 512]);  view_45 = None
            unsqueeze_27: "f32[1, 1723, 1, 2, 512]" = torch.ops.aten.unsqueeze.default(view_46, 0);  view_46 = None
            transpose_22: "f32[2, 1723, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_27, 0, -2);  unsqueeze_27 = None
            squeeze_2: "f32[2, 1723, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_22, -2);  transpose_22 = None
            clone_11: "f32[2, 1723, 1, 512]" = torch.ops.aten.clone.default(squeeze_2, memory_format = torch.contiguous_format);  squeeze_2 = None
            select_6: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_11, 0, 0)
            select_7: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_11, 0, 1);  clone_11 = None
            view_47: "f32[3448, 8, 64]" = torch.ops.aten.view.default(view_43, [3448, 8, 64]);  view_43 = None
            transpose_23: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_47, 0, 1);  view_47 = None
            view_48: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_6, [1723, 8, 64]);  select_6 = None
            transpose_24: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_48, 0, 1);  view_48 = None
            view_49: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_7, [1723, 8, 64]);  select_7 = None
            transpose_25: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_49, 0, 1);  view_49 = None
            view_50: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_23, [1, 8, 3448, 64]);  transpose_23 = None
            view_51: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_24, [1, 8, 1723, 64]);  transpose_24 = None
            view_52: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_25, [1, 8, 1723, 64]);  transpose_25 = None
            _scaled_dot_product_flash_attention_for_cpu_2 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_50, view_51, view_52);  view_50 = view_51 = view_52 = None
            getitem_32: "f32[1, 8, 3448, 64]" = _scaled_dot_product_flash_attention_for_cpu_2[0];  _scaled_dot_product_flash_attention_for_cpu_2 = None
            permute_15: "f32[3448, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_32, [2, 0, 1, 3]);  getitem_32 = None
            view_53: "f32[3448, 512]" = torch.ops.aten.view.default(permute_15, [3448, 512]);  permute_15 = None
            t_11: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_1_cross_attn_out_proj_weight);  p_crosstransformer_layers_1_cross_attn_out_proj_weight = None
            addmm_10: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_1_cross_attn_out_proj_bias, view_53, t_11);  p_crosstransformer_layers_1_cross_attn_out_proj_bias = view_53 = t_11 = None
            view_54: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_10, [3448, 1, 512]);  addmm_10 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_26: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(view_54, 1, 0);  view_54 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_12: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(transpose_26);  transpose_26 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_35: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_1_gamma_1_scale, clone_12);  p_crosstransformer_layers_1_gamma_1_scale = clone_12 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:494 in forward, code: x = q + self.gamma_1(self._ca_block(self.norm1(q), self.norm2(k), mask))
            add_36: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(transpose_11, mul_35);  mul_35 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_8 = torch.ops.aten.native_layer_norm.default(add_36, [512], p_crosstransformer_layers_1_norm3_weight, p_crosstransformer_layers_1_norm3_bias, 1e-05);  p_crosstransformer_layers_1_norm3_weight = p_crosstransformer_layers_1_norm3_bias = None
            getitem_34: "f32[1, 3448, 512]" = native_layer_norm_8[0];  native_layer_norm_8 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_55: "f32[3448, 512]" = torch.ops.aten.view.default(getitem_34, [3448, 512]);  getitem_34 = None
            t_12: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_1_linear1_weight);  p_crosstransformer_layers_1_linear1_weight = None
            addmm_11: "f32[3448, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_1_linear1_bias, view_55, t_12);  p_crosstransformer_layers_1_linear1_bias = view_55 = t_12 = None
            view_56: "f32[1, 3448, 2048]" = torch.ops.aten.view.default(addmm_11, [1, 3448, 2048]);  addmm_11 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            gelu_26: "f32[1, 3448, 2048]" = torch.ops.aten.gelu.default(view_56);  view_56 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_13: "f32[1, 3448, 2048]" = torch.ops.aten.clone.default(gelu_26);  gelu_26 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_57: "f32[3448, 2048]" = torch.ops.aten.view.default(clone_13, [3448, 2048]);  clone_13 = None
            t_13: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_1_linear2_weight);  p_crosstransformer_layers_1_linear2_weight = None
            addmm_12: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_1_linear2_bias, view_57, t_13);  p_crosstransformer_layers_1_linear2_bias = view_57 = t_13 = None
            view_58: "f32[1, 3448, 512]" = torch.ops.aten.view.default(addmm_12, [1, 3448, 512]);  addmm_12 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_14: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(view_58);  view_58 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_36: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_1_gamma_2_scale, clone_14);  p_crosstransformer_layers_1_gamma_2_scale = clone_14 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            add_37: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(add_36, mul_36);  add_36 = mul_36 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_27: "f32[1, 512, 3448]" = torch.ops.aten.transpose.int(add_37, 1, 2);  add_37 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_34: "f32[1, 512, 3448]" = torch.ops.aten.group_norm.default(transpose_27, 1, p_crosstransformer_layers_1_norm_out_weight, p_crosstransformer_layers_1_norm_out_bias);  transpose_27 = p_crosstransformer_layers_1_norm_out_weight = p_crosstransformer_layers_1_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_28: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(group_norm_34, 1, 2);  group_norm_34 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_9 = torch.ops.aten.native_layer_norm.default(transpose_19, [512], p_crosstransformer_layers_t_1_norm1_weight, p_crosstransformer_layers_t_1_norm1_bias, 1e-05);  p_crosstransformer_layers_t_1_norm1_weight = p_crosstransformer_layers_t_1_norm1_bias = None
            getitem_37: "f32[1, 1723, 512]" = native_layer_norm_9[0];  native_layer_norm_9 = None
            native_layer_norm_10 = torch.ops.aten.native_layer_norm.default(transpose_11, [512], p_crosstransformer_layers_t_1_norm2_weight, p_crosstransformer_layers_t_1_norm2_bias, 1e-05);  transpose_11 = p_crosstransformer_layers_t_1_norm2_weight = p_crosstransformer_layers_t_1_norm2_bias = None
            getitem_40: "f32[1, 3448, 512]" = native_layer_norm_10[0];  native_layer_norm_10 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1336 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))
            transpose_29: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_37, 1, 0);  getitem_37 = None
            transpose_30: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_40, 1, 0);  getitem_40 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_t_1_cross_attn_in_proj_weight, [512, 1024]);  p_crosstransformer_layers_t_1_cross_attn_in_proj_weight = None
            getitem_43: "f32[512, 512]" = split_with_sizes_2[0]
            getitem_44: "f32[1024, 512]" = split_with_sizes_2[1];  split_with_sizes_2 = None
            split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_t_1_cross_attn_in_proj_bias, [512, 1024]);  p_crosstransformer_layers_t_1_cross_attn_in_proj_bias = None
            getitem_45: "f32[512]" = split_with_sizes_3[0]
            getitem_46: "f32[1024]" = split_with_sizes_3[1];  split_with_sizes_3 = None
            view_59: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_29, [1723, 512]);  transpose_29 = None
            t_14: "f32[512, 512]" = torch.ops.aten.t.default(getitem_43);  getitem_43 = None
            addmm_13: "f32[1723, 512]" = torch.ops.aten.addmm.default(getitem_45, view_59, t_14);  getitem_45 = view_59 = t_14 = None
            view_60: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_13, [1723, 1, 512]);  addmm_13 = None
            view_61: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_30, [3448, 512]);  transpose_30 = None
            t_15: "f32[512, 1024]" = torch.ops.aten.t.default(getitem_44);  getitem_44 = None
            addmm_14: "f32[3448, 1024]" = torch.ops.aten.addmm.default(getitem_46, view_61, t_15);  getitem_46 = view_61 = t_15 = None
            view_62: "f32[3448, 1, 1024]" = torch.ops.aten.view.default(addmm_14, [3448, 1, 1024]);  addmm_14 = None
            view_63: "f32[3448, 1, 2, 512]" = torch.ops.aten.view.default(view_62, [3448, 1, 2, 512]);  view_62 = None
            unsqueeze_28: "f32[1, 3448, 1, 2, 512]" = torch.ops.aten.unsqueeze.default(view_63, 0);  view_63 = None
            transpose_31: "f32[2, 3448, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_28, 0, -2);  unsqueeze_28 = None
            squeeze_3: "f32[2, 3448, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_31, -2);  transpose_31 = None
            clone_15: "f32[2, 3448, 1, 512]" = torch.ops.aten.clone.default(squeeze_3, memory_format = torch.contiguous_format);  squeeze_3 = None
            select_8: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_15, 0, 0)
            select_9: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_15, 0, 1);  clone_15 = None
            view_64: "f32[1723, 8, 64]" = torch.ops.aten.view.default(view_60, [1723, 8, 64]);  view_60 = None
            transpose_32: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_64, 0, 1);  view_64 = None
            view_65: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_8, [3448, 8, 64]);  select_8 = None
            transpose_33: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_65, 0, 1);  view_65 = None
            view_66: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_9, [3448, 8, 64]);  select_9 = None
            transpose_34: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_66, 0, 1);  view_66 = None
            view_67: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_32, [1, 8, 1723, 64]);  transpose_32 = None
            view_68: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_33, [1, 8, 3448, 64]);  transpose_33 = None
            view_69: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_34, [1, 8, 3448, 64]);  transpose_34 = None
            _scaled_dot_product_flash_attention_for_cpu_3 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_67, view_68, view_69);  view_67 = view_68 = view_69 = None
            getitem_47: "f32[1, 8, 1723, 64]" = _scaled_dot_product_flash_attention_for_cpu_3[0];  _scaled_dot_product_flash_attention_for_cpu_3 = None
            permute_16: "f32[1723, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_47, [2, 0, 1, 3]);  getitem_47 = None
            view_70: "f32[1723, 512]" = torch.ops.aten.view.default(permute_16, [1723, 512]);  permute_16 = None
            t_16: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_1_cross_attn_out_proj_weight);  p_crosstransformer_layers_t_1_cross_attn_out_proj_weight = None
            addmm_15: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_1_cross_attn_out_proj_bias, view_70, t_16);  p_crosstransformer_layers_t_1_cross_attn_out_proj_bias = view_70 = t_16 = None
            view_71: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_15, [1723, 1, 512]);  addmm_15 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_35: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(view_71, 1, 0);  view_71 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_16: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(transpose_35);  transpose_35 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_37: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_1_gamma_1_scale, clone_16);  p_crosstransformer_layers_t_1_gamma_1_scale = clone_16 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:494 in forward, code: x = q + self.gamma_1(self._ca_block(self.norm1(q), self.norm2(k), mask))
            add_38: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(transpose_19, mul_37);  transpose_19 = mul_37 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_11 = torch.ops.aten.native_layer_norm.default(add_38, [512], p_crosstransformer_layers_t_1_norm3_weight, p_crosstransformer_layers_t_1_norm3_bias, 1e-05);  p_crosstransformer_layers_t_1_norm3_weight = p_crosstransformer_layers_t_1_norm3_bias = None
            getitem_49: "f32[1, 1723, 512]" = native_layer_norm_11[0];  native_layer_norm_11 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_72: "f32[1723, 512]" = torch.ops.aten.view.default(getitem_49, [1723, 512]);  getitem_49 = None
            t_17: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_1_linear1_weight);  p_crosstransformer_layers_t_1_linear1_weight = None
            addmm_16: "f32[1723, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_1_linear1_bias, view_72, t_17);  p_crosstransformer_layers_t_1_linear1_bias = view_72 = t_17 = None
            view_73: "f32[1, 1723, 2048]" = torch.ops.aten.view.default(addmm_16, [1, 1723, 2048]);  addmm_16 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            gelu_27: "f32[1, 1723, 2048]" = torch.ops.aten.gelu.default(view_73);  view_73 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_17: "f32[1, 1723, 2048]" = torch.ops.aten.clone.default(gelu_27);  gelu_27 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_74: "f32[1723, 2048]" = torch.ops.aten.view.default(clone_17, [1723, 2048]);  clone_17 = None
            t_18: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_1_linear2_weight);  p_crosstransformer_layers_t_1_linear2_weight = None
            addmm_17: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_1_linear2_bias, view_74, t_18);  p_crosstransformer_layers_t_1_linear2_bias = view_74 = t_18 = None
            view_75: "f32[1, 1723, 512]" = torch.ops.aten.view.default(addmm_17, [1, 1723, 512]);  addmm_17 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_18: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(view_75);  view_75 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_38: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_1_gamma_2_scale, clone_18);  p_crosstransformer_layers_t_1_gamma_2_scale = clone_18 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            add_39: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(add_38, mul_38);  add_38 = mul_38 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_36: "f32[1, 512, 1723]" = torch.ops.aten.transpose.int(add_39, 1, 2);  add_39 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_35: "f32[1, 512, 1723]" = torch.ops.aten.group_norm.default(transpose_36, 1, p_crosstransformer_layers_t_1_norm_out_weight, p_crosstransformer_layers_t_1_norm_out_bias);  transpose_36 = p_crosstransformer_layers_t_1_norm_out_weight = p_crosstransformer_layers_t_1_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_37: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(group_norm_35, 1, 2);  group_norm_35 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_12 = torch.ops.aten.native_layer_norm.default(transpose_28, [512], p_crosstransformer_layers_2_norm1_weight, p_crosstransformer_layers_2_norm1_bias, 1e-05);  p_crosstransformer_layers_2_norm1_weight = p_crosstransformer_layers_2_norm1_bias = None
            getitem_52: "f32[1, 3448, 512]" = native_layer_norm_12[0];  native_layer_norm_12 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1334 in forward, code: query = key = value = query.transpose(1, 0)
            transpose_38: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_52, 1, 0);  getitem_52 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            view_76: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_38, [3448, 512]);  transpose_38 = None
            t_19: "f32[512, 1536]" = torch.ops.aten.t.default(p_crosstransformer_layers_2_self_attn_in_proj_weight);  p_crosstransformer_layers_2_self_attn_in_proj_weight = None
            addmm_18: "f32[3448, 1536]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_2_self_attn_in_proj_bias, view_76, t_19);  p_crosstransformer_layers_2_self_attn_in_proj_bias = view_76 = t_19 = None
            view_77: "f32[3448, 1, 1536]" = torch.ops.aten.view.default(addmm_18, [3448, 1, 1536]);  addmm_18 = None
            view_78: "f32[3448, 1, 3, 512]" = torch.ops.aten.view.default(view_77, [3448, 1, 3, 512]);  view_77 = None
            unsqueeze_29: "f32[1, 3448, 1, 3, 512]" = torch.ops.aten.unsqueeze.default(view_78, 0);  view_78 = None
            transpose_39: "f32[3, 3448, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_29, 0, -2);  unsqueeze_29 = None
            squeeze_4: "f32[3, 3448, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_39, -2);  transpose_39 = None
            clone_19: "f32[3, 3448, 1, 512]" = torch.ops.aten.clone.default(squeeze_4, memory_format = torch.contiguous_format);  squeeze_4 = None
            select_10: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_19, 0, 0)
            select_11: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_19, 0, 1)
            select_12: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_19, 0, 2);  clone_19 = None
            view_79: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_10, [3448, 8, 64]);  select_10 = None
            transpose_40: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_79, 0, 1);  view_79 = None
            view_80: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_11, [3448, 8, 64]);  select_11 = None
            transpose_41: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_80, 0, 1);  view_80 = None
            view_81: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_12, [3448, 8, 64]);  select_12 = None
            transpose_42: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_81, 0, 1);  view_81 = None
            view_82: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_40, [1, 8, 3448, 64]);  transpose_40 = None
            view_83: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_41, [1, 8, 3448, 64]);  transpose_41 = None
            view_84: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_42, [1, 8, 3448, 64]);  transpose_42 = None
            _scaled_dot_product_flash_attention_for_cpu_4 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_82, view_83, view_84);  view_82 = view_83 = view_84 = None
            getitem_55: "f32[1, 8, 3448, 64]" = _scaled_dot_product_flash_attention_for_cpu_4[0];  _scaled_dot_product_flash_attention_for_cpu_4 = None
            permute_17: "f32[3448, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_55, [2, 0, 1, 3]);  getitem_55 = None
            view_85: "f32[3448, 512]" = torch.ops.aten.view.default(permute_17, [3448, 512]);  permute_17 = None
            t_20: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_2_self_attn_out_proj_weight);  p_crosstransformer_layers_2_self_attn_out_proj_weight = None
            addmm_19: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_2_self_attn_out_proj_bias, view_85, t_20);  p_crosstransformer_layers_2_self_attn_out_proj_bias = view_85 = t_20 = None
            view_86: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_19, [3448, 1, 512]);  addmm_19 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_43: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(view_86, 1, 0);  view_86 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_20: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(transpose_43);  transpose_43 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_39: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_2_gamma_1_scale, clone_20);  p_crosstransformer_layers_2_gamma_1_scale = clone_20 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:364 in forward, code: x = x + self.gamma_1(
            add_40: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(transpose_28, mul_39);  transpose_28 = mul_39 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_13 = torch.ops.aten.native_layer_norm.default(add_40, [512], p_crosstransformer_layers_2_norm2_weight, p_crosstransformer_layers_2_norm2_bias, 1e-05);  p_crosstransformer_layers_2_norm2_weight = p_crosstransformer_layers_2_norm2_bias = None
            getitem_57: "f32[1, 3448, 512]" = native_layer_norm_13[0];  native_layer_norm_13 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_87: "f32[3448, 512]" = torch.ops.aten.view.default(getitem_57, [3448, 512]);  getitem_57 = None
            t_21: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_2_linear1_weight);  p_crosstransformer_layers_2_linear1_weight = None
            addmm_20: "f32[3448, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_2_linear1_bias, view_87, t_21);  p_crosstransformer_layers_2_linear1_bias = view_87 = t_21 = None
            view_88: "f32[1, 3448, 2048]" = torch.ops.aten.view.default(addmm_20, [1, 3448, 2048]);  addmm_20 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            gelu_28: "f32[1, 3448, 2048]" = torch.ops.aten.gelu.default(view_88);  view_88 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_21: "f32[1, 3448, 2048]" = torch.ops.aten.clone.default(gelu_28);  gelu_28 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_89: "f32[3448, 2048]" = torch.ops.aten.view.default(clone_21, [3448, 2048]);  clone_21 = None
            t_22: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_2_linear2_weight);  p_crosstransformer_layers_2_linear2_weight = None
            addmm_21: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_2_linear2_bias, view_89, t_22);  p_crosstransformer_layers_2_linear2_bias = view_89 = t_22 = None
            view_90: "f32[1, 3448, 512]" = torch.ops.aten.view.default(addmm_21, [1, 3448, 512]);  addmm_21 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_22: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(view_90);  view_90 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_40: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_2_gamma_2_scale, clone_22);  p_crosstransformer_layers_2_gamma_2_scale = clone_22 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            add_41: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(add_40, mul_40);  add_40 = mul_40 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_44: "f32[1, 512, 3448]" = torch.ops.aten.transpose.int(add_41, 1, 2);  add_41 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_36: "f32[1, 512, 3448]" = torch.ops.aten.group_norm.default(transpose_44, 1, p_crosstransformer_layers_2_norm_out_weight, p_crosstransformer_layers_2_norm_out_bias);  transpose_44 = p_crosstransformer_layers_2_norm_out_weight = p_crosstransformer_layers_2_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_45: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(group_norm_36, 1, 2);  group_norm_36 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_14 = torch.ops.aten.native_layer_norm.default(transpose_37, [512], p_crosstransformer_layers_t_2_norm1_weight, p_crosstransformer_layers_t_2_norm1_bias, 1e-05);  p_crosstransformer_layers_t_2_norm1_weight = p_crosstransformer_layers_t_2_norm1_bias = None
            getitem_60: "f32[1, 1723, 512]" = native_layer_norm_14[0];  native_layer_norm_14 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1334 in forward, code: query = key = value = query.transpose(1, 0)
            transpose_46: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_60, 1, 0);  getitem_60 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            view_91: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_46, [1723, 512]);  transpose_46 = None
            t_23: "f32[512, 1536]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_2_self_attn_in_proj_weight);  p_crosstransformer_layers_t_2_self_attn_in_proj_weight = None
            addmm_22: "f32[1723, 1536]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_2_self_attn_in_proj_bias, view_91, t_23);  p_crosstransformer_layers_t_2_self_attn_in_proj_bias = view_91 = t_23 = None
            view_92: "f32[1723, 1, 1536]" = torch.ops.aten.view.default(addmm_22, [1723, 1, 1536]);  addmm_22 = None
            view_93: "f32[1723, 1, 3, 512]" = torch.ops.aten.view.default(view_92, [1723, 1, 3, 512]);  view_92 = None
            unsqueeze_30: "f32[1, 1723, 1, 3, 512]" = torch.ops.aten.unsqueeze.default(view_93, 0);  view_93 = None
            transpose_47: "f32[3, 1723, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_30, 0, -2);  unsqueeze_30 = None
            squeeze_5: "f32[3, 1723, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_47, -2);  transpose_47 = None
            clone_23: "f32[3, 1723, 1, 512]" = torch.ops.aten.clone.default(squeeze_5, memory_format = torch.contiguous_format);  squeeze_5 = None
            select_13: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_23, 0, 0)
            select_14: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_23, 0, 1)
            select_15: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_23, 0, 2);  clone_23 = None
            view_94: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_13, [1723, 8, 64]);  select_13 = None
            transpose_48: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_94, 0, 1);  view_94 = None
            view_95: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_14, [1723, 8, 64]);  select_14 = None
            transpose_49: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_95, 0, 1);  view_95 = None
            view_96: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_15, [1723, 8, 64]);  select_15 = None
            transpose_50: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_96, 0, 1);  view_96 = None
            view_97: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_48, [1, 8, 1723, 64]);  transpose_48 = None
            view_98: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_49, [1, 8, 1723, 64]);  transpose_49 = None
            view_99: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_50, [1, 8, 1723, 64]);  transpose_50 = None
            _scaled_dot_product_flash_attention_for_cpu_5 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_97, view_98, view_99);  view_97 = view_98 = view_99 = None
            getitem_63: "f32[1, 8, 1723, 64]" = _scaled_dot_product_flash_attention_for_cpu_5[0];  _scaled_dot_product_flash_attention_for_cpu_5 = None
            permute_18: "f32[1723, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_63, [2, 0, 1, 3]);  getitem_63 = None
            view_100: "f32[1723, 512]" = torch.ops.aten.view.default(permute_18, [1723, 512]);  permute_18 = None
            t_24: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_2_self_attn_out_proj_weight);  p_crosstransformer_layers_t_2_self_attn_out_proj_weight = None
            addmm_23: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_2_self_attn_out_proj_bias, view_100, t_24);  p_crosstransformer_layers_t_2_self_attn_out_proj_bias = view_100 = t_24 = None
            view_101: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_23, [1723, 1, 512]);  addmm_23 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_51: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(view_101, 1, 0);  view_101 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_24: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(transpose_51);  transpose_51 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_41: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_2_gamma_1_scale, clone_24);  p_crosstransformer_layers_t_2_gamma_1_scale = clone_24 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:364 in forward, code: x = x + self.gamma_1(
            add_42: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(transpose_37, mul_41);  transpose_37 = mul_41 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_15 = torch.ops.aten.native_layer_norm.default(add_42, [512], p_crosstransformer_layers_t_2_norm2_weight, p_crosstransformer_layers_t_2_norm2_bias, 1e-05);  p_crosstransformer_layers_t_2_norm2_weight = p_crosstransformer_layers_t_2_norm2_bias = None
            getitem_65: "f32[1, 1723, 512]" = native_layer_norm_15[0];  native_layer_norm_15 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_102: "f32[1723, 512]" = torch.ops.aten.view.default(getitem_65, [1723, 512]);  getitem_65 = None
            t_25: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_2_linear1_weight);  p_crosstransformer_layers_t_2_linear1_weight = None
            addmm_24: "f32[1723, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_2_linear1_bias, view_102, t_25);  p_crosstransformer_layers_t_2_linear1_bias = view_102 = t_25 = None
            view_103: "f32[1, 1723, 2048]" = torch.ops.aten.view.default(addmm_24, [1, 1723, 2048]);  addmm_24 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            gelu_29: "f32[1, 1723, 2048]" = torch.ops.aten.gelu.default(view_103);  view_103 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_25: "f32[1, 1723, 2048]" = torch.ops.aten.clone.default(gelu_29);  gelu_29 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_104: "f32[1723, 2048]" = torch.ops.aten.view.default(clone_25, [1723, 2048]);  clone_25 = None
            t_26: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_2_linear2_weight);  p_crosstransformer_layers_t_2_linear2_weight = None
            addmm_25: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_2_linear2_bias, view_104, t_26);  p_crosstransformer_layers_t_2_linear2_bias = view_104 = t_26 = None
            view_105: "f32[1, 1723, 512]" = torch.ops.aten.view.default(addmm_25, [1, 1723, 512]);  addmm_25 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_26: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(view_105);  view_105 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_42: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_2_gamma_2_scale, clone_26);  p_crosstransformer_layers_t_2_gamma_2_scale = clone_26 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            add_43: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(add_42, mul_42);  add_42 = mul_42 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_52: "f32[1, 512, 1723]" = torch.ops.aten.transpose.int(add_43, 1, 2);  add_43 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_37: "f32[1, 512, 1723]" = torch.ops.aten.group_norm.default(transpose_52, 1, p_crosstransformer_layers_t_2_norm_out_weight, p_crosstransformer_layers_t_2_norm_out_bias);  transpose_52 = p_crosstransformer_layers_t_2_norm_out_weight = p_crosstransformer_layers_t_2_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_53: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(group_norm_37, 1, 2);  group_norm_37 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_16 = torch.ops.aten.native_layer_norm.default(transpose_45, [512], p_crosstransformer_layers_3_norm1_weight, p_crosstransformer_layers_3_norm1_bias, 1e-05);  p_crosstransformer_layers_3_norm1_weight = p_crosstransformer_layers_3_norm1_bias = None
            getitem_68: "f32[1, 3448, 512]" = native_layer_norm_16[0];  native_layer_norm_16 = None
            native_layer_norm_17 = torch.ops.aten.native_layer_norm.default(transpose_53, [512], p_crosstransformer_layers_3_norm2_weight, p_crosstransformer_layers_3_norm2_bias, 1e-05);  p_crosstransformer_layers_3_norm2_weight = p_crosstransformer_layers_3_norm2_bias = None
            getitem_71: "f32[1, 1723, 512]" = native_layer_norm_17[0];  native_layer_norm_17 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1336 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))
            transpose_54: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_68, 1, 0);  getitem_68 = None
            transpose_55: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_71, 1, 0);  getitem_71 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_3_cross_attn_in_proj_weight, [512, 1024]);  p_crosstransformer_layers_3_cross_attn_in_proj_weight = None
            getitem_74: "f32[512, 512]" = split_with_sizes_4[0]
            getitem_75: "f32[1024, 512]" = split_with_sizes_4[1];  split_with_sizes_4 = None
            split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_3_cross_attn_in_proj_bias, [512, 1024]);  p_crosstransformer_layers_3_cross_attn_in_proj_bias = None
            getitem_76: "f32[512]" = split_with_sizes_5[0]
            getitem_77: "f32[1024]" = split_with_sizes_5[1];  split_with_sizes_5 = None
            view_106: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_54, [3448, 512]);  transpose_54 = None
            t_27: "f32[512, 512]" = torch.ops.aten.t.default(getitem_74);  getitem_74 = None
            addmm_26: "f32[3448, 512]" = torch.ops.aten.addmm.default(getitem_76, view_106, t_27);  getitem_76 = view_106 = t_27 = None
            view_107: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_26, [3448, 1, 512]);  addmm_26 = None
            view_108: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_55, [1723, 512]);  transpose_55 = None
            t_28: "f32[512, 1024]" = torch.ops.aten.t.default(getitem_75);  getitem_75 = None
            addmm_27: "f32[1723, 1024]" = torch.ops.aten.addmm.default(getitem_77, view_108, t_28);  getitem_77 = view_108 = t_28 = None
            view_109: "f32[1723, 1, 1024]" = torch.ops.aten.view.default(addmm_27, [1723, 1, 1024]);  addmm_27 = None
            view_110: "f32[1723, 1, 2, 512]" = torch.ops.aten.view.default(view_109, [1723, 1, 2, 512]);  view_109 = None
            unsqueeze_31: "f32[1, 1723, 1, 2, 512]" = torch.ops.aten.unsqueeze.default(view_110, 0);  view_110 = None
            transpose_56: "f32[2, 1723, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_31, 0, -2);  unsqueeze_31 = None
            squeeze_6: "f32[2, 1723, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_56, -2);  transpose_56 = None
            clone_27: "f32[2, 1723, 1, 512]" = torch.ops.aten.clone.default(squeeze_6, memory_format = torch.contiguous_format);  squeeze_6 = None
            select_16: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_27, 0, 0)
            select_17: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_27, 0, 1);  clone_27 = None
            view_111: "f32[3448, 8, 64]" = torch.ops.aten.view.default(view_107, [3448, 8, 64]);  view_107 = None
            transpose_57: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_111, 0, 1);  view_111 = None
            view_112: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_16, [1723, 8, 64]);  select_16 = None
            transpose_58: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_112, 0, 1);  view_112 = None
            view_113: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_17, [1723, 8, 64]);  select_17 = None
            transpose_59: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_113, 0, 1);  view_113 = None
            view_114: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_57, [1, 8, 3448, 64]);  transpose_57 = None
            view_115: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_58, [1, 8, 1723, 64]);  transpose_58 = None
            view_116: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_59, [1, 8, 1723, 64]);  transpose_59 = None
            _scaled_dot_product_flash_attention_for_cpu_6 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_114, view_115, view_116);  view_114 = view_115 = view_116 = None
            getitem_78: "f32[1, 8, 3448, 64]" = _scaled_dot_product_flash_attention_for_cpu_6[0];  _scaled_dot_product_flash_attention_for_cpu_6 = None
            permute_19: "f32[3448, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_78, [2, 0, 1, 3]);  getitem_78 = None
            view_117: "f32[3448, 512]" = torch.ops.aten.view.default(permute_19, [3448, 512]);  permute_19 = None
            t_29: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_3_cross_attn_out_proj_weight);  p_crosstransformer_layers_3_cross_attn_out_proj_weight = None
            addmm_28: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_3_cross_attn_out_proj_bias, view_117, t_29);  p_crosstransformer_layers_3_cross_attn_out_proj_bias = view_117 = t_29 = None
            view_118: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_28, [3448, 1, 512]);  addmm_28 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_60: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(view_118, 1, 0);  view_118 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_28: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(transpose_60);  transpose_60 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_43: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_3_gamma_1_scale, clone_28);  p_crosstransformer_layers_3_gamma_1_scale = clone_28 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:494 in forward, code: x = q + self.gamma_1(self._ca_block(self.norm1(q), self.norm2(k), mask))
            add_44: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(transpose_45, mul_43);  mul_43 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_18 = torch.ops.aten.native_layer_norm.default(add_44, [512], p_crosstransformer_layers_3_norm3_weight, p_crosstransformer_layers_3_norm3_bias, 1e-05);  p_crosstransformer_layers_3_norm3_weight = p_crosstransformer_layers_3_norm3_bias = None
            getitem_80: "f32[1, 3448, 512]" = native_layer_norm_18[0];  native_layer_norm_18 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_119: "f32[3448, 512]" = torch.ops.aten.view.default(getitem_80, [3448, 512]);  getitem_80 = None
            t_30: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_3_linear1_weight);  p_crosstransformer_layers_3_linear1_weight = None
            addmm_29: "f32[3448, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_3_linear1_bias, view_119, t_30);  p_crosstransformer_layers_3_linear1_bias = view_119 = t_30 = None
            view_120: "f32[1, 3448, 2048]" = torch.ops.aten.view.default(addmm_29, [1, 3448, 2048]);  addmm_29 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            gelu_30: "f32[1, 3448, 2048]" = torch.ops.aten.gelu.default(view_120);  view_120 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_29: "f32[1, 3448, 2048]" = torch.ops.aten.clone.default(gelu_30);  gelu_30 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_121: "f32[3448, 2048]" = torch.ops.aten.view.default(clone_29, [3448, 2048]);  clone_29 = None
            t_31: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_3_linear2_weight);  p_crosstransformer_layers_3_linear2_weight = None
            addmm_30: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_3_linear2_bias, view_121, t_31);  p_crosstransformer_layers_3_linear2_bias = view_121 = t_31 = None
            view_122: "f32[1, 3448, 512]" = torch.ops.aten.view.default(addmm_30, [1, 3448, 512]);  addmm_30 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_30: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(view_122);  view_122 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_44: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_3_gamma_2_scale, clone_30);  p_crosstransformer_layers_3_gamma_2_scale = clone_30 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            add_45: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(add_44, mul_44);  add_44 = mul_44 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_61: "f32[1, 512, 3448]" = torch.ops.aten.transpose.int(add_45, 1, 2);  add_45 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_38: "f32[1, 512, 3448]" = torch.ops.aten.group_norm.default(transpose_61, 1, p_crosstransformer_layers_3_norm_out_weight, p_crosstransformer_layers_3_norm_out_bias);  transpose_61 = p_crosstransformer_layers_3_norm_out_weight = p_crosstransformer_layers_3_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_62: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(group_norm_38, 1, 2);  group_norm_38 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_19 = torch.ops.aten.native_layer_norm.default(transpose_53, [512], p_crosstransformer_layers_t_3_norm1_weight, p_crosstransformer_layers_t_3_norm1_bias, 1e-05);  p_crosstransformer_layers_t_3_norm1_weight = p_crosstransformer_layers_t_3_norm1_bias = None
            getitem_83: "f32[1, 1723, 512]" = native_layer_norm_19[0];  native_layer_norm_19 = None
            native_layer_norm_20 = torch.ops.aten.native_layer_norm.default(transpose_45, [512], p_crosstransformer_layers_t_3_norm2_weight, p_crosstransformer_layers_t_3_norm2_bias, 1e-05);  transpose_45 = p_crosstransformer_layers_t_3_norm2_weight = p_crosstransformer_layers_t_3_norm2_bias = None
            getitem_86: "f32[1, 3448, 512]" = native_layer_norm_20[0];  native_layer_norm_20 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1336 in forward, code: query, key = (x.transpose(1, 0) for x in (query, key))
            transpose_63: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_83, 1, 0);  getitem_83 = None
            transpose_64: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_86, 1, 0);  getitem_86 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_t_3_cross_attn_in_proj_weight, [512, 1024]);  p_crosstransformer_layers_t_3_cross_attn_in_proj_weight = None
            getitem_89: "f32[512, 512]" = split_with_sizes_6[0]
            getitem_90: "f32[1024, 512]" = split_with_sizes_6[1];  split_with_sizes_6 = None
            split_with_sizes_7 = torch.ops.aten.split_with_sizes.default(p_crosstransformer_layers_t_3_cross_attn_in_proj_bias, [512, 1024]);  p_crosstransformer_layers_t_3_cross_attn_in_proj_bias = None
            getitem_91: "f32[512]" = split_with_sizes_7[0]
            getitem_92: "f32[1024]" = split_with_sizes_7[1];  split_with_sizes_7 = None
            view_123: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_63, [1723, 512]);  transpose_63 = None
            t_32: "f32[512, 512]" = torch.ops.aten.t.default(getitem_89);  getitem_89 = None
            addmm_31: "f32[1723, 512]" = torch.ops.aten.addmm.default(getitem_91, view_123, t_32);  getitem_91 = view_123 = t_32 = None
            view_124: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_31, [1723, 1, 512]);  addmm_31 = None
            view_125: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_64, [3448, 512]);  transpose_64 = None
            t_33: "f32[512, 1024]" = torch.ops.aten.t.default(getitem_90);  getitem_90 = None
            addmm_32: "f32[3448, 1024]" = torch.ops.aten.addmm.default(getitem_92, view_125, t_33);  getitem_92 = view_125 = t_33 = None
            view_126: "f32[3448, 1, 1024]" = torch.ops.aten.view.default(addmm_32, [3448, 1, 1024]);  addmm_32 = None
            view_127: "f32[3448, 1, 2, 512]" = torch.ops.aten.view.default(view_126, [3448, 1, 2, 512]);  view_126 = None
            unsqueeze_32: "f32[1, 3448, 1, 2, 512]" = torch.ops.aten.unsqueeze.default(view_127, 0);  view_127 = None
            transpose_65: "f32[2, 3448, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_32, 0, -2);  unsqueeze_32 = None
            squeeze_7: "f32[2, 3448, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_65, -2);  transpose_65 = None
            clone_31: "f32[2, 3448, 1, 512]" = torch.ops.aten.clone.default(squeeze_7, memory_format = torch.contiguous_format);  squeeze_7 = None
            select_18: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_31, 0, 0)
            select_19: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_31, 0, 1);  clone_31 = None
            view_128: "f32[1723, 8, 64]" = torch.ops.aten.view.default(view_124, [1723, 8, 64]);  view_124 = None
            transpose_66: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_128, 0, 1);  view_128 = None
            view_129: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_18, [3448, 8, 64]);  select_18 = None
            transpose_67: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_129, 0, 1);  view_129 = None
            view_130: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_19, [3448, 8, 64]);  select_19 = None
            transpose_68: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_130, 0, 1);  view_130 = None
            view_131: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_66, [1, 8, 1723, 64]);  transpose_66 = None
            view_132: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_67, [1, 8, 3448, 64]);  transpose_67 = None
            view_133: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_68, [1, 8, 3448, 64]);  transpose_68 = None
            _scaled_dot_product_flash_attention_for_cpu_7 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_131, view_132, view_133);  view_131 = view_132 = view_133 = None
            getitem_93: "f32[1, 8, 1723, 64]" = _scaled_dot_product_flash_attention_for_cpu_7[0];  _scaled_dot_product_flash_attention_for_cpu_7 = None
            permute_20: "f32[1723, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_93, [2, 0, 1, 3]);  getitem_93 = None
            view_134: "f32[1723, 512]" = torch.ops.aten.view.default(permute_20, [1723, 512]);  permute_20 = None
            t_34: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_3_cross_attn_out_proj_weight);  p_crosstransformer_layers_t_3_cross_attn_out_proj_weight = None
            addmm_33: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_3_cross_attn_out_proj_bias, view_134, t_34);  p_crosstransformer_layers_t_3_cross_attn_out_proj_bias = view_134 = t_34 = None
            view_135: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_33, [1723, 1, 512]);  addmm_33 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_69: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(view_135, 1, 0);  view_135 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_32: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(transpose_69);  transpose_69 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_45: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_3_gamma_1_scale, clone_32);  p_crosstransformer_layers_t_3_gamma_1_scale = clone_32 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:494 in forward, code: x = q + self.gamma_1(self._ca_block(self.norm1(q), self.norm2(k), mask))
            add_46: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(transpose_53, mul_45);  transpose_53 = mul_45 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_21 = torch.ops.aten.native_layer_norm.default(add_46, [512], p_crosstransformer_layers_t_3_norm3_weight, p_crosstransformer_layers_t_3_norm3_bias, 1e-05);  p_crosstransformer_layers_t_3_norm3_weight = p_crosstransformer_layers_t_3_norm3_bias = None
            getitem_95: "f32[1, 1723, 512]" = native_layer_norm_21[0];  native_layer_norm_21 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_136: "f32[1723, 512]" = torch.ops.aten.view.default(getitem_95, [1723, 512]);  getitem_95 = None
            t_35: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_3_linear1_weight);  p_crosstransformer_layers_t_3_linear1_weight = None
            addmm_34: "f32[1723, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_3_linear1_bias, view_136, t_35);  p_crosstransformer_layers_t_3_linear1_bias = view_136 = t_35 = None
            view_137: "f32[1, 1723, 2048]" = torch.ops.aten.view.default(addmm_34, [1, 1723, 2048]);  addmm_34 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            gelu_31: "f32[1, 1723, 2048]" = torch.ops.aten.gelu.default(view_137);  view_137 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_33: "f32[1, 1723, 2048]" = torch.ops.aten.clone.default(gelu_31);  gelu_31 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_138: "f32[1723, 2048]" = torch.ops.aten.view.default(clone_33, [1723, 2048]);  clone_33 = None
            t_36: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_3_linear2_weight);  p_crosstransformer_layers_t_3_linear2_weight = None
            addmm_35: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_3_linear2_bias, view_138, t_36);  p_crosstransformer_layers_t_3_linear2_bias = view_138 = t_36 = None
            view_139: "f32[1, 1723, 512]" = torch.ops.aten.view.default(addmm_35, [1, 1723, 512]);  addmm_35 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_34: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(view_139);  view_139 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_46: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_3_gamma_2_scale, clone_34);  p_crosstransformer_layers_t_3_gamma_2_scale = clone_34 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:495 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm3(x)))
            add_47: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(add_46, mul_46);  add_46 = mul_46 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_70: "f32[1, 512, 1723]" = torch.ops.aten.transpose.int(add_47, 1, 2);  add_47 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_39: "f32[1, 512, 1723]" = torch.ops.aten.group_norm.default(transpose_70, 1, p_crosstransformer_layers_t_3_norm_out_weight, p_crosstransformer_layers_t_3_norm_out_bias);  transpose_70 = p_crosstransformer_layers_t_3_norm_out_weight = p_crosstransformer_layers_t_3_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_71: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(group_norm_39, 1, 2);  group_norm_39 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_22 = torch.ops.aten.native_layer_norm.default(transpose_62, [512], p_crosstransformer_layers_4_norm1_weight, p_crosstransformer_layers_4_norm1_bias, 1e-05);  p_crosstransformer_layers_4_norm1_weight = p_crosstransformer_layers_4_norm1_bias = None
            getitem_98: "f32[1, 3448, 512]" = native_layer_norm_22[0];  native_layer_norm_22 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1334 in forward, code: query = key = value = query.transpose(1, 0)
            transpose_72: "f32[3448, 1, 512]" = torch.ops.aten.transpose.int(getitem_98, 1, 0);  getitem_98 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            view_140: "f32[3448, 512]" = torch.ops.aten.view.default(transpose_72, [3448, 512]);  transpose_72 = None
            t_37: "f32[512, 1536]" = torch.ops.aten.t.default(p_crosstransformer_layers_4_self_attn_in_proj_weight);  p_crosstransformer_layers_4_self_attn_in_proj_weight = None
            addmm_36: "f32[3448, 1536]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_4_self_attn_in_proj_bias, view_140, t_37);  p_crosstransformer_layers_4_self_attn_in_proj_bias = view_140 = t_37 = None
            view_141: "f32[3448, 1, 1536]" = torch.ops.aten.view.default(addmm_36, [3448, 1, 1536]);  addmm_36 = None
            view_142: "f32[3448, 1, 3, 512]" = torch.ops.aten.view.default(view_141, [3448, 1, 3, 512]);  view_141 = None
            unsqueeze_33: "f32[1, 3448, 1, 3, 512]" = torch.ops.aten.unsqueeze.default(view_142, 0);  view_142 = None
            transpose_73: "f32[3, 3448, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_33, 0, -2);  unsqueeze_33 = None
            squeeze_8: "f32[3, 3448, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_73, -2);  transpose_73 = None
            clone_35: "f32[3, 3448, 1, 512]" = torch.ops.aten.clone.default(squeeze_8, memory_format = torch.contiguous_format);  squeeze_8 = None
            select_20: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_35, 0, 0)
            select_21: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_35, 0, 1)
            select_22: "f32[3448, 1, 512]" = torch.ops.aten.select.int(clone_35, 0, 2);  clone_35 = None
            view_143: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_20, [3448, 8, 64]);  select_20 = None
            transpose_74: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_143, 0, 1);  view_143 = None
            view_144: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_21, [3448, 8, 64]);  select_21 = None
            transpose_75: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_144, 0, 1);  view_144 = None
            view_145: "f32[3448, 8, 64]" = torch.ops.aten.view.default(select_22, [3448, 8, 64]);  select_22 = None
            transpose_76: "f32[8, 3448, 64]" = torch.ops.aten.transpose.int(view_145, 0, 1);  view_145 = None
            view_146: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_74, [1, 8, 3448, 64]);  transpose_74 = None
            view_147: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_75, [1, 8, 3448, 64]);  transpose_75 = None
            view_148: "f32[1, 8, 3448, 64]" = torch.ops.aten.view.default(transpose_76, [1, 8, 3448, 64]);  transpose_76 = None
            _scaled_dot_product_flash_attention_for_cpu_8 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_146, view_147, view_148);  view_146 = view_147 = view_148 = None
            getitem_101: "f32[1, 8, 3448, 64]" = _scaled_dot_product_flash_attention_for_cpu_8[0];  _scaled_dot_product_flash_attention_for_cpu_8 = None
            permute_21: "f32[3448, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_101, [2, 0, 1, 3]);  getitem_101 = None
            view_149: "f32[3448, 512]" = torch.ops.aten.view.default(permute_21, [3448, 512]);  permute_21 = None
            t_38: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_4_self_attn_out_proj_weight);  p_crosstransformer_layers_4_self_attn_out_proj_weight = None
            addmm_37: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_4_self_attn_out_proj_bias, view_149, t_38);  p_crosstransformer_layers_4_self_attn_out_proj_bias = view_149 = t_38 = None
            view_150: "f32[3448, 1, 512]" = torch.ops.aten.view.default(addmm_37, [3448, 1, 512]);  addmm_37 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_77: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(view_150, 1, 0);  view_150 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_36: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(transpose_77);  transpose_77 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_47: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_4_gamma_1_scale, clone_36);  p_crosstransformer_layers_4_gamma_1_scale = clone_36 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:364 in forward, code: x = x + self.gamma_1(
            add_48: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(transpose_62, mul_47);  transpose_62 = mul_47 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_23 = torch.ops.aten.native_layer_norm.default(add_48, [512], p_crosstransformer_layers_4_norm2_weight, p_crosstransformer_layers_4_norm2_bias, 1e-05);  p_crosstransformer_layers_4_norm2_weight = p_crosstransformer_layers_4_norm2_bias = None
            getitem_103: "f32[1, 3448, 512]" = native_layer_norm_23[0];  native_layer_norm_23 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_151: "f32[3448, 512]" = torch.ops.aten.view.default(getitem_103, [3448, 512]);  getitem_103 = None
            t_39: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_4_linear1_weight);  p_crosstransformer_layers_4_linear1_weight = None
            addmm_38: "f32[3448, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_4_linear1_bias, view_151, t_39);  p_crosstransformer_layers_4_linear1_bias = view_151 = t_39 = None
            view_152: "f32[1, 3448, 2048]" = torch.ops.aten.view.default(addmm_38, [1, 3448, 2048]);  addmm_38 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            gelu_32: "f32[1, 3448, 2048]" = torch.ops.aten.gelu.default(view_152);  view_152 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_37: "f32[1, 3448, 2048]" = torch.ops.aten.clone.default(gelu_32);  gelu_32 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_153: "f32[3448, 2048]" = torch.ops.aten.view.default(clone_37, [3448, 2048]);  clone_37 = None
            t_40: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_4_linear2_weight);  p_crosstransformer_layers_4_linear2_weight = None
            addmm_39: "f32[3448, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_4_linear2_bias, view_153, t_40);  p_crosstransformer_layers_4_linear2_bias = view_153 = t_40 = None
            view_154: "f32[1, 3448, 512]" = torch.ops.aten.view.default(addmm_39, [1, 3448, 512]);  addmm_39 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_38: "f32[1, 3448, 512]" = torch.ops.aten.clone.default(view_154);  view_154 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_48: "f32[1, 3448, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_4_gamma_2_scale, clone_38);  p_crosstransformer_layers_4_gamma_2_scale = clone_38 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            add_49: "f32[1, 3448, 512]" = torch.ops.aten.add.Tensor(add_48, mul_48);  add_48 = mul_48 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_78: "f32[1, 512, 3448]" = torch.ops.aten.transpose.int(add_49, 1, 2);  add_49 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_40: "f32[1, 512, 3448]" = torch.ops.aten.group_norm.default(transpose_78, 1, p_crosstransformer_layers_4_norm_out_weight, p_crosstransformer_layers_4_norm_out_bias);  transpose_78 = p_crosstransformer_layers_4_norm_out_weight = p_crosstransformer_layers_4_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_79: "f32[1, 3448, 512]" = torch.ops.aten.transpose.int(group_norm_40, 1, 2);  group_norm_40 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_24 = torch.ops.aten.native_layer_norm.default(transpose_71, [512], p_crosstransformer_layers_t_4_norm1_weight, p_crosstransformer_layers_t_4_norm1_bias, 1e-05);  p_crosstransformer_layers_t_4_norm1_weight = p_crosstransformer_layers_t_4_norm1_bias = None
            getitem_106: "f32[1, 1723, 512]" = native_layer_norm_24[0];  native_layer_norm_24 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1334 in forward, code: query = key = value = query.transpose(1, 0)
            transpose_80: "f32[1723, 1, 512]" = torch.ops.aten.transpose.int(getitem_106, 1, 0);  getitem_106 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(
            view_155: "f32[1723, 512]" = torch.ops.aten.view.default(transpose_80, [1723, 512]);  transpose_80 = None
            t_41: "f32[512, 1536]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_4_self_attn_in_proj_weight);  p_crosstransformer_layers_t_4_self_attn_in_proj_weight = None
            addmm_40: "f32[1723, 1536]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_4_self_attn_in_proj_bias, view_155, t_41);  p_crosstransformer_layers_t_4_self_attn_in_proj_bias = view_155 = t_41 = None
            view_156: "f32[1723, 1, 1536]" = torch.ops.aten.view.default(addmm_40, [1723, 1, 1536]);  addmm_40 = None
            view_157: "f32[1723, 1, 3, 512]" = torch.ops.aten.view.default(view_156, [1723, 1, 3, 512]);  view_156 = None
            unsqueeze_34: "f32[1, 1723, 1, 3, 512]" = torch.ops.aten.unsqueeze.default(view_157, 0);  view_157 = None
            transpose_81: "f32[3, 1723, 1, 1, 512]" = torch.ops.aten.transpose.int(unsqueeze_34, 0, -2);  unsqueeze_34 = None
            squeeze_9: "f32[3, 1723, 1, 512]" = torch.ops.aten.squeeze.dim(transpose_81, -2);  transpose_81 = None
            clone_39: "f32[3, 1723, 1, 512]" = torch.ops.aten.clone.default(squeeze_9, memory_format = torch.contiguous_format);  squeeze_9 = None
            select_23: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_39, 0, 0)
            select_24: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_39, 0, 1)
            select_25: "f32[1723, 1, 512]" = torch.ops.aten.select.int(clone_39, 0, 2);  clone_39 = None
            view_158: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_23, [1723, 8, 64]);  select_23 = None
            transpose_82: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_158, 0, 1);  view_158 = None
            view_159: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_24, [1723, 8, 64]);  select_24 = None
            transpose_83: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_159, 0, 1);  view_159 = None
            view_160: "f32[1723, 8, 64]" = torch.ops.aten.view.default(select_25, [1723, 8, 64]);  select_25 = None
            transpose_84: "f32[8, 1723, 64]" = torch.ops.aten.transpose.int(view_160, 0, 1);  view_160 = None
            view_161: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_82, [1, 8, 1723, 64]);  transpose_82 = None
            view_162: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_83, [1, 8, 1723, 64]);  transpose_83 = None
            view_163: "f32[1, 8, 1723, 64]" = torch.ops.aten.view.default(transpose_84, [1, 8, 1723, 64]);  transpose_84 = None
            _scaled_dot_product_flash_attention_for_cpu_9 = torch.ops.aten._scaled_dot_product_flash_attention_for_cpu.default(view_161, view_162, view_163);  view_161 = view_162 = view_163 = None
            getitem_109: "f32[1, 8, 1723, 64]" = _scaled_dot_product_flash_attention_for_cpu_9[0];  _scaled_dot_product_flash_attention_for_cpu_9 = None
            permute_22: "f32[1723, 1, 8, 64]" = torch.ops.aten.permute.default(getitem_109, [2, 0, 1, 3]);  getitem_109 = None
            view_164: "f32[1723, 512]" = torch.ops.aten.view.default(permute_22, [1723, 512]);  permute_22 = None
            t_42: "f32[512, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_4_self_attn_out_proj_weight);  p_crosstransformer_layers_t_4_self_attn_out_proj_weight = None
            addmm_41: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_4_self_attn_out_proj_bias, view_164, t_42);  p_crosstransformer_layers_t_4_self_attn_out_proj_bias = view_164 = t_42 = None
            view_165: "f32[1723, 1, 512]" = torch.ops.aten.view.default(addmm_41, [1723, 1, 512]);  addmm_41 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:1390 in forward, code: return attn_output.transpose(1, 0), attn_output_weights
            transpose_85: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(view_165, 1, 0);  view_165 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_40: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(transpose_85);  transpose_85 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_49: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_4_gamma_1_scale, clone_40);  p_crosstransformer_layers_t_4_gamma_1_scale = clone_40 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:364 in forward, code: x = x + self.gamma_1(
            add_50: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(transpose_71, mul_49);  transpose_71 = mul_49 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:217 in forward, code: return F.layer_norm(
            native_layer_norm_25 = torch.ops.aten.native_layer_norm.default(add_50, [512], p_crosstransformer_layers_t_4_norm2_weight, p_crosstransformer_layers_t_4_norm2_bias, 1e-05);  p_crosstransformer_layers_t_4_norm2_weight = p_crosstransformer_layers_t_4_norm2_bias = None
            getitem_111: "f32[1, 1723, 512]" = native_layer_norm_25[0];  native_layer_norm_25 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_166: "f32[1723, 512]" = torch.ops.aten.view.default(getitem_111, [1723, 512]);  getitem_111 = None
            t_43: "f32[512, 2048]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_4_linear1_weight);  p_crosstransformer_layers_t_4_linear1_weight = None
            addmm_42: "f32[1723, 2048]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_4_linear1_bias, view_166, t_43);  p_crosstransformer_layers_t_4_linear1_bias = view_166 = t_43 = None
            view_167: "f32[1, 1723, 2048]" = torch.ops.aten.view.default(addmm_42, [1, 1723, 2048]);  addmm_42 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            gelu_33: "f32[1, 1723, 2048]" = torch.ops.aten.gelu.default(view_167);  view_167 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_41: "f32[1, 1723, 2048]" = torch.ops.aten.clone.default(gelu_33);  gelu_33 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)
            view_168: "f32[1723, 2048]" = torch.ops.aten.view.default(clone_41, [1723, 2048]);  clone_41 = None
            t_44: "f32[2048, 512]" = torch.ops.aten.t.default(p_crosstransformer_layers_t_4_linear2_weight);  p_crosstransformer_layers_t_4_linear2_weight = None
            addmm_43: "f32[1723, 512]" = torch.ops.aten.addmm.default(p_crosstransformer_layers_t_4_linear2_bias, view_168, t_44);  p_crosstransformer_layers_t_4_linear2_bias = view_168 = t_44 = None
            view_169: "f32[1, 1723, 512]" = torch.ops.aten.view.default(addmm_43, [1, 1723, 512]);  addmm_43 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)
            clone_42: "f32[1, 1723, 512]" = torch.ops.aten.clone.default(view_169);  view_169 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:253 in forward, code: return self.scale * x
            mul_50: "f32[1, 1723, 512]" = torch.ops.aten.mul.Tensor(p_crosstransformer_layers_t_4_gamma_2_scale, clone_42);  p_crosstransformer_layers_t_4_gamma_2_scale = clone_42 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:367 in forward, code: x = x + self.gamma_2(self._ff_block(self.norm2(x)))
            add_51: "f32[1, 1723, 512]" = torch.ops.aten.add.Tensor(add_50, mul_50);  add_50 = mul_50 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:267 in forward, code: x = x.transpose(1, 2)
            transpose_86: "f32[1, 512, 1723]" = torch.ops.aten.transpose.int(add_51, 1, 2);  add_51 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_41: "f32[1, 512, 1723]" = torch.ops.aten.group_norm.default(transpose_86, 1, p_crosstransformer_layers_t_4_norm_out_weight, p_crosstransformer_layers_t_4_norm_out_bias);  transpose_86 = p_crosstransformer_layers_t_4_norm_out_weight = p_crosstransformer_layers_t_4_norm_out_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:268 in forward, code: return super().forward(x).transpose(1, 2)
            transpose_87: "f32[1, 1723, 512]" = torch.ops.aten.transpose.int(group_norm_41, 1, 2);  group_norm_41 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:674 in forward, code: x = rearrange(x, "b (t1 fr) c -> b c fr t1", t1=T1)
            view_170: "f32[1, 431, 8, 512]" = torch.ops.aten.view.default(transpose_79, [1, 431, 8, 512]);  transpose_79 = None
            permute_23: "f32[1, 512, 8, 431]" = torch.ops.aten.permute.default(view_170, [0, 3, 2, 1]);  view_170 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:675 in forward, code: xt = rearrange(xt, "b t2 c -> b c t2")
            permute_24: "f32[1, 512, 1723]" = torch.ops.aten.permute.default(transpose_87, [0, 2, 1]);  transpose_87 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:487 in forward, code: x = rearrange(x, "b c f t-> b c (f t)")
            clone_43: "f32[1, 512, 8, 431]" = torch.ops.aten.clone.default(permute_23, memory_format = torch.contiguous_format);  permute_23 = None
            _unsafe_view_3: "f32[1, 512, 3448]" = torch.ops.aten._unsafe_view.default(clone_43, [1, 512, 3448]);  clone_43 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_50: "f32[1, 384, 3448]" = torch.ops.aten.convolution.default(_unsafe_view_3, p_channel_downsampler_weight, p_channel_downsampler_bias, [1], [0], [1], False, [0], 1);  _unsafe_view_3 = p_channel_downsampler_weight = p_channel_downsampler_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:489 in forward, code: x = rearrange(x, "b c (f t)-> b c f t", f=f)
            view_171: "f32[1, 384, 8, 431]" = torch.ops.aten.view.default(convolution_50, [1, 384, 8, 431]);  convolution_50 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_51: "f32[1, 384, 1723]" = torch.ops.aten.convolution.default(permute_24, p_channel_downsampler_t_weight, p_channel_downsampler_t_bias, [1], [0], [1], False, [0], 1);  permute_24 = p_channel_downsampler_t_weight = p_channel_downsampler_t_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_52: "f32[1, 384, 8, 431]" = torch.ops.aten.add.Tensor(view_171, glu_23);  view_171 = glu_23 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_52: "f32[1, 768, 8, 431]" = torch.ops.aten.convolution.default(add_52, p_decoder_0_rewrite_weight, p_decoder_0_rewrite_bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  add_52 = p_decoder_0_rewrite_weight = p_decoder_0_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_24: "f32[1, 384, 8, 431]" = torch.ops.aten.glu.default(convolution_52, 1);  convolution_52 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:319 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_25: "f32[1, 8, 384, 431]" = torch.ops.aten.permute.default(glu_24, [0, 2, 1, 3]);  glu_24 = None
            view_172: "f32[8, 384, 431]" = torch.ops.aten.view.default(permute_25, [8, 384, 431]);  permute_25 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_53: "f32[8, 48, 431]" = torch.ops.aten.convolution.default(view_172, p_decoder_0_dconv_layers_0_0_weight, p_decoder_0_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_decoder_0_dconv_layers_0_0_weight = p_decoder_0_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_42: "f32[8, 48, 431]" = torch.ops.aten.group_norm.default(convolution_53, 1, p_decoder_0_dconv_layers_0_1_weight, p_decoder_0_dconv_layers_0_1_bias);  convolution_53 = p_decoder_0_dconv_layers_0_1_weight = p_decoder_0_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_34: "f32[8, 48, 431]" = torch.ops.aten.gelu.default(group_norm_42);  group_norm_42 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_54: "f32[8, 768, 431]" = torch.ops.aten.convolution.default(gelu_34, p_decoder_0_dconv_layers_0_3_weight, p_decoder_0_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_34 = p_decoder_0_dconv_layers_0_3_weight = p_decoder_0_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_43: "f32[8, 768, 431]" = torch.ops.aten.group_norm.default(convolution_54, 1, p_decoder_0_dconv_layers_0_4_weight, p_decoder_0_dconv_layers_0_4_bias);  convolution_54 = p_decoder_0_dconv_layers_0_4_weight = p_decoder_0_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_25: "f32[8, 384, 431]" = torch.ops.aten.glu.default(group_norm_43, 1);  group_norm_43 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_40: "f32[384]" = torch.ops.aten.slice.Tensor(p_decoder_0_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_decoder_0_dconv_layers_0_6_scale = None
            unsqueeze_35: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_40, 1);  slice_40 = None
            mul_51: "f32[8, 384, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_35, glu_25);  unsqueeze_35 = glu_25 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_53: "f32[8, 384, 431]" = torch.ops.aten.add.Tensor(view_172, mul_51);  view_172 = mul_51 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_55: "f32[8, 48, 431]" = torch.ops.aten.convolution.default(add_53, p_decoder_0_dconv_layers_1_0_weight, p_decoder_0_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_decoder_0_dconv_layers_1_0_weight = p_decoder_0_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_44: "f32[8, 48, 431]" = torch.ops.aten.group_norm.default(convolution_55, 1, p_decoder_0_dconv_layers_1_1_weight, p_decoder_0_dconv_layers_1_1_bias);  convolution_55 = p_decoder_0_dconv_layers_1_1_weight = p_decoder_0_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_35: "f32[8, 48, 431]" = torch.ops.aten.gelu.default(group_norm_44);  group_norm_44 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_56: "f32[8, 768, 431]" = torch.ops.aten.convolution.default(gelu_35, p_decoder_0_dconv_layers_1_3_weight, p_decoder_0_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_35 = p_decoder_0_dconv_layers_1_3_weight = p_decoder_0_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_45: "f32[8, 768, 431]" = torch.ops.aten.group_norm.default(convolution_56, 1, p_decoder_0_dconv_layers_1_4_weight, p_decoder_0_dconv_layers_1_4_bias);  convolution_56 = p_decoder_0_dconv_layers_1_4_weight = p_decoder_0_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_26: "f32[8, 384, 431]" = torch.ops.aten.glu.default(group_norm_45, 1);  group_norm_45 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_41: "f32[384]" = torch.ops.aten.slice.Tensor(p_decoder_0_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_decoder_0_dconv_layers_1_6_scale = None
            unsqueeze_36: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_41, 1);  slice_41 = None
            mul_52: "f32[8, 384, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_36, glu_26);  unsqueeze_36 = glu_26 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_54: "f32[8, 384, 431]" = torch.ops.aten.add.Tensor(add_53, mul_52);  add_53 = mul_52 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:322 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_173: "f32[1, 8, 384, 431]" = torch.ops.aten.view.default(add_54, [1, 8, 384, 431]);  add_54 = None
            permute_26: "f32[1, 384, 8, 431]" = torch.ops.aten.permute.default(view_173, [0, 2, 1, 3]);  view_173 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:1150 in forward, code: return F.conv_transpose2d(
            convolution_57: "f32[1, 192, 36, 431]" = torch.ops.aten.convolution.default(permute_26, p_decoder_0_conv_tr_weight, p_decoder_0_conv_tr_bias, [4, 1], [0, 0], [1, 1], True, [0, 0], 1);  permute_26 = p_decoder_0_conv_tr_weight = p_decoder_0_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:329 in forward, code: z = z[..., self.pad:-self.pad, :]
            slice_42: "f32[1, 192, 32, 431]" = torch.ops.aten.slice.Tensor(convolution_57, 2, 2, -2);  convolution_57 = None
            slice_43: "f32[1, 192, 32, 431]" = torch.ops.aten.slice.Tensor(slice_42, 3, 0, 9223372036854775807);  slice_42 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:334 in forward, code: z = F.gelu(z)
            gelu_36: "f32[1, 192, 32, 431]" = torch.ops.aten.gelu.default(slice_43);  slice_43 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_55: "f32[1, 384, 1723]" = torch.ops.aten.add.Tensor(convolution_51, glu_20);  convolution_51 = glu_20 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_58: "f32[1, 768, 1723]" = torch.ops.aten.convolution.default(add_55, p_tdecoder_0_rewrite_weight, p_tdecoder_0_rewrite_bias, [1], [1], [1], False, [0], 1);  add_55 = p_tdecoder_0_rewrite_weight = p_tdecoder_0_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_27: "f32[1, 384, 1723]" = torch.ops.aten.glu.default(convolution_58, 1);  convolution_58 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_59: "f32[1, 48, 1723]" = torch.ops.aten.convolution.default(glu_27, p_tdecoder_0_dconv_layers_0_0_weight, p_tdecoder_0_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tdecoder_0_dconv_layers_0_0_weight = p_tdecoder_0_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_46: "f32[1, 48, 1723]" = torch.ops.aten.group_norm.default(convolution_59, 1, p_tdecoder_0_dconv_layers_0_1_weight, p_tdecoder_0_dconv_layers_0_1_bias);  convolution_59 = p_tdecoder_0_dconv_layers_0_1_weight = p_tdecoder_0_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_37: "f32[1, 48, 1723]" = torch.ops.aten.gelu.default(group_norm_46);  group_norm_46 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_60: "f32[1, 768, 1723]" = torch.ops.aten.convolution.default(gelu_37, p_tdecoder_0_dconv_layers_0_3_weight, p_tdecoder_0_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_37 = p_tdecoder_0_dconv_layers_0_3_weight = p_tdecoder_0_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_47: "f32[1, 768, 1723]" = torch.ops.aten.group_norm.default(convolution_60, 1, p_tdecoder_0_dconv_layers_0_4_weight, p_tdecoder_0_dconv_layers_0_4_bias);  convolution_60 = p_tdecoder_0_dconv_layers_0_4_weight = p_tdecoder_0_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_28: "f32[1, 384, 1723]" = torch.ops.aten.glu.default(group_norm_47, 1);  group_norm_47 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_44: "f32[384]" = torch.ops.aten.slice.Tensor(p_tdecoder_0_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_0_dconv_layers_0_6_scale = None
            unsqueeze_37: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_44, 1);  slice_44 = None
            mul_53: "f32[1, 384, 1723]" = torch.ops.aten.mul.Tensor(unsqueeze_37, glu_28);  unsqueeze_37 = glu_28 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_56: "f32[1, 384, 1723]" = torch.ops.aten.add.Tensor(glu_27, mul_53);  glu_27 = mul_53 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_61: "f32[1, 48, 1723]" = torch.ops.aten.convolution.default(add_56, p_tdecoder_0_dconv_layers_1_0_weight, p_tdecoder_0_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tdecoder_0_dconv_layers_1_0_weight = p_tdecoder_0_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_48: "f32[1, 48, 1723]" = torch.ops.aten.group_norm.default(convolution_61, 1, p_tdecoder_0_dconv_layers_1_1_weight, p_tdecoder_0_dconv_layers_1_1_bias);  convolution_61 = p_tdecoder_0_dconv_layers_1_1_weight = p_tdecoder_0_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_38: "f32[1, 48, 1723]" = torch.ops.aten.gelu.default(group_norm_48);  group_norm_48 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_62: "f32[1, 768, 1723]" = torch.ops.aten.convolution.default(gelu_38, p_tdecoder_0_dconv_layers_1_3_weight, p_tdecoder_0_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_38 = p_tdecoder_0_dconv_layers_1_3_weight = p_tdecoder_0_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_49: "f32[1, 768, 1723]" = torch.ops.aten.group_norm.default(convolution_62, 1, p_tdecoder_0_dconv_layers_1_4_weight, p_tdecoder_0_dconv_layers_1_4_bias);  convolution_62 = p_tdecoder_0_dconv_layers_1_4_weight = p_tdecoder_0_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_29: "f32[1, 384, 1723]" = torch.ops.aten.glu.default(group_norm_49, 1);  group_norm_49 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_45: "f32[384]" = torch.ops.aten.slice.Tensor(p_tdecoder_0_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_0_dconv_layers_1_6_scale = None
            unsqueeze_38: "f32[384, 1]" = torch.ops.aten.unsqueeze.default(slice_45, 1);  slice_45 = None
            mul_54: "f32[1, 384, 1723]" = torch.ops.aten.mul.Tensor(unsqueeze_38, glu_29);  unsqueeze_38 = glu_29 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_57: "f32[1, 384, 1723]" = torch.ops.aten.add.Tensor(add_56, mul_54);  add_56 = mul_54 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:964 in forward, code: return F.conv_transpose1d(
            convolution_63: "f32[1, 192, 6896]" = torch.ops.aten.convolution.default(add_57, p_tdecoder_0_conv_tr_weight, p_tdecoder_0_conv_tr_bias, [4], [0], [1], True, [0], 1);  add_57 = p_tdecoder_0_conv_tr_weight = p_tdecoder_0_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:331 in forward, code: z = z[..., self.pad:self.pad + length]
            slice_46: "f32[1, 192, 6891]" = torch.ops.aten.slice.Tensor(convolution_63, 2, 2, 6893);  convolution_63 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:334 in forward, code: z = F.gelu(z)
            gelu_39: "f32[1, 192, 6891]" = torch.ops.aten.gelu.default(slice_46);  slice_46 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_58: "f32[1, 192, 32, 431]" = torch.ops.aten.add.Tensor(gelu_36, glu_17);  gelu_36 = glu_17 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_64: "f32[1, 384, 32, 431]" = torch.ops.aten.convolution.default(add_58, p_decoder_1_rewrite_weight, p_decoder_1_rewrite_bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  add_58 = p_decoder_1_rewrite_weight = p_decoder_1_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_30: "f32[1, 192, 32, 431]" = torch.ops.aten.glu.default(convolution_64, 1);  convolution_64 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:319 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_27: "f32[1, 32, 192, 431]" = torch.ops.aten.permute.default(glu_30, [0, 2, 1, 3]);  glu_30 = None
            view_174: "f32[32, 192, 431]" = torch.ops.aten.view.default(permute_27, [32, 192, 431]);  permute_27 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_65: "f32[32, 24, 431]" = torch.ops.aten.convolution.default(view_174, p_decoder_1_dconv_layers_0_0_weight, p_decoder_1_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_decoder_1_dconv_layers_0_0_weight = p_decoder_1_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_50: "f32[32, 24, 431]" = torch.ops.aten.group_norm.default(convolution_65, 1, p_decoder_1_dconv_layers_0_1_weight, p_decoder_1_dconv_layers_0_1_bias);  convolution_65 = p_decoder_1_dconv_layers_0_1_weight = p_decoder_1_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_40: "f32[32, 24, 431]" = torch.ops.aten.gelu.default(group_norm_50);  group_norm_50 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_66: "f32[32, 384, 431]" = torch.ops.aten.convolution.default(gelu_40, p_decoder_1_dconv_layers_0_3_weight, p_decoder_1_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_40 = p_decoder_1_dconv_layers_0_3_weight = p_decoder_1_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_51: "f32[32, 384, 431]" = torch.ops.aten.group_norm.default(convolution_66, 1, p_decoder_1_dconv_layers_0_4_weight, p_decoder_1_dconv_layers_0_4_bias);  convolution_66 = p_decoder_1_dconv_layers_0_4_weight = p_decoder_1_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_31: "f32[32, 192, 431]" = torch.ops.aten.glu.default(group_norm_51, 1);  group_norm_51 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_47: "f32[192]" = torch.ops.aten.slice.Tensor(p_decoder_1_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_decoder_1_dconv_layers_0_6_scale = None
            unsqueeze_39: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_47, 1);  slice_47 = None
            mul_55: "f32[32, 192, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_39, glu_31);  unsqueeze_39 = glu_31 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_59: "f32[32, 192, 431]" = torch.ops.aten.add.Tensor(view_174, mul_55);  view_174 = mul_55 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_67: "f32[32, 24, 431]" = torch.ops.aten.convolution.default(add_59, p_decoder_1_dconv_layers_1_0_weight, p_decoder_1_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_decoder_1_dconv_layers_1_0_weight = p_decoder_1_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_52: "f32[32, 24, 431]" = torch.ops.aten.group_norm.default(convolution_67, 1, p_decoder_1_dconv_layers_1_1_weight, p_decoder_1_dconv_layers_1_1_bias);  convolution_67 = p_decoder_1_dconv_layers_1_1_weight = p_decoder_1_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_41: "f32[32, 24, 431]" = torch.ops.aten.gelu.default(group_norm_52);  group_norm_52 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_68: "f32[32, 384, 431]" = torch.ops.aten.convolution.default(gelu_41, p_decoder_1_dconv_layers_1_3_weight, p_decoder_1_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_41 = p_decoder_1_dconv_layers_1_3_weight = p_decoder_1_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_53: "f32[32, 384, 431]" = torch.ops.aten.group_norm.default(convolution_68, 1, p_decoder_1_dconv_layers_1_4_weight, p_decoder_1_dconv_layers_1_4_bias);  convolution_68 = p_decoder_1_dconv_layers_1_4_weight = p_decoder_1_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_32: "f32[32, 192, 431]" = torch.ops.aten.glu.default(group_norm_53, 1);  group_norm_53 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_48: "f32[192]" = torch.ops.aten.slice.Tensor(p_decoder_1_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_decoder_1_dconv_layers_1_6_scale = None
            unsqueeze_40: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_48, 1);  slice_48 = None
            mul_56: "f32[32, 192, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_40, glu_32);  unsqueeze_40 = glu_32 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_60: "f32[32, 192, 431]" = torch.ops.aten.add.Tensor(add_59, mul_56);  add_59 = mul_56 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:322 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_175: "f32[1, 32, 192, 431]" = torch.ops.aten.view.default(add_60, [1, 32, 192, 431]);  add_60 = None
            permute_28: "f32[1, 192, 32, 431]" = torch.ops.aten.permute.default(view_175, [0, 2, 1, 3]);  view_175 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:1150 in forward, code: return F.conv_transpose2d(
            convolution_69: "f32[1, 96, 132, 431]" = torch.ops.aten.convolution.default(permute_28, p_decoder_1_conv_tr_weight, p_decoder_1_conv_tr_bias, [4, 1], [0, 0], [1, 1], True, [0, 0], 1);  permute_28 = p_decoder_1_conv_tr_weight = p_decoder_1_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:329 in forward, code: z = z[..., self.pad:-self.pad, :]
            slice_49: "f32[1, 96, 128, 431]" = torch.ops.aten.slice.Tensor(convolution_69, 2, 2, -2);  convolution_69 = None
            slice_50: "f32[1, 96, 128, 431]" = torch.ops.aten.slice.Tensor(slice_49, 3, 0, 9223372036854775807);  slice_49 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:334 in forward, code: z = F.gelu(z)
            gelu_42: "f32[1, 96, 128, 431]" = torch.ops.aten.gelu.default(slice_50);  slice_50 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_61: "f32[1, 192, 6891]" = torch.ops.aten.add.Tensor(gelu_39, glu_14);  gelu_39 = glu_14 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_70: "f32[1, 384, 6891]" = torch.ops.aten.convolution.default(add_61, p_tdecoder_1_rewrite_weight, p_tdecoder_1_rewrite_bias, [1], [1], [1], False, [0], 1);  add_61 = p_tdecoder_1_rewrite_weight = p_tdecoder_1_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_33: "f32[1, 192, 6891]" = torch.ops.aten.glu.default(convolution_70, 1);  convolution_70 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_71: "f32[1, 24, 6891]" = torch.ops.aten.convolution.default(glu_33, p_tdecoder_1_dconv_layers_0_0_weight, p_tdecoder_1_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tdecoder_1_dconv_layers_0_0_weight = p_tdecoder_1_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_54: "f32[1, 24, 6891]" = torch.ops.aten.group_norm.default(convolution_71, 1, p_tdecoder_1_dconv_layers_0_1_weight, p_tdecoder_1_dconv_layers_0_1_bias);  convolution_71 = p_tdecoder_1_dconv_layers_0_1_weight = p_tdecoder_1_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_43: "f32[1, 24, 6891]" = torch.ops.aten.gelu.default(group_norm_54);  group_norm_54 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_72: "f32[1, 384, 6891]" = torch.ops.aten.convolution.default(gelu_43, p_tdecoder_1_dconv_layers_0_3_weight, p_tdecoder_1_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_43 = p_tdecoder_1_dconv_layers_0_3_weight = p_tdecoder_1_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_55: "f32[1, 384, 6891]" = torch.ops.aten.group_norm.default(convolution_72, 1, p_tdecoder_1_dconv_layers_0_4_weight, p_tdecoder_1_dconv_layers_0_4_bias);  convolution_72 = p_tdecoder_1_dconv_layers_0_4_weight = p_tdecoder_1_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_34: "f32[1, 192, 6891]" = torch.ops.aten.glu.default(group_norm_55, 1);  group_norm_55 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_51: "f32[192]" = torch.ops.aten.slice.Tensor(p_tdecoder_1_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_1_dconv_layers_0_6_scale = None
            unsqueeze_41: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_51, 1);  slice_51 = None
            mul_57: "f32[1, 192, 6891]" = torch.ops.aten.mul.Tensor(unsqueeze_41, glu_34);  unsqueeze_41 = glu_34 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_62: "f32[1, 192, 6891]" = torch.ops.aten.add.Tensor(glu_33, mul_57);  glu_33 = mul_57 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_73: "f32[1, 24, 6891]" = torch.ops.aten.convolution.default(add_62, p_tdecoder_1_dconv_layers_1_0_weight, p_tdecoder_1_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tdecoder_1_dconv_layers_1_0_weight = p_tdecoder_1_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_56: "f32[1, 24, 6891]" = torch.ops.aten.group_norm.default(convolution_73, 1, p_tdecoder_1_dconv_layers_1_1_weight, p_tdecoder_1_dconv_layers_1_1_bias);  convolution_73 = p_tdecoder_1_dconv_layers_1_1_weight = p_tdecoder_1_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_44: "f32[1, 24, 6891]" = torch.ops.aten.gelu.default(group_norm_56);  group_norm_56 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_74: "f32[1, 384, 6891]" = torch.ops.aten.convolution.default(gelu_44, p_tdecoder_1_dconv_layers_1_3_weight, p_tdecoder_1_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_44 = p_tdecoder_1_dconv_layers_1_3_weight = p_tdecoder_1_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_57: "f32[1, 384, 6891]" = torch.ops.aten.group_norm.default(convolution_74, 1, p_tdecoder_1_dconv_layers_1_4_weight, p_tdecoder_1_dconv_layers_1_4_bias);  convolution_74 = p_tdecoder_1_dconv_layers_1_4_weight = p_tdecoder_1_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_35: "f32[1, 192, 6891]" = torch.ops.aten.glu.default(group_norm_57, 1);  group_norm_57 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_52: "f32[192]" = torch.ops.aten.slice.Tensor(p_tdecoder_1_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_1_dconv_layers_1_6_scale = None
            unsqueeze_42: "f32[192, 1]" = torch.ops.aten.unsqueeze.default(slice_52, 1);  slice_52 = None
            mul_58: "f32[1, 192, 6891]" = torch.ops.aten.mul.Tensor(unsqueeze_42, glu_35);  unsqueeze_42 = glu_35 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_63: "f32[1, 192, 6891]" = torch.ops.aten.add.Tensor(add_62, mul_58);  add_62 = mul_58 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:964 in forward, code: return F.conv_transpose1d(
            convolution_75: "f32[1, 96, 27568]" = torch.ops.aten.convolution.default(add_63, p_tdecoder_1_conv_tr_weight, p_tdecoder_1_conv_tr_bias, [4], [0], [1], True, [0], 1);  add_63 = p_tdecoder_1_conv_tr_weight = p_tdecoder_1_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:331 in forward, code: z = z[..., self.pad:self.pad + length]
            slice_53: "f32[1, 96, 27563]" = torch.ops.aten.slice.Tensor(convolution_75, 2, 2, 27565);  convolution_75 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:334 in forward, code: z = F.gelu(z)
            gelu_45: "f32[1, 96, 27563]" = torch.ops.aten.gelu.default(slice_53);  slice_53 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_64: "f32[1, 96, 128, 431]" = torch.ops.aten.add.Tensor(gelu_42, glu_11);  gelu_42 = glu_11 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_76: "f32[1, 192, 128, 431]" = torch.ops.aten.convolution.default(add_64, p_decoder_2_rewrite_weight, p_decoder_2_rewrite_bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  add_64 = p_decoder_2_rewrite_weight = p_decoder_2_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_36: "f32[1, 96, 128, 431]" = torch.ops.aten.glu.default(convolution_76, 1);  convolution_76 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:319 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_29: "f32[1, 128, 96, 431]" = torch.ops.aten.permute.default(glu_36, [0, 2, 1, 3]);  glu_36 = None
            view_176: "f32[128, 96, 431]" = torch.ops.aten.view.default(permute_29, [128, 96, 431]);  permute_29 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_77: "f32[128, 12, 431]" = torch.ops.aten.convolution.default(view_176, p_decoder_2_dconv_layers_0_0_weight, p_decoder_2_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_decoder_2_dconv_layers_0_0_weight = p_decoder_2_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_58: "f32[128, 12, 431]" = torch.ops.aten.group_norm.default(convolution_77, 1, p_decoder_2_dconv_layers_0_1_weight, p_decoder_2_dconv_layers_0_1_bias);  convolution_77 = p_decoder_2_dconv_layers_0_1_weight = p_decoder_2_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_46: "f32[128, 12, 431]" = torch.ops.aten.gelu.default(group_norm_58);  group_norm_58 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_78: "f32[128, 192, 431]" = torch.ops.aten.convolution.default(gelu_46, p_decoder_2_dconv_layers_0_3_weight, p_decoder_2_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_46 = p_decoder_2_dconv_layers_0_3_weight = p_decoder_2_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_59: "f32[128, 192, 431]" = torch.ops.aten.group_norm.default(convolution_78, 1, p_decoder_2_dconv_layers_0_4_weight, p_decoder_2_dconv_layers_0_4_bias);  convolution_78 = p_decoder_2_dconv_layers_0_4_weight = p_decoder_2_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_37: "f32[128, 96, 431]" = torch.ops.aten.glu.default(group_norm_59, 1);  group_norm_59 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_54: "f32[96]" = torch.ops.aten.slice.Tensor(p_decoder_2_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_decoder_2_dconv_layers_0_6_scale = None
            unsqueeze_43: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_54, 1);  slice_54 = None
            mul_59: "f32[128, 96, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_43, glu_37);  unsqueeze_43 = glu_37 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_65: "f32[128, 96, 431]" = torch.ops.aten.add.Tensor(view_176, mul_59);  view_176 = mul_59 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_79: "f32[128, 12, 431]" = torch.ops.aten.convolution.default(add_65, p_decoder_2_dconv_layers_1_0_weight, p_decoder_2_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_decoder_2_dconv_layers_1_0_weight = p_decoder_2_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_60: "f32[128, 12, 431]" = torch.ops.aten.group_norm.default(convolution_79, 1, p_decoder_2_dconv_layers_1_1_weight, p_decoder_2_dconv_layers_1_1_bias);  convolution_79 = p_decoder_2_dconv_layers_1_1_weight = p_decoder_2_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_47: "f32[128, 12, 431]" = torch.ops.aten.gelu.default(group_norm_60);  group_norm_60 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_80: "f32[128, 192, 431]" = torch.ops.aten.convolution.default(gelu_47, p_decoder_2_dconv_layers_1_3_weight, p_decoder_2_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_47 = p_decoder_2_dconv_layers_1_3_weight = p_decoder_2_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_61: "f32[128, 192, 431]" = torch.ops.aten.group_norm.default(convolution_80, 1, p_decoder_2_dconv_layers_1_4_weight, p_decoder_2_dconv_layers_1_4_bias);  convolution_80 = p_decoder_2_dconv_layers_1_4_weight = p_decoder_2_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_38: "f32[128, 96, 431]" = torch.ops.aten.glu.default(group_norm_61, 1);  group_norm_61 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_55: "f32[96]" = torch.ops.aten.slice.Tensor(p_decoder_2_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_decoder_2_dconv_layers_1_6_scale = None
            unsqueeze_44: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_55, 1);  slice_55 = None
            mul_60: "f32[128, 96, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_44, glu_38);  unsqueeze_44 = glu_38 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_66: "f32[128, 96, 431]" = torch.ops.aten.add.Tensor(add_65, mul_60);  add_65 = mul_60 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:322 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_177: "f32[1, 128, 96, 431]" = torch.ops.aten.view.default(add_66, [1, 128, 96, 431]);  add_66 = None
            permute_30: "f32[1, 96, 128, 431]" = torch.ops.aten.permute.default(view_177, [0, 2, 1, 3]);  view_177 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:1150 in forward, code: return F.conv_transpose2d(
            convolution_81: "f32[1, 48, 516, 431]" = torch.ops.aten.convolution.default(permute_30, p_decoder_2_conv_tr_weight, p_decoder_2_conv_tr_bias, [4, 1], [0, 0], [1, 1], True, [0, 0], 1);  permute_30 = p_decoder_2_conv_tr_weight = p_decoder_2_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:329 in forward, code: z = z[..., self.pad:-self.pad, :]
            slice_56: "f32[1, 48, 512, 431]" = torch.ops.aten.slice.Tensor(convolution_81, 2, 2, -2);  convolution_81 = None
            slice_57: "f32[1, 48, 512, 431]" = torch.ops.aten.slice.Tensor(slice_56, 3, 0, 9223372036854775807);  slice_56 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:334 in forward, code: z = F.gelu(z)
            gelu_48: "f32[1, 48, 512, 431]" = torch.ops.aten.gelu.default(slice_57);  slice_57 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_67: "f32[1, 96, 27563]" = torch.ops.aten.add.Tensor(gelu_45, glu_8);  gelu_45 = glu_8 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_82: "f32[1, 192, 27563]" = torch.ops.aten.convolution.default(add_67, p_tdecoder_2_rewrite_weight, p_tdecoder_2_rewrite_bias, [1], [1], [1], False, [0], 1);  add_67 = p_tdecoder_2_rewrite_weight = p_tdecoder_2_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_39: "f32[1, 96, 27563]" = torch.ops.aten.glu.default(convolution_82, 1);  convolution_82 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_83: "f32[1, 12, 27563]" = torch.ops.aten.convolution.default(glu_39, p_tdecoder_2_dconv_layers_0_0_weight, p_tdecoder_2_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tdecoder_2_dconv_layers_0_0_weight = p_tdecoder_2_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_62: "f32[1, 12, 27563]" = torch.ops.aten.group_norm.default(convolution_83, 1, p_tdecoder_2_dconv_layers_0_1_weight, p_tdecoder_2_dconv_layers_0_1_bias);  convolution_83 = p_tdecoder_2_dconv_layers_0_1_weight = p_tdecoder_2_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_49: "f32[1, 12, 27563]" = torch.ops.aten.gelu.default(group_norm_62);  group_norm_62 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_84: "f32[1, 192, 27563]" = torch.ops.aten.convolution.default(gelu_49, p_tdecoder_2_dconv_layers_0_3_weight, p_tdecoder_2_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_49 = p_tdecoder_2_dconv_layers_0_3_weight = p_tdecoder_2_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_63: "f32[1, 192, 27563]" = torch.ops.aten.group_norm.default(convolution_84, 1, p_tdecoder_2_dconv_layers_0_4_weight, p_tdecoder_2_dconv_layers_0_4_bias);  convolution_84 = p_tdecoder_2_dconv_layers_0_4_weight = p_tdecoder_2_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_40: "f32[1, 96, 27563]" = torch.ops.aten.glu.default(group_norm_63, 1);  group_norm_63 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_58: "f32[96]" = torch.ops.aten.slice.Tensor(p_tdecoder_2_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_2_dconv_layers_0_6_scale = None
            unsqueeze_45: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_58, 1);  slice_58 = None
            mul_61: "f32[1, 96, 27563]" = torch.ops.aten.mul.Tensor(unsqueeze_45, glu_40);  unsqueeze_45 = glu_40 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_68: "f32[1, 96, 27563]" = torch.ops.aten.add.Tensor(glu_39, mul_61);  glu_39 = mul_61 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_85: "f32[1, 12, 27563]" = torch.ops.aten.convolution.default(add_68, p_tdecoder_2_dconv_layers_1_0_weight, p_tdecoder_2_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tdecoder_2_dconv_layers_1_0_weight = p_tdecoder_2_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_64: "f32[1, 12, 27563]" = torch.ops.aten.group_norm.default(convolution_85, 1, p_tdecoder_2_dconv_layers_1_1_weight, p_tdecoder_2_dconv_layers_1_1_bias);  convolution_85 = p_tdecoder_2_dconv_layers_1_1_weight = p_tdecoder_2_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_50: "f32[1, 12, 27563]" = torch.ops.aten.gelu.default(group_norm_64);  group_norm_64 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_86: "f32[1, 192, 27563]" = torch.ops.aten.convolution.default(gelu_50, p_tdecoder_2_dconv_layers_1_3_weight, p_tdecoder_2_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_50 = p_tdecoder_2_dconv_layers_1_3_weight = p_tdecoder_2_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_65: "f32[1, 192, 27563]" = torch.ops.aten.group_norm.default(convolution_86, 1, p_tdecoder_2_dconv_layers_1_4_weight, p_tdecoder_2_dconv_layers_1_4_bias);  convolution_86 = p_tdecoder_2_dconv_layers_1_4_weight = p_tdecoder_2_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_41: "f32[1, 96, 27563]" = torch.ops.aten.glu.default(group_norm_65, 1);  group_norm_65 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_59: "f32[96]" = torch.ops.aten.slice.Tensor(p_tdecoder_2_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_2_dconv_layers_1_6_scale = None
            unsqueeze_46: "f32[96, 1]" = torch.ops.aten.unsqueeze.default(slice_59, 1);  slice_59 = None
            mul_62: "f32[1, 96, 27563]" = torch.ops.aten.mul.Tensor(unsqueeze_46, glu_41);  unsqueeze_46 = glu_41 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_69: "f32[1, 96, 27563]" = torch.ops.aten.add.Tensor(add_68, mul_62);  add_68 = mul_62 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:964 in forward, code: return F.conv_transpose1d(
            convolution_87: "f32[1, 48, 110256]" = torch.ops.aten.convolution.default(add_69, p_tdecoder_2_conv_tr_weight, p_tdecoder_2_conv_tr_bias, [4], [0], [1], True, [0], 1);  add_69 = p_tdecoder_2_conv_tr_weight = p_tdecoder_2_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:331 in forward, code: z = z[..., self.pad:self.pad + length]
            slice_60: "f32[1, 48, 110250]" = torch.ops.aten.slice.Tensor(convolution_87, 2, 2, 110252);  convolution_87 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:334 in forward, code: z = F.gelu(z)
            gelu_51: "f32[1, 48, 110250]" = torch.ops.aten.gelu.default(slice_60);  slice_60 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_70: "f32[1, 48, 512, 431]" = torch.ops.aten.add.Tensor(gelu_48, add_16);  gelu_48 = add_16 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:549 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_88: "f32[1, 96, 512, 431]" = torch.ops.aten.convolution.default(add_70, p_decoder_3_rewrite_weight, p_decoder_3_rewrite_bias, [1, 1], [1, 1], [1, 1], False, [0, 0], 1);  add_70 = p_decoder_3_rewrite_weight = p_decoder_3_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_42: "f32[1, 48, 512, 431]" = torch.ops.aten.glu.default(convolution_88, 1);  convolution_88 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:319 in forward, code: y = y.permute(0, 2, 1, 3).reshape(-1, C, T)
            permute_31: "f32[1, 512, 48, 431]" = torch.ops.aten.permute.default(glu_42, [0, 2, 1, 3]);  glu_42 = None
            view_178: "f32[512, 48, 431]" = torch.ops.aten.view.default(permute_31, [512, 48, 431]);  permute_31 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_89: "f32[512, 6, 431]" = torch.ops.aten.convolution.default(view_178, p_decoder_3_dconv_layers_0_0_weight, p_decoder_3_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_decoder_3_dconv_layers_0_0_weight = p_decoder_3_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_66: "f32[512, 6, 431]" = torch.ops.aten.group_norm.default(convolution_89, 1, p_decoder_3_dconv_layers_0_1_weight, p_decoder_3_dconv_layers_0_1_bias);  convolution_89 = p_decoder_3_dconv_layers_0_1_weight = p_decoder_3_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_52: "f32[512, 6, 431]" = torch.ops.aten.gelu.default(group_norm_66);  group_norm_66 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_90: "f32[512, 96, 431]" = torch.ops.aten.convolution.default(gelu_52, p_decoder_3_dconv_layers_0_3_weight, p_decoder_3_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_52 = p_decoder_3_dconv_layers_0_3_weight = p_decoder_3_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_67: "f32[512, 96, 431]" = torch.ops.aten.group_norm.default(convolution_90, 1, p_decoder_3_dconv_layers_0_4_weight, p_decoder_3_dconv_layers_0_4_bias);  convolution_90 = p_decoder_3_dconv_layers_0_4_weight = p_decoder_3_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_43: "f32[512, 48, 431]" = torch.ops.aten.glu.default(group_norm_67, 1);  group_norm_67 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_61: "f32[48]" = torch.ops.aten.slice.Tensor(p_decoder_3_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_decoder_3_dconv_layers_0_6_scale = None
            unsqueeze_47: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_61, 1);  slice_61 = None
            mul_63: "f32[512, 48, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_47, glu_43);  unsqueeze_47 = glu_43 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_71: "f32[512, 48, 431]" = torch.ops.aten.add.Tensor(view_178, mul_63);  view_178 = mul_63 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_91: "f32[512, 6, 431]" = torch.ops.aten.convolution.default(add_71, p_decoder_3_dconv_layers_1_0_weight, p_decoder_3_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_decoder_3_dconv_layers_1_0_weight = p_decoder_3_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_68: "f32[512, 6, 431]" = torch.ops.aten.group_norm.default(convolution_91, 1, p_decoder_3_dconv_layers_1_1_weight, p_decoder_3_dconv_layers_1_1_bias);  convolution_91 = p_decoder_3_dconv_layers_1_1_weight = p_decoder_3_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_53: "f32[512, 6, 431]" = torch.ops.aten.gelu.default(group_norm_68);  group_norm_68 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_92: "f32[512, 96, 431]" = torch.ops.aten.convolution.default(gelu_53, p_decoder_3_dconv_layers_1_3_weight, p_decoder_3_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_53 = p_decoder_3_dconv_layers_1_3_weight = p_decoder_3_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_69: "f32[512, 96, 431]" = torch.ops.aten.group_norm.default(convolution_92, 1, p_decoder_3_dconv_layers_1_4_weight, p_decoder_3_dconv_layers_1_4_bias);  convolution_92 = p_decoder_3_dconv_layers_1_4_weight = p_decoder_3_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_44: "f32[512, 48, 431]" = torch.ops.aten.glu.default(group_norm_69, 1);  group_norm_69 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_62: "f32[48]" = torch.ops.aten.slice.Tensor(p_decoder_3_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_decoder_3_dconv_layers_1_6_scale = None
            unsqueeze_48: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_62, 1);  slice_62 = None
            mul_64: "f32[512, 48, 431]" = torch.ops.aten.mul.Tensor(unsqueeze_48, glu_44);  unsqueeze_48 = glu_44 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_72: "f32[512, 48, 431]" = torch.ops.aten.add.Tensor(add_71, mul_64);  add_71 = mul_64 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:322 in forward, code: y = y.view(B, Fr, C, T).permute(0, 2, 1, 3)
            view_179: "f32[1, 512, 48, 431]" = torch.ops.aten.view.default(add_72, [1, 512, 48, 431]);  add_72 = None
            permute_32: "f32[1, 48, 512, 431]" = torch.ops.aten.permute.default(view_179, [0, 2, 1, 3]);  view_179 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:1150 in forward, code: return F.conv_transpose2d(
            convolution_93: "f32[1, 16, 2052, 431]" = torch.ops.aten.convolution.default(permute_32, p_decoder_3_conv_tr_weight, p_decoder_3_conv_tr_bias, [4, 1], [0, 0], [1, 1], True, [0, 0], 1);  permute_32 = p_decoder_3_conv_tr_weight = p_decoder_3_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:329 in forward, code: z = z[..., self.pad:-self.pad, :]
            slice_63: "f32[1, 16, 2048, 431]" = torch.ops.aten.slice.Tensor(convolution_93, 2, 2, -2);  convolution_93 = None
            slice_64: "f32[1, 16, 2048, 431]" = torch.ops.aten.slice.Tensor(slice_63, 3, 0, 9223372036854775807);  slice_63 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:310 in forward, code: x = x + skip
            add_73: "f32[1, 48, 110250]" = torch.ops.aten.add.Tensor(gelu_51, glu_2);  gelu_51 = glu_2 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_94: "f32[1, 96, 110250]" = torch.ops.aten.convolution.default(add_73, p_tdecoder_3_rewrite_weight, p_tdecoder_3_rewrite_bias, [1], [1], [1], False, [0], 1);  add_73 = p_tdecoder_3_rewrite_weight = p_tdecoder_3_rewrite_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:313 in forward, code: y = F.glu(self.norm1(self.rewrite(x)), dim=1)
            glu_45: "f32[1, 48, 110250]" = torch.ops.aten.glu.default(convolution_94, 1);  convolution_94 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_95: "f32[1, 6, 110250]" = torch.ops.aten.convolution.default(glu_45, p_tdecoder_3_dconv_layers_0_0_weight, p_tdecoder_3_dconv_layers_0_0_bias, [1], [1], [1], False, [0], 1);  p_tdecoder_3_dconv_layers_0_0_weight = p_tdecoder_3_dconv_layers_0_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_70: "f32[1, 6, 110250]" = torch.ops.aten.group_norm.default(convolution_95, 1, p_tdecoder_3_dconv_layers_0_1_weight, p_tdecoder_3_dconv_layers_0_1_bias);  convolution_95 = p_tdecoder_3_dconv_layers_0_1_weight = p_tdecoder_3_dconv_layers_0_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_54: "f32[1, 6, 110250]" = torch.ops.aten.gelu.default(group_norm_70);  group_norm_70 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_96: "f32[1, 96, 110250]" = torch.ops.aten.convolution.default(gelu_54, p_tdecoder_3_dconv_layers_0_3_weight, p_tdecoder_3_dconv_layers_0_3_bias, [1], [0], [1], False, [0], 1);  gelu_54 = p_tdecoder_3_dconv_layers_0_3_weight = p_tdecoder_3_dconv_layers_0_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_71: "f32[1, 96, 110250]" = torch.ops.aten.group_norm.default(convolution_96, 1, p_tdecoder_3_dconv_layers_0_4_weight, p_tdecoder_3_dconv_layers_0_4_bias);  convolution_96 = p_tdecoder_3_dconv_layers_0_4_weight = p_tdecoder_3_dconv_layers_0_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_46: "f32[1, 48, 110250]" = torch.ops.aten.glu.default(group_norm_71, 1);  group_norm_71 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_65: "f32[48]" = torch.ops.aten.slice.Tensor(p_tdecoder_3_dconv_layers_0_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_3_dconv_layers_0_6_scale = None
            unsqueeze_49: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_65, 1);  slice_65 = None
            mul_65: "f32[1, 48, 110250]" = torch.ops.aten.mul.Tensor(unsqueeze_49, glu_46);  unsqueeze_49 = glu_46 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_74: "f32[1, 48, 110250]" = torch.ops.aten.add.Tensor(glu_45, mul_65);  glu_45 = mul_65 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_97: "f32[1, 6, 110250]" = torch.ops.aten.convolution.default(add_74, p_tdecoder_3_dconv_layers_1_0_weight, p_tdecoder_3_dconv_layers_1_0_bias, [1], [2], [2], False, [0], 1);  p_tdecoder_3_dconv_layers_1_0_weight = p_tdecoder_3_dconv_layers_1_0_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_72: "f32[1, 6, 110250]" = torch.ops.aten.group_norm.default(convolution_97, 1, p_tdecoder_3_dconv_layers_1_1_weight, p_tdecoder_3_dconv_layers_1_1_bias);  convolution_97 = p_tdecoder_3_dconv_layers_1_1_weight = p_tdecoder_3_dconv_layers_1_1_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:734 in forward, code: return F.gelu(input, approximate=self.approximate)
            gelu_55: "f32[1, 6, 110250]" = torch.ops.aten.gelu.default(group_norm_72);  group_norm_72 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:373 in forward, code: return self._conv_forward(input, self.weight, self.bias)
            convolution_98: "f32[1, 96, 110250]" = torch.ops.aten.convolution.default(gelu_55, p_tdecoder_3_dconv_layers_1_3_weight, p_tdecoder_3_dconv_layers_1_3_bias, [1], [0], [1], False, [0], 1);  gelu_55 = p_tdecoder_3_dconv_layers_1_3_weight = p_tdecoder_3_dconv_layers_1_3_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/normalization.py:313 in forward, code: return F.group_norm(input, self.num_groups, self.weight, self.bias, self.eps)
            group_norm_73: "f32[1, 96, 110250]" = torch.ops.aten.group_norm.default(convolution_98, 1, p_tdecoder_3_dconv_layers_1_4_weight, p_tdecoder_3_dconv_layers_1_4_bias);  convolution_98 = p_tdecoder_3_dconv_layers_1_4_weight = p_tdecoder_3_dconv_layers_1_4_bias = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/activation.py:692 in forward, code: return F.glu(input, self.dim)
            glu_47: "f32[1, 48, 110250]" = torch.ops.aten.glu.default(group_norm_73, 1);  group_norm_73 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/transformer.py:255 in forward, code: return self.scale[:, None] * x
            slice_66: "f32[48]" = torch.ops.aten.slice.Tensor(p_tdecoder_3_dconv_layers_1_6_scale, 0, 0, 9223372036854775807);  p_tdecoder_3_dconv_layers_1_6_scale = None
            unsqueeze_50: "f32[48, 1]" = torch.ops.aten.unsqueeze.default(slice_66, 1);  slice_66 = None
            mul_66: "f32[1, 48, 110250]" = torch.ops.aten.mul.Tensor(unsqueeze_50, glu_47);  unsqueeze_50 = glu_47 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/demucs.py:153 in forward, code: x = x + layer(x)
            add_75: "f32[1, 48, 110250]" = torch.ops.aten.add.Tensor(add_74, mul_66);  add_74 = mul_66 = None
            
             # File: /home/gianlorenzo/miniconda3/envs/torch-nightly/lib/python3.10/site-packages/torch/nn/modules/conv.py:964 in forward, code: return F.conv_transpose1d(
            convolution_99: "f32[1, 8, 441004]" = torch.ops.aten.convolution.default(add_75, p_tdecoder_3_conv_tr_weight, p_tdecoder_3_conv_tr_bias, [4], [0], [1], True, [0], 1);  add_75 = p_tdecoder_3_conv_tr_weight = p_tdecoder_3_conv_tr_bias = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/hdemucs.py:331 in forward, code: z = z[..., self.pad:self.pad + length]
            slice_67: "f32[1, 8, 441000]" = torch.ops.aten.slice.Tensor(convolution_99, 2, 2, 441002);  convolution_99 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:516 in forward, code: x = x.view(B, S, -1, Fq, T)
            view_180: "f32[1, 4, 4, 2048, 431]" = torch.ops.aten.view.default(slice_64, [1, 4, -1, 2048, 431]);  slice_64 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:517 in forward, code: x = x * std[:, None] + mean[:, None]
            slice_68: "f32[1, 1, 1, 1]" = torch.ops.aten.slice.Tensor(sqrt, 0, 0, 9223372036854775807);  sqrt = None
            unsqueeze_51: "f32[1, 1, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_68, 1);  slice_68 = None
            mul_67: "f32[1, 4, 4, 2048, 431]" = torch.ops.aten.mul.Tensor(view_180, unsqueeze_51);  view_180 = unsqueeze_51 = None
            slice_69: "f32[1, 1, 1, 1]" = torch.ops.aten.slice.Tensor(mean, 0, 0, 9223372036854775807);  mean = None
            unsqueeze_52: "f32[1, 1, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_69, 1);  slice_69 = None
            add_76: "f32[1, 4, 4, 2048, 431]" = torch.ops.aten.add.Tensor(mul_67, unsqueeze_52);  mul_67 = unsqueeze_52 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:519 in forward, code: xt = xt.view(B, S, -1, length)
            view_181: "f32[1, 4, 2, 441000]" = torch.ops.aten.view.default(slice_67, [1, 4, -1, 441000]);  slice_67 = None
            
             # File: /home/gianlorenzo/INTERN/demucs-fork/demucs/htdemucs.py:520 in forward, code: xt = xt * stdt[:, None] + meant[:, None]
            slice_70: "f32[1, 1, 1]" = torch.ops.aten.slice.Tensor(sqrt_1, 0, 0, 9223372036854775807);  sqrt_1 = None
            unsqueeze_53: "f32[1, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_70, 1);  slice_70 = None
            mul_68: "f32[1, 4, 2, 441000]" = torch.ops.aten.mul.Tensor(view_181, unsqueeze_53);  view_181 = unsqueeze_53 = None
            slice_71: "f32[1, 1, 1]" = torch.ops.aten.slice.Tensor(mean_1, 0, 0, 9223372036854775807);  mean_1 = None
            unsqueeze_54: "f32[1, 1, 1, 1]" = torch.ops.aten.unsqueeze.default(slice_71, 1);  slice_71 = None
            add_77: "f32[1, 4, 2, 441000]" = torch.ops.aten.add.Tensor(mul_68, unsqueeze_54);  mul_68 = unsqueeze_54 = None
            return (add_76, add_77)
            
Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_conv_weight'), target='encoder.0.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_conv_bias'), target='encoder.0.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_rewrite_weight'), target='encoder.0.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_rewrite_bias'), target='encoder.0.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_0_weight'), target='encoder.0.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_0_bias'), target='encoder.0.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_1_weight'), target='encoder.0.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_1_bias'), target='encoder.0.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_3_weight'), target='encoder.0.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_3_bias'), target='encoder.0.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_4_weight'), target='encoder.0.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_4_bias'), target='encoder.0.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_0_6_scale'), target='encoder.0.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_0_weight'), target='encoder.0.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_0_bias'), target='encoder.0.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_1_weight'), target='encoder.0.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_1_bias'), target='encoder.0.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_3_weight'), target='encoder.0.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_3_bias'), target='encoder.0.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_4_weight'), target='encoder.0.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_4_bias'), target='encoder.0.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_0_dconv_layers_1_6_scale'), target='encoder.0.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_conv_weight'), target='encoder.1.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_conv_bias'), target='encoder.1.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_rewrite_weight'), target='encoder.1.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_rewrite_bias'), target='encoder.1.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_0_weight'), target='encoder.1.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_0_bias'), target='encoder.1.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_1_weight'), target='encoder.1.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_1_bias'), target='encoder.1.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_3_weight'), target='encoder.1.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_3_bias'), target='encoder.1.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_4_weight'), target='encoder.1.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_4_bias'), target='encoder.1.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_0_6_scale'), target='encoder.1.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_0_weight'), target='encoder.1.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_0_bias'), target='encoder.1.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_1_weight'), target='encoder.1.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_1_bias'), target='encoder.1.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_3_weight'), target='encoder.1.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_3_bias'), target='encoder.1.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_4_weight'), target='encoder.1.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_4_bias'), target='encoder.1.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_1_dconv_layers_1_6_scale'), target='encoder.1.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_conv_weight'), target='encoder.2.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_conv_bias'), target='encoder.2.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_rewrite_weight'), target='encoder.2.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_rewrite_bias'), target='encoder.2.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_0_weight'), target='encoder.2.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_0_bias'), target='encoder.2.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_1_weight'), target='encoder.2.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_1_bias'), target='encoder.2.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_3_weight'), target='encoder.2.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_3_bias'), target='encoder.2.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_4_weight'), target='encoder.2.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_4_bias'), target='encoder.2.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_0_6_scale'), target='encoder.2.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_0_weight'), target='encoder.2.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_0_bias'), target='encoder.2.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_1_weight'), target='encoder.2.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_1_bias'), target='encoder.2.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_3_weight'), target='encoder.2.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_3_bias'), target='encoder.2.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_4_weight'), target='encoder.2.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_4_bias'), target='encoder.2.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_2_dconv_layers_1_6_scale'), target='encoder.2.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_conv_weight'), target='encoder.3.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_conv_bias'), target='encoder.3.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_rewrite_weight'), target='encoder.3.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_rewrite_bias'), target='encoder.3.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_0_weight'), target='encoder.3.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_0_bias'), target='encoder.3.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_1_weight'), target='encoder.3.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_1_bias'), target='encoder.3.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_3_weight'), target='encoder.3.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_3_bias'), target='encoder.3.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_4_weight'), target='encoder.3.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_4_bias'), target='encoder.3.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_0_6_scale'), target='encoder.3.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_0_weight'), target='encoder.3.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_0_bias'), target='encoder.3.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_1_weight'), target='encoder.3.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_1_bias'), target='encoder.3.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_3_weight'), target='encoder.3.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_3_bias'), target='encoder.3.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_4_weight'), target='encoder.3.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_4_bias'), target='encoder.3.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_3_dconv_layers_1_6_scale'), target='encoder.3.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_conv_tr_weight'), target='decoder.0.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_conv_tr_bias'), target='decoder.0.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_rewrite_weight'), target='decoder.0.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_rewrite_bias'), target='decoder.0.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_0_weight'), target='decoder.0.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_0_bias'), target='decoder.0.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_1_weight'), target='decoder.0.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_1_bias'), target='decoder.0.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_3_weight'), target='decoder.0.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_3_bias'), target='decoder.0.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_4_weight'), target='decoder.0.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_4_bias'), target='decoder.0.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_0_6_scale'), target='decoder.0.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_0_weight'), target='decoder.0.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_0_bias'), target='decoder.0.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_1_weight'), target='decoder.0.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_1_bias'), target='decoder.0.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_3_weight'), target='decoder.0.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_3_bias'), target='decoder.0.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_4_weight'), target='decoder.0.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_4_bias'), target='decoder.0.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_0_dconv_layers_1_6_scale'), target='decoder.0.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_conv_tr_weight'), target='decoder.1.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_conv_tr_bias'), target='decoder.1.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_rewrite_weight'), target='decoder.1.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_rewrite_bias'), target='decoder.1.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_0_weight'), target='decoder.1.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_0_bias'), target='decoder.1.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_1_weight'), target='decoder.1.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_1_bias'), target='decoder.1.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_3_weight'), target='decoder.1.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_3_bias'), target='decoder.1.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_4_weight'), target='decoder.1.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_4_bias'), target='decoder.1.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_0_6_scale'), target='decoder.1.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_0_weight'), target='decoder.1.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_0_bias'), target='decoder.1.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_1_weight'), target='decoder.1.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_1_bias'), target='decoder.1.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_3_weight'), target='decoder.1.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_3_bias'), target='decoder.1.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_4_weight'), target='decoder.1.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_4_bias'), target='decoder.1.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_1_dconv_layers_1_6_scale'), target='decoder.1.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_conv_tr_weight'), target='decoder.2.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_conv_tr_bias'), target='decoder.2.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_rewrite_weight'), target='decoder.2.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_rewrite_bias'), target='decoder.2.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_0_weight'), target='decoder.2.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_0_bias'), target='decoder.2.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_1_weight'), target='decoder.2.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_1_bias'), target='decoder.2.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_3_weight'), target='decoder.2.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_3_bias'), target='decoder.2.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_4_weight'), target='decoder.2.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_4_bias'), target='decoder.2.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_0_6_scale'), target='decoder.2.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_0_weight'), target='decoder.2.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_0_bias'), target='decoder.2.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_1_weight'), target='decoder.2.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_1_bias'), target='decoder.2.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_3_weight'), target='decoder.2.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_3_bias'), target='decoder.2.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_4_weight'), target='decoder.2.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_4_bias'), target='decoder.2.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_2_dconv_layers_1_6_scale'), target='decoder.2.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_conv_tr_weight'), target='decoder.3.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_conv_tr_bias'), target='decoder.3.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_rewrite_weight'), target='decoder.3.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_rewrite_bias'), target='decoder.3.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_0_weight'), target='decoder.3.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_0_bias'), target='decoder.3.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_1_weight'), target='decoder.3.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_1_bias'), target='decoder.3.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_3_weight'), target='decoder.3.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_3_bias'), target='decoder.3.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_4_weight'), target='decoder.3.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_4_bias'), target='decoder.3.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_0_6_scale'), target='decoder.3.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_0_weight'), target='decoder.3.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_0_bias'), target='decoder.3.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_1_weight'), target='decoder.3.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_1_bias'), target='decoder.3.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_3_weight'), target='decoder.3.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_3_bias'), target='decoder.3.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_4_weight'), target='decoder.3.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_4_bias'), target='decoder.3.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_decoder_3_dconv_layers_1_6_scale'), target='decoder.3.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_conv_weight'), target='tencoder.0.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_conv_bias'), target='tencoder.0.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_rewrite_weight'), target='tencoder.0.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_rewrite_bias'), target='tencoder.0.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_0_weight'), target='tencoder.0.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_0_bias'), target='tencoder.0.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_1_weight'), target='tencoder.0.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_1_bias'), target='tencoder.0.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_3_weight'), target='tencoder.0.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_3_bias'), target='tencoder.0.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_4_weight'), target='tencoder.0.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_4_bias'), target='tencoder.0.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_0_6_scale'), target='tencoder.0.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_0_weight'), target='tencoder.0.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_0_bias'), target='tencoder.0.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_1_weight'), target='tencoder.0.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_1_bias'), target='tencoder.0.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_3_weight'), target='tencoder.0.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_3_bias'), target='tencoder.0.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_4_weight'), target='tencoder.0.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_4_bias'), target='tencoder.0.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_0_dconv_layers_1_6_scale'), target='tencoder.0.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_conv_weight'), target='tencoder.1.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_conv_bias'), target='tencoder.1.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_rewrite_weight'), target='tencoder.1.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_rewrite_bias'), target='tencoder.1.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_0_weight'), target='tencoder.1.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_0_bias'), target='tencoder.1.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_1_weight'), target='tencoder.1.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_1_bias'), target='tencoder.1.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_3_weight'), target='tencoder.1.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_3_bias'), target='tencoder.1.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_4_weight'), target='tencoder.1.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_4_bias'), target='tencoder.1.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_0_6_scale'), target='tencoder.1.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_0_weight'), target='tencoder.1.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_0_bias'), target='tencoder.1.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_1_weight'), target='tencoder.1.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_1_bias'), target='tencoder.1.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_3_weight'), target='tencoder.1.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_3_bias'), target='tencoder.1.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_4_weight'), target='tencoder.1.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_4_bias'), target='tencoder.1.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_1_dconv_layers_1_6_scale'), target='tencoder.1.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_conv_weight'), target='tencoder.2.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_conv_bias'), target='tencoder.2.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_rewrite_weight'), target='tencoder.2.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_rewrite_bias'), target='tencoder.2.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_0_weight'), target='tencoder.2.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_0_bias'), target='tencoder.2.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_1_weight'), target='tencoder.2.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_1_bias'), target='tencoder.2.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_3_weight'), target='tencoder.2.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_3_bias'), target='tencoder.2.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_4_weight'), target='tencoder.2.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_4_bias'), target='tencoder.2.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_0_6_scale'), target='tencoder.2.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_0_weight'), target='tencoder.2.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_0_bias'), target='tencoder.2.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_1_weight'), target='tencoder.2.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_1_bias'), target='tencoder.2.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_3_weight'), target='tencoder.2.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_3_bias'), target='tencoder.2.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_4_weight'), target='tencoder.2.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_4_bias'), target='tencoder.2.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_2_dconv_layers_1_6_scale'), target='tencoder.2.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_conv_weight'), target='tencoder.3.conv.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_conv_bias'), target='tencoder.3.conv.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_rewrite_weight'), target='tencoder.3.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_rewrite_bias'), target='tencoder.3.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_0_weight'), target='tencoder.3.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_0_bias'), target='tencoder.3.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_1_weight'), target='tencoder.3.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_1_bias'), target='tencoder.3.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_3_weight'), target='tencoder.3.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_3_bias'), target='tencoder.3.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_4_weight'), target='tencoder.3.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_4_bias'), target='tencoder.3.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_0_6_scale'), target='tencoder.3.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_0_weight'), target='tencoder.3.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_0_bias'), target='tencoder.3.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_1_weight'), target='tencoder.3.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_1_bias'), target='tencoder.3.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_3_weight'), target='tencoder.3.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_3_bias'), target='tencoder.3.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_4_weight'), target='tencoder.3.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_4_bias'), target='tencoder.3.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tencoder_3_dconv_layers_1_6_scale'), target='tencoder.3.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_conv_tr_weight'), target='tdecoder.0.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_conv_tr_bias'), target='tdecoder.0.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_rewrite_weight'), target='tdecoder.0.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_rewrite_bias'), target='tdecoder.0.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_0_weight'), target='tdecoder.0.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_0_bias'), target='tdecoder.0.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_1_weight'), target='tdecoder.0.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_1_bias'), target='tdecoder.0.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_3_weight'), target='tdecoder.0.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_3_bias'), target='tdecoder.0.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_4_weight'), target='tdecoder.0.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_4_bias'), target='tdecoder.0.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_0_6_scale'), target='tdecoder.0.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_0_weight'), target='tdecoder.0.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_0_bias'), target='tdecoder.0.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_1_weight'), target='tdecoder.0.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_1_bias'), target='tdecoder.0.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_3_weight'), target='tdecoder.0.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_3_bias'), target='tdecoder.0.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_4_weight'), target='tdecoder.0.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_4_bias'), target='tdecoder.0.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_0_dconv_layers_1_6_scale'), target='tdecoder.0.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_conv_tr_weight'), target='tdecoder.1.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_conv_tr_bias'), target='tdecoder.1.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_rewrite_weight'), target='tdecoder.1.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_rewrite_bias'), target='tdecoder.1.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_0_weight'), target='tdecoder.1.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_0_bias'), target='tdecoder.1.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_1_weight'), target='tdecoder.1.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_1_bias'), target='tdecoder.1.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_3_weight'), target='tdecoder.1.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_3_bias'), target='tdecoder.1.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_4_weight'), target='tdecoder.1.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_4_bias'), target='tdecoder.1.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_0_6_scale'), target='tdecoder.1.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_0_weight'), target='tdecoder.1.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_0_bias'), target='tdecoder.1.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_1_weight'), target='tdecoder.1.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_1_bias'), target='tdecoder.1.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_3_weight'), target='tdecoder.1.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_3_bias'), target='tdecoder.1.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_4_weight'), target='tdecoder.1.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_4_bias'), target='tdecoder.1.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_1_dconv_layers_1_6_scale'), target='tdecoder.1.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_conv_tr_weight'), target='tdecoder.2.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_conv_tr_bias'), target='tdecoder.2.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_rewrite_weight'), target='tdecoder.2.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_rewrite_bias'), target='tdecoder.2.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_0_weight'), target='tdecoder.2.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_0_bias'), target='tdecoder.2.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_1_weight'), target='tdecoder.2.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_1_bias'), target='tdecoder.2.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_3_weight'), target='tdecoder.2.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_3_bias'), target='tdecoder.2.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_4_weight'), target='tdecoder.2.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_4_bias'), target='tdecoder.2.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_0_6_scale'), target='tdecoder.2.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_0_weight'), target='tdecoder.2.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_0_bias'), target='tdecoder.2.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_1_weight'), target='tdecoder.2.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_1_bias'), target='tdecoder.2.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_3_weight'), target='tdecoder.2.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_3_bias'), target='tdecoder.2.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_4_weight'), target='tdecoder.2.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_4_bias'), target='tdecoder.2.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_2_dconv_layers_1_6_scale'), target='tdecoder.2.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_conv_tr_weight'), target='tdecoder.3.conv_tr.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_conv_tr_bias'), target='tdecoder.3.conv_tr.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_rewrite_weight'), target='tdecoder.3.rewrite.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_rewrite_bias'), target='tdecoder.3.rewrite.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_0_weight'), target='tdecoder.3.dconv.layers.0.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_0_bias'), target='tdecoder.3.dconv.layers.0.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_1_weight'), target='tdecoder.3.dconv.layers.0.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_1_bias'), target='tdecoder.3.dconv.layers.0.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_3_weight'), target='tdecoder.3.dconv.layers.0.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_3_bias'), target='tdecoder.3.dconv.layers.0.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_4_weight'), target='tdecoder.3.dconv.layers.0.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_4_bias'), target='tdecoder.3.dconv.layers.0.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_0_6_scale'), target='tdecoder.3.dconv.layers.0.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_0_weight'), target='tdecoder.3.dconv.layers.1.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_0_bias'), target='tdecoder.3.dconv.layers.1.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_1_weight'), target='tdecoder.3.dconv.layers.1.1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_1_bias'), target='tdecoder.3.dconv.layers.1.1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_3_weight'), target='tdecoder.3.dconv.layers.1.3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_3_bias'), target='tdecoder.3.dconv.layers.1.3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_4_weight'), target='tdecoder.3.dconv.layers.1.4.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_4_bias'), target='tdecoder.3.dconv.layers.1.4.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_tdecoder_3_dconv_layers_1_6_scale'), target='tdecoder.3.dconv.layers.1.6.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_freq_emb_embedding_weight'), target='freq_emb.embedding.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_upsampler_weight'), target='channel_upsampler.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_upsampler_bias'), target='channel_upsampler.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_downsampler_weight'), target='channel_downsampler.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_downsampler_bias'), target='channel_downsampler.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_upsampler_t_weight'), target='channel_upsampler_t.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_upsampler_t_bias'), target='channel_upsampler_t.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_downsampler_t_weight'), target='channel_downsampler_t.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_channel_downsampler_t_bias'), target='channel_downsampler_t.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_norm_in_weight'), target='crosstransformer.norm_in.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_norm_in_bias'), target='crosstransformer.norm_in.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_norm_in_t_weight'), target='crosstransformer.norm_in_t.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_norm_in_t_bias'), target='crosstransformer.norm_in_t.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_self_attn_in_proj_weight'), target='crosstransformer.layers.0.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_self_attn_in_proj_bias'), target='crosstransformer.layers.0.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_self_attn_out_proj_weight'), target='crosstransformer.layers.0.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_self_attn_out_proj_bias'), target='crosstransformer.layers.0.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_linear1_weight'), target='crosstransformer.layers.0.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_linear1_bias'), target='crosstransformer.layers.0.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_linear2_weight'), target='crosstransformer.layers.0.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_linear2_bias'), target='crosstransformer.layers.0.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_norm1_weight'), target='crosstransformer.layers.0.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_norm1_bias'), target='crosstransformer.layers.0.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_norm2_weight'), target='crosstransformer.layers.0.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_norm2_bias'), target='crosstransformer.layers.0.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_norm_out_weight'), target='crosstransformer.layers.0.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_norm_out_bias'), target='crosstransformer.layers.0.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_gamma_1_scale'), target='crosstransformer.layers.0.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_0_gamma_2_scale'), target='crosstransformer.layers.0.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_cross_attn_in_proj_weight'), target='crosstransformer.layers.1.cross_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_cross_attn_in_proj_bias'), target='crosstransformer.layers.1.cross_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_cross_attn_out_proj_weight'), target='crosstransformer.layers.1.cross_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_cross_attn_out_proj_bias'), target='crosstransformer.layers.1.cross_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_linear1_weight'), target='crosstransformer.layers.1.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_linear1_bias'), target='crosstransformer.layers.1.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_linear2_weight'), target='crosstransformer.layers.1.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_linear2_bias'), target='crosstransformer.layers.1.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm1_weight'), target='crosstransformer.layers.1.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm1_bias'), target='crosstransformer.layers.1.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm2_weight'), target='crosstransformer.layers.1.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm2_bias'), target='crosstransformer.layers.1.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm3_weight'), target='crosstransformer.layers.1.norm3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm3_bias'), target='crosstransformer.layers.1.norm3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm_out_weight'), target='crosstransformer.layers.1.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_norm_out_bias'), target='crosstransformer.layers.1.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_gamma_1_scale'), target='crosstransformer.layers.1.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_1_gamma_2_scale'), target='crosstransformer.layers.1.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_self_attn_in_proj_weight'), target='crosstransformer.layers.2.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_self_attn_in_proj_bias'), target='crosstransformer.layers.2.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_self_attn_out_proj_weight'), target='crosstransformer.layers.2.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_self_attn_out_proj_bias'), target='crosstransformer.layers.2.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_linear1_weight'), target='crosstransformer.layers.2.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_linear1_bias'), target='crosstransformer.layers.2.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_linear2_weight'), target='crosstransformer.layers.2.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_linear2_bias'), target='crosstransformer.layers.2.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_norm1_weight'), target='crosstransformer.layers.2.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_norm1_bias'), target='crosstransformer.layers.2.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_norm2_weight'), target='crosstransformer.layers.2.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_norm2_bias'), target='crosstransformer.layers.2.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_norm_out_weight'), target='crosstransformer.layers.2.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_norm_out_bias'), target='crosstransformer.layers.2.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_gamma_1_scale'), target='crosstransformer.layers.2.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_2_gamma_2_scale'), target='crosstransformer.layers.2.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_cross_attn_in_proj_weight'), target='crosstransformer.layers.3.cross_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_cross_attn_in_proj_bias'), target='crosstransformer.layers.3.cross_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_cross_attn_out_proj_weight'), target='crosstransformer.layers.3.cross_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_cross_attn_out_proj_bias'), target='crosstransformer.layers.3.cross_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_linear1_weight'), target='crosstransformer.layers.3.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_linear1_bias'), target='crosstransformer.layers.3.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_linear2_weight'), target='crosstransformer.layers.3.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_linear2_bias'), target='crosstransformer.layers.3.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm1_weight'), target='crosstransformer.layers.3.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm1_bias'), target='crosstransformer.layers.3.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm2_weight'), target='crosstransformer.layers.3.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm2_bias'), target='crosstransformer.layers.3.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm3_weight'), target='crosstransformer.layers.3.norm3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm3_bias'), target='crosstransformer.layers.3.norm3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm_out_weight'), target='crosstransformer.layers.3.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_norm_out_bias'), target='crosstransformer.layers.3.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_gamma_1_scale'), target='crosstransformer.layers.3.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_3_gamma_2_scale'), target='crosstransformer.layers.3.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_self_attn_in_proj_weight'), target='crosstransformer.layers.4.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_self_attn_in_proj_bias'), target='crosstransformer.layers.4.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_self_attn_out_proj_weight'), target='crosstransformer.layers.4.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_self_attn_out_proj_bias'), target='crosstransformer.layers.4.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_linear1_weight'), target='crosstransformer.layers.4.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_linear1_bias'), target='crosstransformer.layers.4.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_linear2_weight'), target='crosstransformer.layers.4.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_linear2_bias'), target='crosstransformer.layers.4.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_norm1_weight'), target='crosstransformer.layers.4.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_norm1_bias'), target='crosstransformer.layers.4.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_norm2_weight'), target='crosstransformer.layers.4.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_norm2_bias'), target='crosstransformer.layers.4.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_norm_out_weight'), target='crosstransformer.layers.4.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_norm_out_bias'), target='crosstransformer.layers.4.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_gamma_1_scale'), target='crosstransformer.layers.4.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_4_gamma_2_scale'), target='crosstransformer.layers.4.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_self_attn_in_proj_weight'), target='crosstransformer.layers_t.0.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_self_attn_in_proj_bias'), target='crosstransformer.layers_t.0.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_self_attn_out_proj_weight'), target='crosstransformer.layers_t.0.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_self_attn_out_proj_bias'), target='crosstransformer.layers_t.0.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_linear1_weight'), target='crosstransformer.layers_t.0.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_linear1_bias'), target='crosstransformer.layers_t.0.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_linear2_weight'), target='crosstransformer.layers_t.0.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_linear2_bias'), target='crosstransformer.layers_t.0.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_norm1_weight'), target='crosstransformer.layers_t.0.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_norm1_bias'), target='crosstransformer.layers_t.0.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_norm2_weight'), target='crosstransformer.layers_t.0.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_norm2_bias'), target='crosstransformer.layers_t.0.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_norm_out_weight'), target='crosstransformer.layers_t.0.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_norm_out_bias'), target='crosstransformer.layers_t.0.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_gamma_1_scale'), target='crosstransformer.layers_t.0.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_0_gamma_2_scale'), target='crosstransformer.layers_t.0.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_cross_attn_in_proj_weight'), target='crosstransformer.layers_t.1.cross_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_cross_attn_in_proj_bias'), target='crosstransformer.layers_t.1.cross_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_cross_attn_out_proj_weight'), target='crosstransformer.layers_t.1.cross_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_cross_attn_out_proj_bias'), target='crosstransformer.layers_t.1.cross_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_linear1_weight'), target='crosstransformer.layers_t.1.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_linear1_bias'), target='crosstransformer.layers_t.1.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_linear2_weight'), target='crosstransformer.layers_t.1.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_linear2_bias'), target='crosstransformer.layers_t.1.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm1_weight'), target='crosstransformer.layers_t.1.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm1_bias'), target='crosstransformer.layers_t.1.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm2_weight'), target='crosstransformer.layers_t.1.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm2_bias'), target='crosstransformer.layers_t.1.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm3_weight'), target='crosstransformer.layers_t.1.norm3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm3_bias'), target='crosstransformer.layers_t.1.norm3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm_out_weight'), target='crosstransformer.layers_t.1.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_norm_out_bias'), target='crosstransformer.layers_t.1.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_gamma_1_scale'), target='crosstransformer.layers_t.1.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_1_gamma_2_scale'), target='crosstransformer.layers_t.1.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_self_attn_in_proj_weight'), target='crosstransformer.layers_t.2.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_self_attn_in_proj_bias'), target='crosstransformer.layers_t.2.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_self_attn_out_proj_weight'), target='crosstransformer.layers_t.2.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_self_attn_out_proj_bias'), target='crosstransformer.layers_t.2.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_linear1_weight'), target='crosstransformer.layers_t.2.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_linear1_bias'), target='crosstransformer.layers_t.2.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_linear2_weight'), target='crosstransformer.layers_t.2.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_linear2_bias'), target='crosstransformer.layers_t.2.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_norm1_weight'), target='crosstransformer.layers_t.2.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_norm1_bias'), target='crosstransformer.layers_t.2.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_norm2_weight'), target='crosstransformer.layers_t.2.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_norm2_bias'), target='crosstransformer.layers_t.2.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_norm_out_weight'), target='crosstransformer.layers_t.2.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_norm_out_bias'), target='crosstransformer.layers_t.2.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_gamma_1_scale'), target='crosstransformer.layers_t.2.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_2_gamma_2_scale'), target='crosstransformer.layers_t.2.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_cross_attn_in_proj_weight'), target='crosstransformer.layers_t.3.cross_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_cross_attn_in_proj_bias'), target='crosstransformer.layers_t.3.cross_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_cross_attn_out_proj_weight'), target='crosstransformer.layers_t.3.cross_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_cross_attn_out_proj_bias'), target='crosstransformer.layers_t.3.cross_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_linear1_weight'), target='crosstransformer.layers_t.3.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_linear1_bias'), target='crosstransformer.layers_t.3.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_linear2_weight'), target='crosstransformer.layers_t.3.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_linear2_bias'), target='crosstransformer.layers_t.3.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm1_weight'), target='crosstransformer.layers_t.3.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm1_bias'), target='crosstransformer.layers_t.3.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm2_weight'), target='crosstransformer.layers_t.3.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm2_bias'), target='crosstransformer.layers_t.3.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm3_weight'), target='crosstransformer.layers_t.3.norm3.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm3_bias'), target='crosstransformer.layers_t.3.norm3.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm_out_weight'), target='crosstransformer.layers_t.3.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_norm_out_bias'), target='crosstransformer.layers_t.3.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_gamma_1_scale'), target='crosstransformer.layers_t.3.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_3_gamma_2_scale'), target='crosstransformer.layers_t.3.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_self_attn_in_proj_weight'), target='crosstransformer.layers_t.4.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_self_attn_in_proj_bias'), target='crosstransformer.layers_t.4.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_self_attn_out_proj_weight'), target='crosstransformer.layers_t.4.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_self_attn_out_proj_bias'), target='crosstransformer.layers_t.4.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_linear1_weight'), target='crosstransformer.layers_t.4.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_linear1_bias'), target='crosstransformer.layers_t.4.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_linear2_weight'), target='crosstransformer.layers_t.4.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_linear2_bias'), target='crosstransformer.layers_t.4.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_norm1_weight'), target='crosstransformer.layers_t.4.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_norm1_bias'), target='crosstransformer.layers_t.4.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_norm2_weight'), target='crosstransformer.layers_t.4.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_norm2_bias'), target='crosstransformer.layers_t.4.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_norm_out_weight'), target='crosstransformer.layers_t.4.norm_out.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_norm_out_bias'), target='crosstransformer.layers_t.4.norm_out.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_gamma_1_scale'), target='crosstransformer.layers_t.4.gamma_1.scale', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_crosstransformer_layers_t_4_gamma_2_scale'), target='crosstransformer.layers_t.4.gamma_2.scale', persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='mix'), target=None, persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='spec'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_76'), target=None), OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_77'), target=None)])
Range constraints: {s0: VR[2, int_oo], s1: VR[2, int_oo], s2: VR[2, int_oo], s3: VR[2, int_oo], s4: VR[2, int_oo]}

```

## ONNX model

```python
<
    ir_version=9,
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib': 1, 'pkg.onnxscript.torch_lib.common': 1},
    producer_name='torch',
    producer_version='2.5.0.dev20240828+cpu',
    domain=None,
    model_version=None,
>
graph(
    name=main_graph,
    inputs=(
        %"mix"<FLOAT,[1,2,441000]>,
        %"spec"<FLOAT,[1,2,2048,431,2]>
    ),
    outputs=(
        %"add_76"<FLOAT,[1,4,4,2048,431]>,
        %"add_77"<FLOAT,[1,4,2,441000]>
    ),
    initializers=(
        %"encoder.0.conv.weight"<FLOAT,[48,4,8,1]>,
        %"encoder.0.conv.bias"<FLOAT,[48]>,
        %"encoder.0.rewrite.weight"<FLOAT,[96,48,1,1]>,
        %"encoder.0.rewrite.bias"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.0.0.weight"<FLOAT,[6,48,3]>,
        %"encoder.0.dconv.layers.0.0.bias"<FLOAT,[6]>,
        %"encoder.0.dconv.layers.0.1.weight"<FLOAT,[6]>,
        %"encoder.0.dconv.layers.0.1.bias"<FLOAT,[6]>,
        %"encoder.0.dconv.layers.0.3.weight"<FLOAT,[96,6,1]>,
        %"encoder.0.dconv.layers.0.3.bias"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.0.4.weight"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.0.4.bias"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.0.6.scale"<FLOAT,[48]>,
        %"encoder.0.dconv.layers.1.0.weight"<FLOAT,[6,48,3]>,
        %"encoder.0.dconv.layers.1.0.bias"<FLOAT,[6]>,
        %"encoder.0.dconv.layers.1.1.weight"<FLOAT,[6]>,
        %"encoder.0.dconv.layers.1.1.bias"<FLOAT,[6]>,
        %"encoder.0.dconv.layers.1.3.weight"<FLOAT,[96,6,1]>,
        %"encoder.0.dconv.layers.1.3.bias"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.1.4.weight"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.1.4.bias"<FLOAT,[96]>,
        %"encoder.0.dconv.layers.1.6.scale"<FLOAT,[48]>,
        %"encoder.1.conv.weight"<FLOAT,[96,48,8,1]>,
        %"encoder.1.conv.bias"<FLOAT,[96]>,
        %"encoder.1.rewrite.weight"<FLOAT,[192,96,1,1]>,
        %"encoder.1.rewrite.bias"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.0.0.weight"<FLOAT,[12,96,3]>,
        %"encoder.1.dconv.layers.0.0.bias"<FLOAT,[12]>,
        %"encoder.1.dconv.layers.0.1.weight"<FLOAT,[12]>,
        %"encoder.1.dconv.layers.0.1.bias"<FLOAT,[12]>,
        %"encoder.1.dconv.layers.0.3.weight"<FLOAT,[192,12,1]>,
        %"encoder.1.dconv.layers.0.3.bias"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.0.4.weight"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.0.4.bias"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.0.6.scale"<FLOAT,[96]>,
        %"encoder.1.dconv.layers.1.0.weight"<FLOAT,[12,96,3]>,
        %"encoder.1.dconv.layers.1.0.bias"<FLOAT,[12]>,
        %"encoder.1.dconv.layers.1.1.weight"<FLOAT,[12]>,
        %"encoder.1.dconv.layers.1.1.bias"<FLOAT,[12]>,
        %"encoder.1.dconv.layers.1.3.weight"<FLOAT,[192,12,1]>,
        %"encoder.1.dconv.layers.1.3.bias"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.1.4.weight"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.1.4.bias"<FLOAT,[192]>,
        %"encoder.1.dconv.layers.1.6.scale"<FLOAT,[96]>,
        %"encoder.2.conv.weight"<FLOAT,[192,96,8,1]>,
        %"encoder.2.conv.bias"<FLOAT,[192]>,
        %"encoder.2.rewrite.weight"<FLOAT,[384,192,1,1]>,
        %"encoder.2.rewrite.bias"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.0.0.weight"<FLOAT,[24,192,3]>,
        %"encoder.2.dconv.layers.0.0.bias"<FLOAT,[24]>,
        %"encoder.2.dconv.layers.0.1.weight"<FLOAT,[24]>,
        %"encoder.2.dconv.layers.0.1.bias"<FLOAT,[24]>,
        %"encoder.2.dconv.layers.0.3.weight"<FLOAT,[384,24,1]>,
        %"encoder.2.dconv.layers.0.3.bias"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.0.4.weight"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.0.4.bias"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.0.6.scale"<FLOAT,[192]>,
        %"encoder.2.dconv.layers.1.0.weight"<FLOAT,[24,192,3]>,
        %"encoder.2.dconv.layers.1.0.bias"<FLOAT,[24]>,
        %"encoder.2.dconv.layers.1.1.weight"<FLOAT,[24]>,
        %"encoder.2.dconv.layers.1.1.bias"<FLOAT,[24]>,
        %"encoder.2.dconv.layers.1.3.weight"<FLOAT,[384,24,1]>,
        %"encoder.2.dconv.layers.1.3.bias"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.1.4.weight"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.1.4.bias"<FLOAT,[384]>,
        %"encoder.2.dconv.layers.1.6.scale"<FLOAT,[192]>,
        %"encoder.3.conv.weight"<FLOAT,[384,192,8,1]>,
        %"encoder.3.conv.bias"<FLOAT,[384]>,
        %"encoder.3.rewrite.weight"<FLOAT,[768,384,1,1]>,
        %"encoder.3.rewrite.bias"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.0.0.weight"<FLOAT,[48,384,3]>,
        %"encoder.3.dconv.layers.0.0.bias"<FLOAT,[48]>,
        %"encoder.3.dconv.layers.0.1.weight"<FLOAT,[48]>,
        %"encoder.3.dconv.layers.0.1.bias"<FLOAT,[48]>,
        %"encoder.3.dconv.layers.0.3.weight"<FLOAT,[768,48,1]>,
        %"encoder.3.dconv.layers.0.3.bias"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.0.4.weight"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.0.4.bias"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.0.6.scale"<FLOAT,[384]>,
        %"encoder.3.dconv.layers.1.0.weight"<FLOAT,[48,384,3]>,
        %"encoder.3.dconv.layers.1.0.bias"<FLOAT,[48]>,
        %"encoder.3.dconv.layers.1.1.weight"<FLOAT,[48]>,
        %"encoder.3.dconv.layers.1.1.bias"<FLOAT,[48]>,
        %"encoder.3.dconv.layers.1.3.weight"<FLOAT,[768,48,1]>,
        %"encoder.3.dconv.layers.1.3.bias"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.1.4.weight"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.1.4.bias"<FLOAT,[768]>,
        %"encoder.3.dconv.layers.1.6.scale"<FLOAT,[384]>,
        %"decoder.0.conv_tr.weight"<FLOAT,[384,192,8,1]>,
        %"decoder.0.conv_tr.bias"<FLOAT,[192]>,
        %"decoder.0.rewrite.weight"<FLOAT,[768,384,3,3]>,
        %"decoder.0.rewrite.bias"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.0.0.weight"<FLOAT,[48,384,3]>,
        %"decoder.0.dconv.layers.0.0.bias"<FLOAT,[48]>,
        %"decoder.0.dconv.layers.0.1.weight"<FLOAT,[48]>,
        %"decoder.0.dconv.layers.0.1.bias"<FLOAT,[48]>,
        %"decoder.0.dconv.layers.0.3.weight"<FLOAT,[768,48,1]>,
        %"decoder.0.dconv.layers.0.3.bias"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.0.4.weight"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.0.4.bias"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.0.6.scale"<FLOAT,[384]>,
        %"decoder.0.dconv.layers.1.0.weight"<FLOAT,[48,384,3]>,
        %"decoder.0.dconv.layers.1.0.bias"<FLOAT,[48]>,
        %"decoder.0.dconv.layers.1.1.weight"<FLOAT,[48]>,
        %"decoder.0.dconv.layers.1.1.bias"<FLOAT,[48]>,
        %"decoder.0.dconv.layers.1.3.weight"<FLOAT,[768,48,1]>,
        %"decoder.0.dconv.layers.1.3.bias"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.1.4.weight"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.1.4.bias"<FLOAT,[768]>,
        %"decoder.0.dconv.layers.1.6.scale"<FLOAT,[384]>,
        %"decoder.1.conv_tr.weight"<FLOAT,[192,96,8,1]>,
        %"decoder.1.conv_tr.bias"<FLOAT,[96]>,
        %"decoder.1.rewrite.weight"<FLOAT,[384,192,3,3]>,
        %"decoder.1.rewrite.bias"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.0.0.weight"<FLOAT,[24,192,3]>,
        %"decoder.1.dconv.layers.0.0.bias"<FLOAT,[24]>,
        %"decoder.1.dconv.layers.0.1.weight"<FLOAT,[24]>,
        %"decoder.1.dconv.layers.0.1.bias"<FLOAT,[24]>,
        %"decoder.1.dconv.layers.0.3.weight"<FLOAT,[384,24,1]>,
        %"decoder.1.dconv.layers.0.3.bias"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.0.4.weight"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.0.4.bias"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.0.6.scale"<FLOAT,[192]>,
        %"decoder.1.dconv.layers.1.0.weight"<FLOAT,[24,192,3]>,
        %"decoder.1.dconv.layers.1.0.bias"<FLOAT,[24]>,
        %"decoder.1.dconv.layers.1.1.weight"<FLOAT,[24]>,
        %"decoder.1.dconv.layers.1.1.bias"<FLOAT,[24]>,
        %"decoder.1.dconv.layers.1.3.weight"<FLOAT,[384,24,1]>,
        %"decoder.1.dconv.layers.1.3.bias"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.1.4.weight"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.1.4.bias"<FLOAT,[384]>,
        %"decoder.1.dconv.layers.1.6.scale"<FLOAT,[192]>,
        %"decoder.2.conv_tr.weight"<FLOAT,[96,48,8,1]>,
        %"decoder.2.conv_tr.bias"<FLOAT,[48]>,
        %"decoder.2.rewrite.weight"<FLOAT,[192,96,3,3]>,
        %"decoder.2.rewrite.bias"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.0.0.weight"<FLOAT,[12,96,3]>,
        %"decoder.2.dconv.layers.0.0.bias"<FLOAT,[12]>,
        %"decoder.2.dconv.layers.0.1.weight"<FLOAT,[12]>,
        %"decoder.2.dconv.layers.0.1.bias"<FLOAT,[12]>,
        %"decoder.2.dconv.layers.0.3.weight"<FLOAT,[192,12,1]>,
        %"decoder.2.dconv.layers.0.3.bias"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.0.4.weight"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.0.4.bias"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.0.6.scale"<FLOAT,[96]>,
        %"decoder.2.dconv.layers.1.0.weight"<FLOAT,[12,96,3]>,
        %"decoder.2.dconv.layers.1.0.bias"<FLOAT,[12]>,
        %"decoder.2.dconv.layers.1.1.weight"<FLOAT,[12]>,
        %"decoder.2.dconv.layers.1.1.bias"<FLOAT,[12]>,
        %"decoder.2.dconv.layers.1.3.weight"<FLOAT,[192,12,1]>,
        %"decoder.2.dconv.layers.1.3.bias"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.1.4.weight"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.1.4.bias"<FLOAT,[192]>,
        %"decoder.2.dconv.layers.1.6.scale"<FLOAT,[96]>,
        %"decoder.3.conv_tr.weight"<FLOAT,[48,16,8,1]>,
        %"decoder.3.conv_tr.bias"<FLOAT,[16]>,
        %"decoder.3.rewrite.weight"<FLOAT,[96,48,3,3]>,
        %"decoder.3.rewrite.bias"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.0.0.weight"<FLOAT,[6,48,3]>,
        %"decoder.3.dconv.layers.0.0.bias"<FLOAT,[6]>,
        %"decoder.3.dconv.layers.0.1.weight"<FLOAT,[6]>,
        %"decoder.3.dconv.layers.0.1.bias"<FLOAT,[6]>,
        %"decoder.3.dconv.layers.0.3.weight"<FLOAT,[96,6,1]>,
        %"decoder.3.dconv.layers.0.3.bias"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.0.4.weight"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.0.4.bias"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.0.6.scale"<FLOAT,[48]>,
        %"decoder.3.dconv.layers.1.0.weight"<FLOAT,[6,48,3]>,
        %"decoder.3.dconv.layers.1.0.bias"<FLOAT,[6]>,
        %"decoder.3.dconv.layers.1.1.weight"<FLOAT,[6]>,
        %"decoder.3.dconv.layers.1.1.bias"<FLOAT,[6]>,
        %"decoder.3.dconv.layers.1.3.weight"<FLOAT,[96,6,1]>,
        %"decoder.3.dconv.layers.1.3.bias"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.1.4.weight"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.1.4.bias"<FLOAT,[96]>,
        %"decoder.3.dconv.layers.1.6.scale"<FLOAT,[48]>,
        %"tencoder.0.conv.weight"<FLOAT,[48,2,8]>,
        %"tencoder.0.conv.bias"<FLOAT,[48]>,
        %"tencoder.0.rewrite.weight"<FLOAT,[96,48,1]>,
        %"tencoder.0.rewrite.bias"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.0.0.weight"<FLOAT,[6,48,3]>,
        %"tencoder.0.dconv.layers.0.0.bias"<FLOAT,[6]>,
        %"tencoder.0.dconv.layers.0.1.weight"<FLOAT,[6]>,
        %"tencoder.0.dconv.layers.0.1.bias"<FLOAT,[6]>,
        %"tencoder.0.dconv.layers.0.3.weight"<FLOAT,[96,6,1]>,
        %"tencoder.0.dconv.layers.0.3.bias"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.0.4.weight"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.0.4.bias"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.0.6.scale"<FLOAT,[48]>,
        %"tencoder.0.dconv.layers.1.0.weight"<FLOAT,[6,48,3]>,
        %"tencoder.0.dconv.layers.1.0.bias"<FLOAT,[6]>,
        %"tencoder.0.dconv.layers.1.1.weight"<FLOAT,[6]>,
        %"tencoder.0.dconv.layers.1.1.bias"<FLOAT,[6]>,
        %"tencoder.0.dconv.layers.1.3.weight"<FLOAT,[96,6,1]>,
        %"tencoder.0.dconv.layers.1.3.bias"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.1.4.weight"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.1.4.bias"<FLOAT,[96]>,
        %"tencoder.0.dconv.layers.1.6.scale"<FLOAT,[48]>,
        %"tencoder.1.conv.weight"<FLOAT,[96,48,8]>,
        %"tencoder.1.conv.bias"<FLOAT,[96]>,
        %"tencoder.1.rewrite.weight"<FLOAT,[192,96,1]>,
        %"tencoder.1.rewrite.bias"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.0.0.weight"<FLOAT,[12,96,3]>,
        %"tencoder.1.dconv.layers.0.0.bias"<FLOAT,[12]>,
        %"tencoder.1.dconv.layers.0.1.weight"<FLOAT,[12]>,
        %"tencoder.1.dconv.layers.0.1.bias"<FLOAT,[12]>,
        %"tencoder.1.dconv.layers.0.3.weight"<FLOAT,[192,12,1]>,
        %"tencoder.1.dconv.layers.0.3.bias"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.0.4.weight"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.0.4.bias"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.0.6.scale"<FLOAT,[96]>,
        %"tencoder.1.dconv.layers.1.0.weight"<FLOAT,[12,96,3]>,
        %"tencoder.1.dconv.layers.1.0.bias"<FLOAT,[12]>,
        %"tencoder.1.dconv.layers.1.1.weight"<FLOAT,[12]>,
        %"tencoder.1.dconv.layers.1.1.bias"<FLOAT,[12]>,
        %"tencoder.1.dconv.layers.1.3.weight"<FLOAT,[192,12,1]>,
        %"tencoder.1.dconv.layers.1.3.bias"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.1.4.weight"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.1.4.bias"<FLOAT,[192]>,
        %"tencoder.1.dconv.layers.1.6.scale"<FLOAT,[96]>,
        %"tencoder.2.conv.weight"<FLOAT,[192,96,8]>,
        %"tencoder.2.conv.bias"<FLOAT,[192]>,
        %"tencoder.2.rewrite.weight"<FLOAT,[384,192,1]>,
        %"tencoder.2.rewrite.bias"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.0.0.weight"<FLOAT,[24,192,3]>,
        %"tencoder.2.dconv.layers.0.0.bias"<FLOAT,[24]>,
        %"tencoder.2.dconv.layers.0.1.weight"<FLOAT,[24]>,
        %"tencoder.2.dconv.layers.0.1.bias"<FLOAT,[24]>,
        %"tencoder.2.dconv.layers.0.3.weight"<FLOAT,[384,24,1]>,
        %"tencoder.2.dconv.layers.0.3.bias"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.0.4.weight"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.0.4.bias"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.0.6.scale"<FLOAT,[192]>,
        %"tencoder.2.dconv.layers.1.0.weight"<FLOAT,[24,192,3]>,
        %"tencoder.2.dconv.layers.1.0.bias"<FLOAT,[24]>,
        %"tencoder.2.dconv.layers.1.1.weight"<FLOAT,[24]>,
        %"tencoder.2.dconv.layers.1.1.bias"<FLOAT,[24]>,
        %"tencoder.2.dconv.layers.1.3.weight"<FLOAT,[384,24,1]>,
        %"tencoder.2.dconv.layers.1.3.bias"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.1.4.weight"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.1.4.bias"<FLOAT,[384]>,
        %"tencoder.2.dconv.layers.1.6.scale"<FLOAT,[192]>,
        %"tencoder.3.conv.weight"<FLOAT,[384,192,8]>,
        %"tencoder.3.conv.bias"<FLOAT,[384]>,
        %"tencoder.3.rewrite.weight"<FLOAT,[768,384,1]>,
        %"tencoder.3.rewrite.bias"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.0.0.weight"<FLOAT,[48,384,3]>,
        %"tencoder.3.dconv.layers.0.0.bias"<FLOAT,[48]>,
        %"tencoder.3.dconv.layers.0.1.weight"<FLOAT,[48]>,
        %"tencoder.3.dconv.layers.0.1.bias"<FLOAT,[48]>,
        %"tencoder.3.dconv.layers.0.3.weight"<FLOAT,[768,48,1]>,
        %"tencoder.3.dconv.layers.0.3.bias"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.0.4.weight"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.0.4.bias"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.0.6.scale"<FLOAT,[384]>,
        %"tencoder.3.dconv.layers.1.0.weight"<FLOAT,[48,384,3]>,
        %"tencoder.3.dconv.layers.1.0.bias"<FLOAT,[48]>,
        %"tencoder.3.dconv.layers.1.1.weight"<FLOAT,[48]>,
        %"tencoder.3.dconv.layers.1.1.bias"<FLOAT,[48]>,
        %"tencoder.3.dconv.layers.1.3.weight"<FLOAT,[768,48,1]>,
        %"tencoder.3.dconv.layers.1.3.bias"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.1.4.weight"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.1.4.bias"<FLOAT,[768]>,
        %"tencoder.3.dconv.layers.1.6.scale"<FLOAT,[384]>,
        %"tdecoder.0.conv_tr.weight"<FLOAT,[384,192,8]>,
        %"tdecoder.0.conv_tr.bias"<FLOAT,[192]>,
        %"tdecoder.0.rewrite.weight"<FLOAT,[768,384,3]>,
        %"tdecoder.0.rewrite.bias"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.0.0.weight"<FLOAT,[48,384,3]>,
        %"tdecoder.0.dconv.layers.0.0.bias"<FLOAT,[48]>,
        %"tdecoder.0.dconv.layers.0.1.weight"<FLOAT,[48]>,
        %"tdecoder.0.dconv.layers.0.1.bias"<FLOAT,[48]>,
        %"tdecoder.0.dconv.layers.0.3.weight"<FLOAT,[768,48,1]>,
        %"tdecoder.0.dconv.layers.0.3.bias"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.0.4.weight"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.0.4.bias"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.0.6.scale"<FLOAT,[384]>,
        %"tdecoder.0.dconv.layers.1.0.weight"<FLOAT,[48,384,3]>,
        %"tdecoder.0.dconv.layers.1.0.bias"<FLOAT,[48]>,
        %"tdecoder.0.dconv.layers.1.1.weight"<FLOAT,[48]>,
        %"tdecoder.0.dconv.layers.1.1.bias"<FLOAT,[48]>,
        %"tdecoder.0.dconv.layers.1.3.weight"<FLOAT,[768,48,1]>,
        %"tdecoder.0.dconv.layers.1.3.bias"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.1.4.weight"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.1.4.bias"<FLOAT,[768]>,
        %"tdecoder.0.dconv.layers.1.6.scale"<FLOAT,[384]>,
        %"tdecoder.1.conv_tr.weight"<FLOAT,[192,96,8]>,
        %"tdecoder.1.conv_tr.bias"<FLOAT,[96]>,
        %"tdecoder.1.rewrite.weight"<FLOAT,[384,192,3]>,
        %"tdecoder.1.rewrite.bias"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.0.0.weight"<FLOAT,[24,192,3]>,
        %"tdecoder.1.dconv.layers.0.0.bias"<FLOAT,[24]>,
        %"tdecoder.1.dconv.layers.0.1.weight"<FLOAT,[24]>,
        %"tdecoder.1.dconv.layers.0.1.bias"<FLOAT,[24]>,
        %"tdecoder.1.dconv.layers.0.3.weight"<FLOAT,[384,24,1]>,
        %"tdecoder.1.dconv.layers.0.3.bias"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.0.4.weight"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.0.4.bias"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.0.6.scale"<FLOAT,[192]>,
        %"tdecoder.1.dconv.layers.1.0.weight"<FLOAT,[24,192,3]>,
        %"tdecoder.1.dconv.layers.1.0.bias"<FLOAT,[24]>,
        %"tdecoder.1.dconv.layers.1.1.weight"<FLOAT,[24]>,
        %"tdecoder.1.dconv.layers.1.1.bias"<FLOAT,[24]>,
        %"tdecoder.1.dconv.layers.1.3.weight"<FLOAT,[384,24,1]>,
        %"tdecoder.1.dconv.layers.1.3.bias"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.1.4.weight"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.1.4.bias"<FLOAT,[384]>,
        %"tdecoder.1.dconv.layers.1.6.scale"<FLOAT,[192]>,
        %"tdecoder.2.conv_tr.weight"<FLOAT,[96,48,8]>,
        %"tdecoder.2.conv_tr.bias"<FLOAT,[48]>,
        %"tdecoder.2.rewrite.weight"<FLOAT,[192,96,3]>,
        %"tdecoder.2.rewrite.bias"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.0.0.weight"<FLOAT,[12,96,3]>,
        %"tdecoder.2.dconv.layers.0.0.bias"<FLOAT,[12]>,
        %"tdecoder.2.dconv.layers.0.1.weight"<FLOAT,[12]>,
        %"tdecoder.2.dconv.layers.0.1.bias"<FLOAT,[12]>,
        %"tdecoder.2.dconv.layers.0.3.weight"<FLOAT,[192,12,1]>,
        %"tdecoder.2.dconv.layers.0.3.bias"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.0.4.weight"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.0.4.bias"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.0.6.scale"<FLOAT,[96]>,
        %"tdecoder.2.dconv.layers.1.0.weight"<FLOAT,[12,96,3]>,
        %"tdecoder.2.dconv.layers.1.0.bias"<FLOAT,[12]>,
        %"tdecoder.2.dconv.layers.1.1.weight"<FLOAT,[12]>,
        %"tdecoder.2.dconv.layers.1.1.bias"<FLOAT,[12]>,
        %"tdecoder.2.dconv.layers.1.3.weight"<FLOAT,[192,12,1]>,
        %"tdecoder.2.dconv.layers.1.3.bias"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.1.4.weight"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.1.4.bias"<FLOAT,[192]>,
        %"tdecoder.2.dconv.layers.1.6.scale"<FLOAT,[96]>,
        %"tdecoder.3.conv_tr.weight"<FLOAT,[48,8,8]>,
        %"tdecoder.3.conv_tr.bias"<FLOAT,[8]>,
        %"tdecoder.3.rewrite.weight"<FLOAT,[96,48,3]>,
        %"tdecoder.3.rewrite.bias"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.0.0.weight"<FLOAT,[6,48,3]>,
        %"tdecoder.3.dconv.layers.0.0.bias"<FLOAT,[6]>,
        %"tdecoder.3.dconv.layers.0.1.weight"<FLOAT,[6]>,
        %"tdecoder.3.dconv.layers.0.1.bias"<FLOAT,[6]>,
        %"tdecoder.3.dconv.layers.0.3.weight"<FLOAT,[96,6,1]>,
        %"tdecoder.3.dconv.layers.0.3.bias"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.0.4.weight"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.0.4.bias"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.0.6.scale"<FLOAT,[48]>,
        %"tdecoder.3.dconv.layers.1.0.weight"<FLOAT,[6,48,3]>,
        %"tdecoder.3.dconv.layers.1.0.bias"<FLOAT,[6]>,
        %"tdecoder.3.dconv.layers.1.1.weight"<FLOAT,[6]>,
        %"tdecoder.3.dconv.layers.1.1.bias"<FLOAT,[6]>,
        %"tdecoder.3.dconv.layers.1.3.weight"<FLOAT,[96,6,1]>,
        %"tdecoder.3.dconv.layers.1.3.bias"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.1.4.weight"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.1.4.bias"<FLOAT,[96]>,
        %"tdecoder.3.dconv.layers.1.6.scale"<FLOAT,[48]>,
        %"freq_emb.embedding.weight"<FLOAT,[512,48]>,
        %"channel_upsampler.weight"<FLOAT,[512,384,1]>,
        %"channel_upsampler.bias"<FLOAT,[512]>,
        %"channel_downsampler.weight"<FLOAT,[384,512,1]>,
        %"channel_downsampler.bias"<FLOAT,[384]>,
        %"channel_upsampler_t.weight"<FLOAT,[512,384,1]>,
        %"channel_upsampler_t.bias"<FLOAT,[512]>,
        %"channel_downsampler_t.weight"<FLOAT,[384,512,1]>,
        %"channel_downsampler_t.bias"<FLOAT,[384]>,
        %"crosstransformer.norm_in.weight"<FLOAT,[512]>,
        %"crosstransformer.norm_in.bias"<FLOAT,[512]>,
        %"crosstransformer.norm_in_t.weight"<FLOAT,[512]>,
        %"crosstransformer.norm_in_t.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.0.self_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers.0.self_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers.0.self_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers.0.self_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.0.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers.0.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers.0.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers.0.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.0.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.0.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.0.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.0.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.0.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.0.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.0.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.0.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.1.cross_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers.1.cross_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers.1.cross_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers.1.cross_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.1.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers.1.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers.1.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers.1.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm3.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm3.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.1.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.1.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.1.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.2.self_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers.2.self_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers.2.self_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers.2.self_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.2.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers.2.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers.2.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers.2.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.2.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.2.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.2.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.2.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.2.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.2.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.2.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.2.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.3.cross_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers.3.cross_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers.3.cross_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers.3.cross_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.3.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers.3.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers.3.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers.3.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm3.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm3.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.3.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.3.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.3.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.4.self_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers.4.self_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers.4.self_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers.4.self_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.4.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers.4.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers.4.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers.4.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.4.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.4.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.4.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.4.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.4.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers.4.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers.4.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers.4.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.self_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers_t.0.self_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers_t.0.self_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers_t.0.self_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers_t.0.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers_t.0.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers_t.0.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.0.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.cross_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers_t.1.cross_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers_t.1.cross_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers_t.1.cross_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers_t.1.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers_t.1.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers_t.1.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm3.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm3.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.1.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.self_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers_t.2.self_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers_t.2.self_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers_t.2.self_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers_t.2.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers_t.2.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers_t.2.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.2.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.cross_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers_t.3.cross_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers_t.3.cross_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers_t.3.cross_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers_t.3.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers_t.3.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers_t.3.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm3.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm3.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.3.gamma_2.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.self_attn.in_proj_weight"<FLOAT,[1536,512]>,
        %"crosstransformer.layers_t.4.self_attn.in_proj_bias"<FLOAT,[1536]>,
        %"crosstransformer.layers_t.4.self_attn.out_proj.weight"<FLOAT,[512,512]>,
        %"crosstransformer.layers_t.4.self_attn.out_proj.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.linear1.weight"<FLOAT,[2048,512]>,
        %"crosstransformer.layers_t.4.linear1.bias"<FLOAT,[2048]>,
        %"crosstransformer.layers_t.4.linear2.weight"<FLOAT,[512,2048]>,
        %"crosstransformer.layers_t.4.linear2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.norm1.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.norm1.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.norm2.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.norm2.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.norm_out.weight"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.norm_out.bias"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.gamma_1.scale"<FLOAT,[512]>,
        %"crosstransformer.layers_t.4.gamma_2.scale"<FLOAT,[512]>
    ),
) {
       0 |  # node_Identity_0
            %"view_as_real"<FLOAT,[1,2,2048,431,2]> ⬅️ ::Identity(%"spec")
       1 |  # node_Transpose_1
            %"permute"<FLOAT,[1,2,2,2048,431]> ⬅️ ::Transpose(%"view_as_real") {perm=[0, 1, 4, 2, 3]}
       2 |  # node_Identity_2
            %"clone"<FLOAT,[1,2,2,2048,431]> ⬅️ ::Identity(%"permute")
       3 |  # node_Constant_3
            %"val_0"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([   1,    4, 2048,  431]), name=None)}
       4 |  # node_Cast_4
            %"val_1"<?,?> ⬅️ ::Cast(%"val_0") {to=7}
       5 |  # node_Reshape_5
            %"_unsafe_view"<FLOAT,[1,4,2048,431]> ⬅️ ::Reshape(%"clone", %"val_1") {allowzero=0}
       6 |  # node_Constant_6
            %"val_2"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1, 2, 3]), name=None)}
       7 |  # node_ReduceMean_7
            %"mean"<FLOAT,[1,1,1,1]> ⬅️ ::ReduceMean(%"_unsafe_view", %"val_2") {keepdims=True, noop_with_empty_axes=0}
       8 |  # node_Constant_8
            %"val_3"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
       9 |  # node_Reshape_9
            %"val_4"<?,?> ⬅️ ::Reshape(%"val_2", %"val_3") {allowzero=0}
      10 |  # node_ReduceMean_10
            %"val_5"<?,?> ⬅️ ::ReduceMean(%"_unsafe_view", %"val_4") {keepdims=True, noop_with_empty_axes=0}
      11 |  # node_Sub_11
            %"val_6"<?,?> ⬅️ ::Sub(%"_unsafe_view", %"val_5")
      12 |  # node_Mul_12
            %"val_7"<?,?> ⬅️ ::Mul(%"val_6", %"val_6")
      13 |  # node_ReduceMean_13
            %"val_8"<?,?> ⬅️ ::ReduceMean(%"val_7", %"val_4") {keepdims=True, noop_with_empty_axes=0}
      14 |  # node_Shape_14
            %"val_9"<?,?> ⬅️ ::Shape(%"_unsafe_view") {start=0}
      15 |  # node_Gather_15
            %"val_10"<?,?> ⬅️ ::Gather(%"val_9", %"val_4") {axis=0}
      16 |  # node_ReduceProd_16
            %"val_11"<?,?> ⬅️ ::ReduceProd(%"val_10", None) {keepdims=False, noop_with_empty_axes=0}
      17 |  # node_CastLike_17
            %"val_12"<?,?> ⬅️ ::CastLike(%"val_11", %"_unsafe_view")
      18 |  # node_Mul_18
            %"val_13"<?,?> ⬅️ ::Mul(%"val_8", %"val_12")
      19 |  # node_Constant_19
            %"val_14"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1., dtype=float32), name=None)}
      20 |  # node_CastLike_20
            %"val_15"<?,?> ⬅️ ::CastLike(%"val_14", %"_unsafe_view")
      21 |  # node_Sub_21
            %"val_16"<?,?> ⬅️ ::Sub(%"val_12", %"val_15")
      22 |  # node_Div_22
            %"val_17"<?,?> ⬅️ ::Div(%"val_13", %"val_16")
      23 |  # node_Sqrt_23
            %"var"<FLOAT,[1,1,1,1]> ⬅️ ::Sqrt(%"val_17")
      24 |  # node_aten_sqrt_24
            %"sqrt"<FLOAT,[1,1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_sqrt(%"var")
      25 |  # node_aten_sub_25
            %"sub"<FLOAT,[1,4,2048,431]> ⬅️ pkg.onnxscript.torch_lib::aten_sub(%"_unsafe_view", %"mean") {alpha=1.0}
      26 |  # node_Constant_26
            %"val_18"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(1.e-05, dtype=float32), name=None)}
      27 |  # node_aten_add_27
            %"add_10"<FLOAT,[1,1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"sqrt", %"val_18") {alpha=1.0}
      28 |  # node_aten_div_28
            %"div"<FLOAT,[1,4,2048,431]> ⬅️ pkg.onnxscript.torch_lib::aten_div(%"sub", %"add_10")
      29 |  # node_Constant_29
            %"val_19"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([1, 2]), name=None)}
      30 |  # node_ReduceMean_30
            %"mean_1"<FLOAT,[1,1,1]> ⬅️ ::ReduceMean(%"mix", %"val_19") {keepdims=True, noop_with_empty_axes=0}
      31 |  # node_Constant_31
            %"val_20"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      32 |  # node_Reshape_32
            %"val_21"<?,?> ⬅️ ::Reshape(%"val_19", %"val_20") {allowzero=0}
      33 |  # node_ReduceMean_33
            %"val_22"<?,?> ⬅️ ::ReduceMean(%"mix", %"val_21") {keepdims=True, noop_with_empty_axes=0}
      34 |  # node_Sub_34
            %"val_23"<?,?> ⬅️ ::Sub(%"mix", %"val_22")
      35 |  # node_Mul_35
            %"val_24"<?,?> ⬅️ ::Mul(%"val_23", %"val_23")
      36 |  # node_ReduceMean_36
            %"val_25"<?,?> ⬅️ ::ReduceMean(%"val_24", %"val_21") {keepdims=True, noop_with_empty_axes=0}
      37 |  # node_Shape_37
            %"val_26"<?,?> ⬅️ ::Shape(%"mix") {start=0}
      38 |  # node_Gather_38
            %"val_27"<?,?> ⬅️ ::Gather(%"val_26", %"val_21") {axis=0}
      39 |  # node_ReduceProd_39
            %"val_28"<?,?> ⬅️ ::ReduceProd(%"val_27", None) {keepdims=False, noop_with_empty_axes=0}
      40 |  # node_CastLike_40
            %"val_29"<?,?> ⬅️ ::CastLike(%"val_28", %"mix")
      41 |  # node_Mul_41
            %"val_30"<?,?> ⬅️ ::Mul(%"val_25", %"val_29")
      42 |  # node_CastLike_42
            %"val_31"<?,?> ⬅️ ::CastLike(%"val_14", %"mix")
      43 |  # node_Sub_43
            %"val_32"<?,?> ⬅️ ::Sub(%"val_29", %"val_31")
      44 |  # node_Div_44
            %"val_33"<?,?> ⬅️ ::Div(%"val_30", %"val_32")
      45 |  # node_Sqrt_45
            %"var_1"<FLOAT,[1,1,1]> ⬅️ ::Sqrt(%"val_33")
      46 |  # node_aten_sqrt_46
            %"sqrt_1"<FLOAT,[1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_sqrt(%"var_1")
      47 |  # node_aten_sub_47
            %"sub_1"<FLOAT,[1,2,441000]> ⬅️ pkg.onnxscript.torch_lib::aten_sub(%"mix", %"mean_1") {alpha=1.0}
      48 |  # node_aten_add_48
            %"add_11"<FLOAT,[1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"sqrt_1", %"val_18") {alpha=1.0}
      49 |  # node_aten_div_49
            %"div_1"<FLOAT,[1,2,441000]> ⬅️ pkg.onnxscript.torch_lib::aten_div(%"sub_1", %"add_11")
      50 |  # node_Conv_50
            %"convolution"<FLOAT,[1,48,110250]> ⬅️ ::Conv(%"div_1", %"tencoder.0.conv.weight", %"tencoder.0.conv.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[2, 2], strides=[4]}
      51 |  # node__aten_gelu_approximate_none_51
            %"gelu"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution")
      52 |  # node_Conv_52
            %"convolution_1"<FLOAT,[1,6,110250]> ⬅️ ::Conv(%"gelu", %"tencoder.0.dconv.layers.0.0.weight", %"tencoder.0.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
      53 |  # node_Constant_53
            %"val_34"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      54 |  # node_Constant_54
            %"val_35"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(1), name=None)}
      55 |  # node_Reshape_55
            %"val_36"<?,?> ⬅️ ::Reshape(%"val_35", %"val_34") {allowzero=0}
      56 |  # node_Constant_56
            %"val_37"<?,?> ⬅️ ::Constant() {value_ints=[0]}
      57 |  # node_Concat_57
            %"val_38"<?,?> ⬅️ ::Concat(%"val_37", %"val_36", %"val_34") {axis=0}
      58 |  # node_Reshape_58
            %"val_39"<?,?> ⬅️ ::Reshape(%"convolution_1", %"val_38") {allowzero=0}
      59 |  # node_Constant_59
            %"val_40"<?,?> ⬅️ ::Constant() {value_float=1.0}
      60 |  # node_CastLike_60
            %"val_41"<?,?> ⬅️ ::CastLike(%"val_40", %"convolution_1")
      61 |  # node_Expand_61
            %"val_42"<?,?> ⬅️ ::Expand(%"val_41", %"val_36")
      62 |  # node_Constant_62
            %"val_43"<?,?> ⬅️ ::Constant() {value_float=0.0}
      63 |  # node_CastLike_63
            %"val_44"<?,?> ⬅️ ::CastLike(%"val_43", %"convolution_1")
      64 |  # node_Expand_64
            %"val_45"<?,?> ⬅️ ::Expand(%"val_44", %"val_36")
      65 |  # node_InstanceNormalization_65
            %"val_46"<?,?> ⬅️ ::InstanceNormalization(%"val_39", %"val_42", %"val_45") {epsilon=1e-05}
      66 |  # node_Shape_66
            %"val_47"<?,?> ⬅️ ::Shape(%"convolution_1") {start=0}
      67 |  # node_Reshape_67
            %"val_48"<?,?> ⬅️ ::Reshape(%"val_46", %"val_47") {allowzero=0}
      68 |  # node_Constant_68
            %"val_49"<?,?> ⬅️ ::Constant() {value_int=1}
      69 |  # node_Constant_69
            %"val_50"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(3), name=None)}
      70 |  # node_Sub_70
            %"val_51"<?,?> ⬅️ ::Sub(%"val_50", %"val_49")
      71 |  # node_Range_71
            %"val_52"<?,?> ⬅️ ::Range(%"val_49", %"val_51", %"val_49")
      72 |  # node_Unsqueeze_72
            %"val_53"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.0.1.weight", %"val_52")
      73 |  # node_Unsqueeze_73
            %"val_54"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.0.1.bias", %"val_52")
      74 |  # node_CastLike_74
            %"val_55"<?,?> ⬅️ ::CastLike(%"val_53", %"val_48")
      75 |  # node_Mul_75
            %"val_56"<?,?> ⬅️ ::Mul(%"val_48", %"val_55")
      76 |  # node_CastLike_76
            %"val_57"<?,?> ⬅️ ::CastLike(%"val_54", %"val_56")
      77 |  # node_Add_77
            %"group_norm"<FLOAT,[1,6,110250]> ⬅️ ::Add(%"val_56", %"val_57")
      78 |  # node__aten_gelu_approximate_none_78
            %"gelu_1"<FLOAT,[1,6,110250]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm")
      79 |  # node_Conv_79
            %"convolution_2"<FLOAT,[1,96,110250]> ⬅️ ::Conv(%"gelu_1", %"tencoder.0.dconv.layers.0.3.weight", %"tencoder.0.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
      80 |  # node_Constant_80
            %"val_58"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
      81 |  # node_Reshape_81
            %"val_59"<?,?> ⬅️ ::Reshape(%"val_35", %"val_58") {allowzero=0}
      82 |  # node_Constant_82
            %"val_60"<?,?> ⬅️ ::Constant() {value_ints=[0]}
      83 |  # node_Concat_83
            %"val_61"<?,?> ⬅️ ::Concat(%"val_60", %"val_59", %"val_58") {axis=0}
      84 |  # node_Reshape_84
            %"val_62"<?,?> ⬅️ ::Reshape(%"convolution_2", %"val_61") {allowzero=0}
      85 |  # node_Constant_85
            %"val_63"<?,?> ⬅️ ::Constant() {value_float=1.0}
      86 |  # node_CastLike_86
            %"val_64"<?,?> ⬅️ ::CastLike(%"val_63", %"convolution_2")
      87 |  # node_Expand_87
            %"val_65"<?,?> ⬅️ ::Expand(%"val_64", %"val_59")
      88 |  # node_Constant_88
            %"val_66"<?,?> ⬅️ ::Constant() {value_float=0.0}
      89 |  # node_CastLike_89
            %"val_67"<?,?> ⬅️ ::CastLike(%"val_66", %"convolution_2")
      90 |  # node_Expand_90
            %"val_68"<?,?> ⬅️ ::Expand(%"val_67", %"val_59")
      91 |  # node_InstanceNormalization_91
            %"val_69"<?,?> ⬅️ ::InstanceNormalization(%"val_62", %"val_65", %"val_68") {epsilon=1e-05}
      92 |  # node_Shape_92
            %"val_70"<?,?> ⬅️ ::Shape(%"convolution_2") {start=0}
      93 |  # node_Reshape_93
            %"val_71"<?,?> ⬅️ ::Reshape(%"val_69", %"val_70") {allowzero=0}
      94 |  # node_Constant_94
            %"val_72"<?,?> ⬅️ ::Constant() {value_int=1}
      95 |  # node_Sub_95
            %"val_73"<?,?> ⬅️ ::Sub(%"val_50", %"val_72")
      96 |  # node_Range_96
            %"val_74"<?,?> ⬅️ ::Range(%"val_72", %"val_73", %"val_72")
      97 |  # node_Unsqueeze_97
            %"val_75"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.0.4.weight", %"val_74")
      98 |  # node_Unsqueeze_98
            %"val_76"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.0.4.bias", %"val_74")
      99 |  # node_CastLike_99
            %"val_77"<?,?> ⬅️ ::CastLike(%"val_75", %"val_71")
     100 |  # node_Mul_100
            %"val_78"<?,?> ⬅️ ::Mul(%"val_71", %"val_77")
     101 |  # node_CastLike_101
            %"val_79"<?,?> ⬅️ ::CastLike(%"val_76", %"val_78")
     102 |  # node_Add_102
            %"group_norm_1"<FLOAT,[1,96,110250]> ⬅️ ::Add(%"val_78", %"val_79")
     103 |  # node_aten_glu_103
            %"glu"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_1") {dim=1}
     104 |  # node_Constant_104
            %"val_80"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(0), name=None)}
     105 |  # node_Cast_105
            %"val_81"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     106 |  # node_Constant_106
            %"val_82"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     107 |  # node_Reshape_107
            %"val_83"<?,?> ⬅️ ::Reshape(%"val_81", %"val_82") {allowzero=0}
     108 |  # node_Constant_108
            %"val_84"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(9223372036854775807), name=None)}
     109 |  # node_Cast_109
            %"val_85"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     110 |  # node_Constant_110
            %"val_86"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     111 |  # node_Reshape_111
            %"val_87"<?,?> ⬅️ ::Reshape(%"val_85", %"val_86") {allowzero=0}
     112 |  # node_Cast_112
            %"val_88"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     113 |  # node_Constant_113
            %"val_89"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     114 |  # node_Reshape_114
            %"val_90"<?,?> ⬅️ ::Reshape(%"val_88", %"val_89") {allowzero=0}
     115 |  # node_Constant_115
            %"val_91"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     116 |  # node_Slice_116
            %"slice_1"<FLOAT,[48]> ⬅️ ::Slice(%"tencoder.0.dconv.layers.0.6.scale", %"val_83", %"val_87", %"val_90", %"val_91")
     117 |  # node_aten_unsqueeze_117
            %"unsqueeze"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_1") {dim=1}
     118 |  # node_Mul_118
            %"mul_6"<FLOAT,[1,48,110250]> ⬅️ ::Mul(%"unsqueeze", %"glu")
     119 |  # node_aten_add_119
            %"add_12"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu", %"mul_6") {alpha=1.0}
     120 |  # node_Conv_120
            %"convolution_3"<FLOAT,[1,6,110250]> ⬅️ ::Conv(%"add_12", %"tencoder.0.dconv.layers.1.0.weight", %"tencoder.0.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     121 |  # node_Constant_121
            %"val_92"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     122 |  # node_Reshape_122
            %"val_93"<?,?> ⬅️ ::Reshape(%"val_35", %"val_92") {allowzero=0}
     123 |  # node_Constant_123
            %"val_94"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     124 |  # node_Concat_124
            %"val_95"<?,?> ⬅️ ::Concat(%"val_94", %"val_93", %"val_92") {axis=0}
     125 |  # node_Reshape_125
            %"val_96"<?,?> ⬅️ ::Reshape(%"convolution_3", %"val_95") {allowzero=0}
     126 |  # node_Constant_126
            %"val_97"<?,?> ⬅️ ::Constant() {value_float=1.0}
     127 |  # node_CastLike_127
            %"val_98"<?,?> ⬅️ ::CastLike(%"val_97", %"convolution_3")
     128 |  # node_Expand_128
            %"val_99"<?,?> ⬅️ ::Expand(%"val_98", %"val_93")
     129 |  # node_Constant_129
            %"val_100"<?,?> ⬅️ ::Constant() {value_float=0.0}
     130 |  # node_CastLike_130
            %"val_101"<?,?> ⬅️ ::CastLike(%"val_100", %"convolution_3")
     131 |  # node_Expand_131
            %"val_102"<?,?> ⬅️ ::Expand(%"val_101", %"val_93")
     132 |  # node_InstanceNormalization_132
            %"val_103"<?,?> ⬅️ ::InstanceNormalization(%"val_96", %"val_99", %"val_102") {epsilon=1e-05}
     133 |  # node_Shape_133
            %"val_104"<?,?> ⬅️ ::Shape(%"convolution_3") {start=0}
     134 |  # node_Reshape_134
            %"val_105"<?,?> ⬅️ ::Reshape(%"val_103", %"val_104") {allowzero=0}
     135 |  # node_Constant_135
            %"val_106"<?,?> ⬅️ ::Constant() {value_int=1}
     136 |  # node_Sub_136
            %"val_107"<?,?> ⬅️ ::Sub(%"val_50", %"val_106")
     137 |  # node_Range_137
            %"val_108"<?,?> ⬅️ ::Range(%"val_106", %"val_107", %"val_106")
     138 |  # node_Unsqueeze_138
            %"val_109"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.1.1.weight", %"val_108")
     139 |  # node_Unsqueeze_139
            %"val_110"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.1.1.bias", %"val_108")
     140 |  # node_CastLike_140
            %"val_111"<?,?> ⬅️ ::CastLike(%"val_109", %"val_105")
     141 |  # node_Mul_141
            %"val_112"<?,?> ⬅️ ::Mul(%"val_105", %"val_111")
     142 |  # node_CastLike_142
            %"val_113"<?,?> ⬅️ ::CastLike(%"val_110", %"val_112")
     143 |  # node_Add_143
            %"group_norm_2"<FLOAT,[1,6,110250]> ⬅️ ::Add(%"val_112", %"val_113")
     144 |  # node__aten_gelu_approximate_none_144
            %"gelu_2"<FLOAT,[1,6,110250]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_2")
     145 |  # node_Conv_145
            %"convolution_4"<FLOAT,[1,96,110250]> ⬅️ ::Conv(%"gelu_2", %"tencoder.0.dconv.layers.1.3.weight", %"tencoder.0.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     146 |  # node_Constant_146
            %"val_114"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     147 |  # node_Reshape_147
            %"val_115"<?,?> ⬅️ ::Reshape(%"val_35", %"val_114") {allowzero=0}
     148 |  # node_Constant_148
            %"val_116"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     149 |  # node_Concat_149
            %"val_117"<?,?> ⬅️ ::Concat(%"val_116", %"val_115", %"val_114") {axis=0}
     150 |  # node_Reshape_150
            %"val_118"<?,?> ⬅️ ::Reshape(%"convolution_4", %"val_117") {allowzero=0}
     151 |  # node_Constant_151
            %"val_119"<?,?> ⬅️ ::Constant() {value_float=1.0}
     152 |  # node_CastLike_152
            %"val_120"<?,?> ⬅️ ::CastLike(%"val_119", %"convolution_4")
     153 |  # node_Expand_153
            %"val_121"<?,?> ⬅️ ::Expand(%"val_120", %"val_115")
     154 |  # node_Constant_154
            %"val_122"<?,?> ⬅️ ::Constant() {value_float=0.0}
     155 |  # node_CastLike_155
            %"val_123"<?,?> ⬅️ ::CastLike(%"val_122", %"convolution_4")
     156 |  # node_Expand_156
            %"val_124"<?,?> ⬅️ ::Expand(%"val_123", %"val_115")
     157 |  # node_InstanceNormalization_157
            %"val_125"<?,?> ⬅️ ::InstanceNormalization(%"val_118", %"val_121", %"val_124") {epsilon=1e-05}
     158 |  # node_Shape_158
            %"val_126"<?,?> ⬅️ ::Shape(%"convolution_4") {start=0}
     159 |  # node_Reshape_159
            %"val_127"<?,?> ⬅️ ::Reshape(%"val_125", %"val_126") {allowzero=0}
     160 |  # node_Constant_160
            %"val_128"<?,?> ⬅️ ::Constant() {value_int=1}
     161 |  # node_Sub_161
            %"val_129"<?,?> ⬅️ ::Sub(%"val_50", %"val_128")
     162 |  # node_Range_162
            %"val_130"<?,?> ⬅️ ::Range(%"val_128", %"val_129", %"val_128")
     163 |  # node_Unsqueeze_163
            %"val_131"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.1.4.weight", %"val_130")
     164 |  # node_Unsqueeze_164
            %"val_132"<?,?> ⬅️ ::Unsqueeze(%"tencoder.0.dconv.layers.1.4.bias", %"val_130")
     165 |  # node_CastLike_165
            %"val_133"<?,?> ⬅️ ::CastLike(%"val_131", %"val_127")
     166 |  # node_Mul_166
            %"val_134"<?,?> ⬅️ ::Mul(%"val_127", %"val_133")
     167 |  # node_CastLike_167
            %"val_135"<?,?> ⬅️ ::CastLike(%"val_132", %"val_134")
     168 |  # node_Add_168
            %"group_norm_3"<FLOAT,[1,96,110250]> ⬅️ ::Add(%"val_134", %"val_135")
     169 |  # node_aten_glu_169
            %"glu_1"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_3") {dim=1}
     170 |  # node_Cast_170
            %"val_136"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     171 |  # node_Constant_171
            %"val_137"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     172 |  # node_Reshape_172
            %"val_138"<?,?> ⬅️ ::Reshape(%"val_136", %"val_137") {allowzero=0}
     173 |  # node_Cast_173
            %"val_139"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     174 |  # node_Constant_174
            %"val_140"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     175 |  # node_Reshape_175
            %"val_141"<?,?> ⬅️ ::Reshape(%"val_139", %"val_140") {allowzero=0}
     176 |  # node_Cast_176
            %"val_142"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     177 |  # node_Constant_177
            %"val_143"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     178 |  # node_Reshape_178
            %"val_144"<?,?> ⬅️ ::Reshape(%"val_142", %"val_143") {allowzero=0}
     179 |  # node_Constant_179
            %"val_145"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     180 |  # node_Slice_180
            %"slice_2"<FLOAT,[48]> ⬅️ ::Slice(%"tencoder.0.dconv.layers.1.6.scale", %"val_138", %"val_141", %"val_144", %"val_145")
     181 |  # node_aten_unsqueeze_181
            %"unsqueeze_1"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_2") {dim=1}
     182 |  # node_Mul_182
            %"mul_7"<FLOAT,[1,48,110250]> ⬅️ ::Mul(%"unsqueeze_1", %"glu_1")
     183 |  # node_aten_add_183
            %"add_13"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_12", %"mul_7") {alpha=1.0}
     184 |  # node_Conv_184
            %"convolution_5"<FLOAT,[1,96,110250]> ⬅️ ::Conv(%"add_13", %"tencoder.0.rewrite.weight", %"tencoder.0.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     185 |  # node_aten_glu_185
            %"glu_2"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_5") {dim=1}
     186 |  # node_Conv_186
            %"convolution_6"<FLOAT,[1,48,512,431]> ⬅️ ::Conv(%"div", %"encoder.0.conv.weight", %"encoder.0.conv.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[2, 0, 2, 0], strides=[4, 1]}
     187 |  # node__aten_gelu_approximate_none_187
            %"gelu_3"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_6")
     188 |  # node_Transpose_188
            %"permute_1"<FLOAT,[1,512,48,431]> ⬅️ ::Transpose(%"gelu_3") {perm=[0, 2, 1, 3]}
     189 |  # node_Constant_189
            %"val_146"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([512,  48, 431]), name=None)}
     190 |  # node_Cast_190
            %"val_147"<?,?> ⬅️ ::Cast(%"val_146") {to=7}
     191 |  # node_Reshape_191
            %"view"<FLOAT,[512,48,431]> ⬅️ ::Reshape(%"permute_1", %"val_147") {allowzero=0}
     192 |  # node_Conv_192
            %"convolution_7"<FLOAT,[512,6,431]> ⬅️ ::Conv(%"view", %"encoder.0.dconv.layers.0.0.weight", %"encoder.0.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
     193 |  # node_Constant_193
            %"val_148"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     194 |  # node_Reshape_194
            %"val_149"<?,?> ⬅️ ::Reshape(%"val_35", %"val_148") {allowzero=0}
     195 |  # node_Constant_195
            %"val_150"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     196 |  # node_Concat_196
            %"val_151"<?,?> ⬅️ ::Concat(%"val_150", %"val_149", %"val_148") {axis=0}
     197 |  # node_Reshape_197
            %"val_152"<?,?> ⬅️ ::Reshape(%"convolution_7", %"val_151") {allowzero=0}
     198 |  # node_Constant_198
            %"val_153"<?,?> ⬅️ ::Constant() {value_float=1.0}
     199 |  # node_CastLike_199
            %"val_154"<?,?> ⬅️ ::CastLike(%"val_153", %"convolution_7")
     200 |  # node_Expand_200
            %"val_155"<?,?> ⬅️ ::Expand(%"val_154", %"val_149")
     201 |  # node_Constant_201
            %"val_156"<?,?> ⬅️ ::Constant() {value_float=0.0}
     202 |  # node_CastLike_202
            %"val_157"<?,?> ⬅️ ::CastLike(%"val_156", %"convolution_7")
     203 |  # node_Expand_203
            %"val_158"<?,?> ⬅️ ::Expand(%"val_157", %"val_149")
     204 |  # node_InstanceNormalization_204
            %"val_159"<?,?> ⬅️ ::InstanceNormalization(%"val_152", %"val_155", %"val_158") {epsilon=1e-05}
     205 |  # node_Shape_205
            %"val_160"<?,?> ⬅️ ::Shape(%"convolution_7") {start=0}
     206 |  # node_Reshape_206
            %"val_161"<?,?> ⬅️ ::Reshape(%"val_159", %"val_160") {allowzero=0}
     207 |  # node_Constant_207
            %"val_162"<?,?> ⬅️ ::Constant() {value_int=1}
     208 |  # node_Sub_208
            %"val_163"<?,?> ⬅️ ::Sub(%"val_50", %"val_162")
     209 |  # node_Range_209
            %"val_164"<?,?> ⬅️ ::Range(%"val_162", %"val_163", %"val_162")
     210 |  # node_Unsqueeze_210
            %"val_165"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.0.1.weight", %"val_164")
     211 |  # node_Unsqueeze_211
            %"val_166"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.0.1.bias", %"val_164")
     212 |  # node_CastLike_212
            %"val_167"<?,?> ⬅️ ::CastLike(%"val_165", %"val_161")
     213 |  # node_Mul_213
            %"val_168"<?,?> ⬅️ ::Mul(%"val_161", %"val_167")
     214 |  # node_CastLike_214
            %"val_169"<?,?> ⬅️ ::CastLike(%"val_166", %"val_168")
     215 |  # node_Add_215
            %"group_norm_4"<FLOAT,[512,6,431]> ⬅️ ::Add(%"val_168", %"val_169")
     216 |  # node__aten_gelu_approximate_none_216
            %"gelu_4"<FLOAT,[512,6,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_4")
     217 |  # node_Conv_217
            %"convolution_8"<FLOAT,[512,96,431]> ⬅️ ::Conv(%"gelu_4", %"encoder.0.dconv.layers.0.3.weight", %"encoder.0.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     218 |  # node_Constant_218
            %"val_170"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     219 |  # node_Reshape_219
            %"val_171"<?,?> ⬅️ ::Reshape(%"val_35", %"val_170") {allowzero=0}
     220 |  # node_Constant_220
            %"val_172"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     221 |  # node_Concat_221
            %"val_173"<?,?> ⬅️ ::Concat(%"val_172", %"val_171", %"val_170") {axis=0}
     222 |  # node_Reshape_222
            %"val_174"<?,?> ⬅️ ::Reshape(%"convolution_8", %"val_173") {allowzero=0}
     223 |  # node_Constant_223
            %"val_175"<?,?> ⬅️ ::Constant() {value_float=1.0}
     224 |  # node_CastLike_224
            %"val_176"<?,?> ⬅️ ::CastLike(%"val_175", %"convolution_8")
     225 |  # node_Expand_225
            %"val_177"<?,?> ⬅️ ::Expand(%"val_176", %"val_171")
     226 |  # node_Constant_226
            %"val_178"<?,?> ⬅️ ::Constant() {value_float=0.0}
     227 |  # node_CastLike_227
            %"val_179"<?,?> ⬅️ ::CastLike(%"val_178", %"convolution_8")
     228 |  # node_Expand_228
            %"val_180"<?,?> ⬅️ ::Expand(%"val_179", %"val_171")
     229 |  # node_InstanceNormalization_229
            %"val_181"<?,?> ⬅️ ::InstanceNormalization(%"val_174", %"val_177", %"val_180") {epsilon=1e-05}
     230 |  # node_Shape_230
            %"val_182"<?,?> ⬅️ ::Shape(%"convolution_8") {start=0}
     231 |  # node_Reshape_231
            %"val_183"<?,?> ⬅️ ::Reshape(%"val_181", %"val_182") {allowzero=0}
     232 |  # node_Constant_232
            %"val_184"<?,?> ⬅️ ::Constant() {value_int=1}
     233 |  # node_Sub_233
            %"val_185"<?,?> ⬅️ ::Sub(%"val_50", %"val_184")
     234 |  # node_Range_234
            %"val_186"<?,?> ⬅️ ::Range(%"val_184", %"val_185", %"val_184")
     235 |  # node_Unsqueeze_235
            %"val_187"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.0.4.weight", %"val_186")
     236 |  # node_Unsqueeze_236
            %"val_188"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.0.4.bias", %"val_186")
     237 |  # node_CastLike_237
            %"val_189"<?,?> ⬅️ ::CastLike(%"val_187", %"val_183")
     238 |  # node_Mul_238
            %"val_190"<?,?> ⬅️ ::Mul(%"val_183", %"val_189")
     239 |  # node_CastLike_239
            %"val_191"<?,?> ⬅️ ::CastLike(%"val_188", %"val_190")
     240 |  # node_Add_240
            %"group_norm_5"<FLOAT,[512,96,431]> ⬅️ ::Add(%"val_190", %"val_191")
     241 |  # node_aten_glu_241
            %"glu_3"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_5") {dim=1}
     242 |  # node_Cast_242
            %"val_192"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     243 |  # node_Constant_243
            %"val_193"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     244 |  # node_Reshape_244
            %"val_194"<?,?> ⬅️ ::Reshape(%"val_192", %"val_193") {allowzero=0}
     245 |  # node_Cast_245
            %"val_195"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     246 |  # node_Constant_246
            %"val_196"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     247 |  # node_Reshape_247
            %"val_197"<?,?> ⬅️ ::Reshape(%"val_195", %"val_196") {allowzero=0}
     248 |  # node_Cast_248
            %"val_198"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     249 |  # node_Constant_249
            %"val_199"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     250 |  # node_Reshape_250
            %"val_200"<?,?> ⬅️ ::Reshape(%"val_198", %"val_199") {allowzero=0}
     251 |  # node_Constant_251
            %"val_201"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     252 |  # node_Slice_252
            %"slice_3"<FLOAT,[48]> ⬅️ ::Slice(%"encoder.0.dconv.layers.0.6.scale", %"val_194", %"val_197", %"val_200", %"val_201")
     253 |  # node_aten_unsqueeze_253
            %"unsqueeze_2"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_3") {dim=1}
     254 |  # node_Mul_254
            %"mul_8"<FLOAT,[512,48,431]> ⬅️ ::Mul(%"unsqueeze_2", %"glu_3")
     255 |  # node_aten_add_255
            %"add_14"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view", %"mul_8") {alpha=1.0}
     256 |  # node_Conv_256
            %"convolution_9"<FLOAT,[512,6,431]> ⬅️ ::Conv(%"add_14", %"encoder.0.dconv.layers.1.0.weight", %"encoder.0.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     257 |  # node_Constant_257
            %"val_202"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     258 |  # node_Reshape_258
            %"val_203"<?,?> ⬅️ ::Reshape(%"val_35", %"val_202") {allowzero=0}
     259 |  # node_Constant_259
            %"val_204"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     260 |  # node_Concat_260
            %"val_205"<?,?> ⬅️ ::Concat(%"val_204", %"val_203", %"val_202") {axis=0}
     261 |  # node_Reshape_261
            %"val_206"<?,?> ⬅️ ::Reshape(%"convolution_9", %"val_205") {allowzero=0}
     262 |  # node_Constant_262
            %"val_207"<?,?> ⬅️ ::Constant() {value_float=1.0}
     263 |  # node_CastLike_263
            %"val_208"<?,?> ⬅️ ::CastLike(%"val_207", %"convolution_9")
     264 |  # node_Expand_264
            %"val_209"<?,?> ⬅️ ::Expand(%"val_208", %"val_203")
     265 |  # node_Constant_265
            %"val_210"<?,?> ⬅️ ::Constant() {value_float=0.0}
     266 |  # node_CastLike_266
            %"val_211"<?,?> ⬅️ ::CastLike(%"val_210", %"convolution_9")
     267 |  # node_Expand_267
            %"val_212"<?,?> ⬅️ ::Expand(%"val_211", %"val_203")
     268 |  # node_InstanceNormalization_268
            %"val_213"<?,?> ⬅️ ::InstanceNormalization(%"val_206", %"val_209", %"val_212") {epsilon=1e-05}
     269 |  # node_Shape_269
            %"val_214"<?,?> ⬅️ ::Shape(%"convolution_9") {start=0}
     270 |  # node_Reshape_270
            %"val_215"<?,?> ⬅️ ::Reshape(%"val_213", %"val_214") {allowzero=0}
     271 |  # node_Constant_271
            %"val_216"<?,?> ⬅️ ::Constant() {value_int=1}
     272 |  # node_Sub_272
            %"val_217"<?,?> ⬅️ ::Sub(%"val_50", %"val_216")
     273 |  # node_Range_273
            %"val_218"<?,?> ⬅️ ::Range(%"val_216", %"val_217", %"val_216")
     274 |  # node_Unsqueeze_274
            %"val_219"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.1.1.weight", %"val_218")
     275 |  # node_Unsqueeze_275
            %"val_220"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.1.1.bias", %"val_218")
     276 |  # node_CastLike_276
            %"val_221"<?,?> ⬅️ ::CastLike(%"val_219", %"val_215")
     277 |  # node_Mul_277
            %"val_222"<?,?> ⬅️ ::Mul(%"val_215", %"val_221")
     278 |  # node_CastLike_278
            %"val_223"<?,?> ⬅️ ::CastLike(%"val_220", %"val_222")
     279 |  # node_Add_279
            %"group_norm_6"<FLOAT,[512,6,431]> ⬅️ ::Add(%"val_222", %"val_223")
     280 |  # node__aten_gelu_approximate_none_280
            %"gelu_5"<FLOAT,[512,6,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_6")
     281 |  # node_Conv_281
            %"convolution_10"<FLOAT,[512,96,431]> ⬅️ ::Conv(%"gelu_5", %"encoder.0.dconv.layers.1.3.weight", %"encoder.0.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     282 |  # node_Constant_282
            %"val_224"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     283 |  # node_Reshape_283
            %"val_225"<?,?> ⬅️ ::Reshape(%"val_35", %"val_224") {allowzero=0}
     284 |  # node_Constant_284
            %"val_226"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     285 |  # node_Concat_285
            %"val_227"<?,?> ⬅️ ::Concat(%"val_226", %"val_225", %"val_224") {axis=0}
     286 |  # node_Reshape_286
            %"val_228"<?,?> ⬅️ ::Reshape(%"convolution_10", %"val_227") {allowzero=0}
     287 |  # node_Constant_287
            %"val_229"<?,?> ⬅️ ::Constant() {value_float=1.0}
     288 |  # node_CastLike_288
            %"val_230"<?,?> ⬅️ ::CastLike(%"val_229", %"convolution_10")
     289 |  # node_Expand_289
            %"val_231"<?,?> ⬅️ ::Expand(%"val_230", %"val_225")
     290 |  # node_Constant_290
            %"val_232"<?,?> ⬅️ ::Constant() {value_float=0.0}
     291 |  # node_CastLike_291
            %"val_233"<?,?> ⬅️ ::CastLike(%"val_232", %"convolution_10")
     292 |  # node_Expand_292
            %"val_234"<?,?> ⬅️ ::Expand(%"val_233", %"val_225")
     293 |  # node_InstanceNormalization_293
            %"val_235"<?,?> ⬅️ ::InstanceNormalization(%"val_228", %"val_231", %"val_234") {epsilon=1e-05}
     294 |  # node_Shape_294
            %"val_236"<?,?> ⬅️ ::Shape(%"convolution_10") {start=0}
     295 |  # node_Reshape_295
            %"val_237"<?,?> ⬅️ ::Reshape(%"val_235", %"val_236") {allowzero=0}
     296 |  # node_Constant_296
            %"val_238"<?,?> ⬅️ ::Constant() {value_int=1}
     297 |  # node_Sub_297
            %"val_239"<?,?> ⬅️ ::Sub(%"val_50", %"val_238")
     298 |  # node_Range_298
            %"val_240"<?,?> ⬅️ ::Range(%"val_238", %"val_239", %"val_238")
     299 |  # node_Unsqueeze_299
            %"val_241"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.1.4.weight", %"val_240")
     300 |  # node_Unsqueeze_300
            %"val_242"<?,?> ⬅️ ::Unsqueeze(%"encoder.0.dconv.layers.1.4.bias", %"val_240")
     301 |  # node_CastLike_301
            %"val_243"<?,?> ⬅️ ::CastLike(%"val_241", %"val_237")
     302 |  # node_Mul_302
            %"val_244"<?,?> ⬅️ ::Mul(%"val_237", %"val_243")
     303 |  # node_CastLike_303
            %"val_245"<?,?> ⬅️ ::CastLike(%"val_242", %"val_244")
     304 |  # node_Add_304
            %"group_norm_7"<FLOAT,[512,96,431]> ⬅️ ::Add(%"val_244", %"val_245")
     305 |  # node_aten_glu_305
            %"glu_4"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_7") {dim=1}
     306 |  # node_Cast_306
            %"val_246"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     307 |  # node_Constant_307
            %"val_247"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     308 |  # node_Reshape_308
            %"val_248"<?,?> ⬅️ ::Reshape(%"val_246", %"val_247") {allowzero=0}
     309 |  # node_Cast_309
            %"val_249"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     310 |  # node_Constant_310
            %"val_250"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     311 |  # node_Reshape_311
            %"val_251"<?,?> ⬅️ ::Reshape(%"val_249", %"val_250") {allowzero=0}
     312 |  # node_Cast_312
            %"val_252"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     313 |  # node_Constant_313
            %"val_253"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     314 |  # node_Reshape_314
            %"val_254"<?,?> ⬅️ ::Reshape(%"val_252", %"val_253") {allowzero=0}
     315 |  # node_Constant_315
            %"val_255"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     316 |  # node_Slice_316
            %"slice_4"<FLOAT,[48]> ⬅️ ::Slice(%"encoder.0.dconv.layers.1.6.scale", %"val_248", %"val_251", %"val_254", %"val_255")
     317 |  # node_aten_unsqueeze_317
            %"unsqueeze_3"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_4") {dim=1}
     318 |  # node_Mul_318
            %"mul_9"<FLOAT,[512,48,431]> ⬅️ ::Mul(%"unsqueeze_3", %"glu_4")
     319 |  # node_aten_add_319
            %"add_15"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_14", %"mul_9") {alpha=1.0}
     320 |  # node_Constant_320
            %"val_256"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 512,  48, 431]), name=None)}
     321 |  # node_Cast_321
            %"val_257"<?,?> ⬅️ ::Cast(%"val_256") {to=7}
     322 |  # node_Reshape_322
            %"view_1"<FLOAT,[1,512,48,431]> ⬅️ ::Reshape(%"add_15", %"val_257") {allowzero=0}
     323 |  # node_Transpose_323
            %"permute_2"<FLOAT,[1,48,512,431]> ⬅️ ::Transpose(%"view_1") {perm=[0, 2, 1, 3]}
     324 |  # node_Conv_324
            %"convolution_11"<FLOAT,[1,96,512,431]> ⬅️ ::Conv(%"permute_2", %"encoder.0.rewrite.weight", %"encoder.0.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[0, 0, 0, 0], strides=[1, 1]}
     325 |  # node_aten_glu_325
            %"glu_5"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_11") {dim=1}
     326 |  # node_Constant_326
            %"val_258"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(512), name=None)}
     327 |  # node_Range_327
            %"arange"<INT64,[512]> ⬅️ ::Range(%"val_80", %"val_258", %"val_35")
     328 |  # node_Gather_328
            %"embedding"<FLOAT,[512,48]> ⬅️ ::Gather(%"freq_emb.embedding.weight", %"arange") {axis=0}
     329 |  # node_Constant_329
            %"val_259"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(10), name=None)}
     330 |  # node_Cast_330
            %"scalar_tensor_default"<FLOAT,[]> ⬅️ ::Cast(%"val_259") {to=FLOAT}
     331 |  # node_Mul_331
            %"mul_10"<FLOAT,[512,48]> ⬅️ ::Mul(%"embedding", %"scalar_tensor_default")
     332 |  # node_Transpose_332
            %"t"<FLOAT,[48,512]> ⬅️ ::Transpose(%"mul_10") {perm=[1, 0]}
     333 |  # node_aten_unsqueeze_333
            %"unsqueeze_4"<FLOAT,[1,48,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"t") {dim=0}
     334 |  # node_Cast_334
            %"val_260"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     335 |  # node_Constant_335
            %"val_261"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     336 |  # node_Reshape_336
            %"val_262"<?,?> ⬅️ ::Reshape(%"val_260", %"val_261") {allowzero=0}
     337 |  # node_Cast_337
            %"val_263"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     338 |  # node_Constant_338
            %"val_264"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     339 |  # node_Reshape_339
            %"val_265"<?,?> ⬅️ ::Reshape(%"val_263", %"val_264") {allowzero=0}
     340 |  # node_Cast_340
            %"val_266"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
     341 |  # node_Constant_341
            %"val_267"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     342 |  # node_Reshape_342
            %"val_268"<?,?> ⬅️ ::Reshape(%"val_266", %"val_267") {allowzero=0}
     343 |  # node_Constant_343
            %"val_269"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     344 |  # node_Slice_344
            %"slice_5"<FLOAT,[1,48,512]> ⬅️ ::Slice(%"unsqueeze_4", %"val_262", %"val_265", %"val_268", %"val_269")
     345 |  # node_Cast_345
            %"val_270"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     346 |  # node_Constant_346
            %"val_271"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     347 |  # node_Reshape_347
            %"val_272"<?,?> ⬅️ ::Reshape(%"val_270", %"val_271") {allowzero=0}
     348 |  # node_Cast_348
            %"val_273"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     349 |  # node_Constant_349
            %"val_274"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     350 |  # node_Reshape_350
            %"val_275"<?,?> ⬅️ ::Reshape(%"val_273", %"val_274") {allowzero=0}
     351 |  # node_Constant_351
            %"val_276"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(2), name=None)}
     352 |  # node_Cast_352
            %"val_277"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
     353 |  # node_Constant_353
            %"val_278"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     354 |  # node_Reshape_354
            %"val_279"<?,?> ⬅️ ::Reshape(%"val_277", %"val_278") {allowzero=0}
     355 |  # node_Constant_355
            %"val_280"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     356 |  # node_Slice_356
            %"slice_6"<FLOAT,[1,48,512]> ⬅️ ::Slice(%"slice_5", %"val_272", %"val_275", %"val_279", %"val_280")
     357 |  # node_aten_unsqueeze_357
            %"unsqueeze_5"<FLOAT,[1,48,512,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_6") {dim=3}
     358 |  # node_Constant_358
            %"val_281"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  48, 512, 431]), name=None)}
     359 |  # node_aten_expand_359
            %"expand"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::aten_expand(%"unsqueeze_5", %"val_281")
     360 |  # node_Constant_360
            %"val_282"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0.2, dtype=float32), name=None)}
     361 |  # node_Mul_361
            %"mul_11"<FLOAT,[1,48,512,431]> ⬅️ ::Mul(%"expand", %"val_282")
     362 |  # node_aten_add_362
            %"add_16"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"glu_5", %"mul_11") {alpha=1.0}
     363 |  # node_Constant_363
            %"val_283"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([0, 2]), name=None)}
     364 |  # node_aten_constant_pad_nd_364
            %"constant_pad_nd"<FLOAT,[1,48,110252]> ⬅️ pkg.onnxscript.torch_lib::aten_constant_pad_nd(%"glu_2", %"val_283") {value=0.0}
     365 |  # node_Conv_365
            %"convolution_12"<FLOAT,[1,96,27563]> ⬅️ ::Conv(%"constant_pad_nd", %"tencoder.1.conv.weight", %"tencoder.1.conv.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[2, 2], strides=[4]}
     366 |  # node__aten_gelu_approximate_none_366
            %"gelu_6"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_12")
     367 |  # node_Conv_367
            %"convolution_13"<FLOAT,[1,12,27563]> ⬅️ ::Conv(%"gelu_6", %"tencoder.1.dconv.layers.0.0.weight", %"tencoder.1.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
     368 |  # node_Constant_368
            %"val_284"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     369 |  # node_Reshape_369
            %"val_285"<?,?> ⬅️ ::Reshape(%"val_35", %"val_284") {allowzero=0}
     370 |  # node_Constant_370
            %"val_286"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     371 |  # node_Concat_371
            %"val_287"<?,?> ⬅️ ::Concat(%"val_286", %"val_285", %"val_284") {axis=0}
     372 |  # node_Reshape_372
            %"val_288"<?,?> ⬅️ ::Reshape(%"convolution_13", %"val_287") {allowzero=0}
     373 |  # node_Constant_373
            %"val_289"<?,?> ⬅️ ::Constant() {value_float=1.0}
     374 |  # node_CastLike_374
            %"val_290"<?,?> ⬅️ ::CastLike(%"val_289", %"convolution_13")
     375 |  # node_Expand_375
            %"val_291"<?,?> ⬅️ ::Expand(%"val_290", %"val_285")
     376 |  # node_Constant_376
            %"val_292"<?,?> ⬅️ ::Constant() {value_float=0.0}
     377 |  # node_CastLike_377
            %"val_293"<?,?> ⬅️ ::CastLike(%"val_292", %"convolution_13")
     378 |  # node_Expand_378
            %"val_294"<?,?> ⬅️ ::Expand(%"val_293", %"val_285")
     379 |  # node_InstanceNormalization_379
            %"val_295"<?,?> ⬅️ ::InstanceNormalization(%"val_288", %"val_291", %"val_294") {epsilon=1e-05}
     380 |  # node_Shape_380
            %"val_296"<?,?> ⬅️ ::Shape(%"convolution_13") {start=0}
     381 |  # node_Reshape_381
            %"val_297"<?,?> ⬅️ ::Reshape(%"val_295", %"val_296") {allowzero=0}
     382 |  # node_Constant_382
            %"val_298"<?,?> ⬅️ ::Constant() {value_int=1}
     383 |  # node_Sub_383
            %"val_299"<?,?> ⬅️ ::Sub(%"val_50", %"val_298")
     384 |  # node_Range_384
            %"val_300"<?,?> ⬅️ ::Range(%"val_298", %"val_299", %"val_298")
     385 |  # node_Unsqueeze_385
            %"val_301"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.0.1.weight", %"val_300")
     386 |  # node_Unsqueeze_386
            %"val_302"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.0.1.bias", %"val_300")
     387 |  # node_CastLike_387
            %"val_303"<?,?> ⬅️ ::CastLike(%"val_301", %"val_297")
     388 |  # node_Mul_388
            %"val_304"<?,?> ⬅️ ::Mul(%"val_297", %"val_303")
     389 |  # node_CastLike_389
            %"val_305"<?,?> ⬅️ ::CastLike(%"val_302", %"val_304")
     390 |  # node_Add_390
            %"group_norm_8"<FLOAT,[1,12,27563]> ⬅️ ::Add(%"val_304", %"val_305")
     391 |  # node__aten_gelu_approximate_none_391
            %"gelu_7"<FLOAT,[1,12,27563]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_8")
     392 |  # node_Conv_392
            %"convolution_14"<FLOAT,[1,192,27563]> ⬅️ ::Conv(%"gelu_7", %"tencoder.1.dconv.layers.0.3.weight", %"tencoder.1.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     393 |  # node_Constant_393
            %"val_306"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     394 |  # node_Reshape_394
            %"val_307"<?,?> ⬅️ ::Reshape(%"val_35", %"val_306") {allowzero=0}
     395 |  # node_Constant_395
            %"val_308"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     396 |  # node_Concat_396
            %"val_309"<?,?> ⬅️ ::Concat(%"val_308", %"val_307", %"val_306") {axis=0}
     397 |  # node_Reshape_397
            %"val_310"<?,?> ⬅️ ::Reshape(%"convolution_14", %"val_309") {allowzero=0}
     398 |  # node_Constant_398
            %"val_311"<?,?> ⬅️ ::Constant() {value_float=1.0}
     399 |  # node_CastLike_399
            %"val_312"<?,?> ⬅️ ::CastLike(%"val_311", %"convolution_14")
     400 |  # node_Expand_400
            %"val_313"<?,?> ⬅️ ::Expand(%"val_312", %"val_307")
     401 |  # node_Constant_401
            %"val_314"<?,?> ⬅️ ::Constant() {value_float=0.0}
     402 |  # node_CastLike_402
            %"val_315"<?,?> ⬅️ ::CastLike(%"val_314", %"convolution_14")
     403 |  # node_Expand_403
            %"val_316"<?,?> ⬅️ ::Expand(%"val_315", %"val_307")
     404 |  # node_InstanceNormalization_404
            %"val_317"<?,?> ⬅️ ::InstanceNormalization(%"val_310", %"val_313", %"val_316") {epsilon=1e-05}
     405 |  # node_Shape_405
            %"val_318"<?,?> ⬅️ ::Shape(%"convolution_14") {start=0}
     406 |  # node_Reshape_406
            %"val_319"<?,?> ⬅️ ::Reshape(%"val_317", %"val_318") {allowzero=0}
     407 |  # node_Constant_407
            %"val_320"<?,?> ⬅️ ::Constant() {value_int=1}
     408 |  # node_Sub_408
            %"val_321"<?,?> ⬅️ ::Sub(%"val_50", %"val_320")
     409 |  # node_Range_409
            %"val_322"<?,?> ⬅️ ::Range(%"val_320", %"val_321", %"val_320")
     410 |  # node_Unsqueeze_410
            %"val_323"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.0.4.weight", %"val_322")
     411 |  # node_Unsqueeze_411
            %"val_324"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.0.4.bias", %"val_322")
     412 |  # node_CastLike_412
            %"val_325"<?,?> ⬅️ ::CastLike(%"val_323", %"val_319")
     413 |  # node_Mul_413
            %"val_326"<?,?> ⬅️ ::Mul(%"val_319", %"val_325")
     414 |  # node_CastLike_414
            %"val_327"<?,?> ⬅️ ::CastLike(%"val_324", %"val_326")
     415 |  # node_Add_415
            %"group_norm_9"<FLOAT,[1,192,27563]> ⬅️ ::Add(%"val_326", %"val_327")
     416 |  # node_aten_glu_416
            %"glu_6"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_9") {dim=1}
     417 |  # node_Cast_417
            %"val_328"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     418 |  # node_Constant_418
            %"val_329"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     419 |  # node_Reshape_419
            %"val_330"<?,?> ⬅️ ::Reshape(%"val_328", %"val_329") {allowzero=0}
     420 |  # node_Cast_420
            %"val_331"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     421 |  # node_Constant_421
            %"val_332"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     422 |  # node_Reshape_422
            %"val_333"<?,?> ⬅️ ::Reshape(%"val_331", %"val_332") {allowzero=0}
     423 |  # node_Cast_423
            %"val_334"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     424 |  # node_Constant_424
            %"val_335"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     425 |  # node_Reshape_425
            %"val_336"<?,?> ⬅️ ::Reshape(%"val_334", %"val_335") {allowzero=0}
     426 |  # node_Constant_426
            %"val_337"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     427 |  # node_Slice_427
            %"slice_7"<FLOAT,[96]> ⬅️ ::Slice(%"tencoder.1.dconv.layers.0.6.scale", %"val_330", %"val_333", %"val_336", %"val_337")
     428 |  # node_aten_unsqueeze_428
            %"unsqueeze_6"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_7") {dim=1}
     429 |  # node_Mul_429
            %"mul_12"<FLOAT,[1,96,27563]> ⬅️ ::Mul(%"unsqueeze_6", %"glu_6")
     430 |  # node_aten_add_430
            %"add_17"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_6", %"mul_12") {alpha=1.0}
     431 |  # node_Conv_431
            %"convolution_15"<FLOAT,[1,12,27563]> ⬅️ ::Conv(%"add_17", %"tencoder.1.dconv.layers.1.0.weight", %"tencoder.1.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     432 |  # node_Constant_432
            %"val_338"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     433 |  # node_Reshape_433
            %"val_339"<?,?> ⬅️ ::Reshape(%"val_35", %"val_338") {allowzero=0}
     434 |  # node_Constant_434
            %"val_340"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     435 |  # node_Concat_435
            %"val_341"<?,?> ⬅️ ::Concat(%"val_340", %"val_339", %"val_338") {axis=0}
     436 |  # node_Reshape_436
            %"val_342"<?,?> ⬅️ ::Reshape(%"convolution_15", %"val_341") {allowzero=0}
     437 |  # node_Constant_437
            %"val_343"<?,?> ⬅️ ::Constant() {value_float=1.0}
     438 |  # node_CastLike_438
            %"val_344"<?,?> ⬅️ ::CastLike(%"val_343", %"convolution_15")
     439 |  # node_Expand_439
            %"val_345"<?,?> ⬅️ ::Expand(%"val_344", %"val_339")
     440 |  # node_Constant_440
            %"val_346"<?,?> ⬅️ ::Constant() {value_float=0.0}
     441 |  # node_CastLike_441
            %"val_347"<?,?> ⬅️ ::CastLike(%"val_346", %"convolution_15")
     442 |  # node_Expand_442
            %"val_348"<?,?> ⬅️ ::Expand(%"val_347", %"val_339")
     443 |  # node_InstanceNormalization_443
            %"val_349"<?,?> ⬅️ ::InstanceNormalization(%"val_342", %"val_345", %"val_348") {epsilon=1e-05}
     444 |  # node_Shape_444
            %"val_350"<?,?> ⬅️ ::Shape(%"convolution_15") {start=0}
     445 |  # node_Reshape_445
            %"val_351"<?,?> ⬅️ ::Reshape(%"val_349", %"val_350") {allowzero=0}
     446 |  # node_Constant_446
            %"val_352"<?,?> ⬅️ ::Constant() {value_int=1}
     447 |  # node_Sub_447
            %"val_353"<?,?> ⬅️ ::Sub(%"val_50", %"val_352")
     448 |  # node_Range_448
            %"val_354"<?,?> ⬅️ ::Range(%"val_352", %"val_353", %"val_352")
     449 |  # node_Unsqueeze_449
            %"val_355"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.1.1.weight", %"val_354")
     450 |  # node_Unsqueeze_450
            %"val_356"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.1.1.bias", %"val_354")
     451 |  # node_CastLike_451
            %"val_357"<?,?> ⬅️ ::CastLike(%"val_355", %"val_351")
     452 |  # node_Mul_452
            %"val_358"<?,?> ⬅️ ::Mul(%"val_351", %"val_357")
     453 |  # node_CastLike_453
            %"val_359"<?,?> ⬅️ ::CastLike(%"val_356", %"val_358")
     454 |  # node_Add_454
            %"group_norm_10"<FLOAT,[1,12,27563]> ⬅️ ::Add(%"val_358", %"val_359")
     455 |  # node__aten_gelu_approximate_none_455
            %"gelu_8"<FLOAT,[1,12,27563]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_10")
     456 |  # node_Conv_456
            %"convolution_16"<FLOAT,[1,192,27563]> ⬅️ ::Conv(%"gelu_8", %"tencoder.1.dconv.layers.1.3.weight", %"tencoder.1.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     457 |  # node_Constant_457
            %"val_360"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     458 |  # node_Reshape_458
            %"val_361"<?,?> ⬅️ ::Reshape(%"val_35", %"val_360") {allowzero=0}
     459 |  # node_Constant_459
            %"val_362"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     460 |  # node_Concat_460
            %"val_363"<?,?> ⬅️ ::Concat(%"val_362", %"val_361", %"val_360") {axis=0}
     461 |  # node_Reshape_461
            %"val_364"<?,?> ⬅️ ::Reshape(%"convolution_16", %"val_363") {allowzero=0}
     462 |  # node_Constant_462
            %"val_365"<?,?> ⬅️ ::Constant() {value_float=1.0}
     463 |  # node_CastLike_463
            %"val_366"<?,?> ⬅️ ::CastLike(%"val_365", %"convolution_16")
     464 |  # node_Expand_464
            %"val_367"<?,?> ⬅️ ::Expand(%"val_366", %"val_361")
     465 |  # node_Constant_465
            %"val_368"<?,?> ⬅️ ::Constant() {value_float=0.0}
     466 |  # node_CastLike_466
            %"val_369"<?,?> ⬅️ ::CastLike(%"val_368", %"convolution_16")
     467 |  # node_Expand_467
            %"val_370"<?,?> ⬅️ ::Expand(%"val_369", %"val_361")
     468 |  # node_InstanceNormalization_468
            %"val_371"<?,?> ⬅️ ::InstanceNormalization(%"val_364", %"val_367", %"val_370") {epsilon=1e-05}
     469 |  # node_Shape_469
            %"val_372"<?,?> ⬅️ ::Shape(%"convolution_16") {start=0}
     470 |  # node_Reshape_470
            %"val_373"<?,?> ⬅️ ::Reshape(%"val_371", %"val_372") {allowzero=0}
     471 |  # node_Constant_471
            %"val_374"<?,?> ⬅️ ::Constant() {value_int=1}
     472 |  # node_Sub_472
            %"val_375"<?,?> ⬅️ ::Sub(%"val_50", %"val_374")
     473 |  # node_Range_473
            %"val_376"<?,?> ⬅️ ::Range(%"val_374", %"val_375", %"val_374")
     474 |  # node_Unsqueeze_474
            %"val_377"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.1.4.weight", %"val_376")
     475 |  # node_Unsqueeze_475
            %"val_378"<?,?> ⬅️ ::Unsqueeze(%"tencoder.1.dconv.layers.1.4.bias", %"val_376")
     476 |  # node_CastLike_476
            %"val_379"<?,?> ⬅️ ::CastLike(%"val_377", %"val_373")
     477 |  # node_Mul_477
            %"val_380"<?,?> ⬅️ ::Mul(%"val_373", %"val_379")
     478 |  # node_CastLike_478
            %"val_381"<?,?> ⬅️ ::CastLike(%"val_378", %"val_380")
     479 |  # node_Add_479
            %"group_norm_11"<FLOAT,[1,192,27563]> ⬅️ ::Add(%"val_380", %"val_381")
     480 |  # node_aten_glu_480
            %"glu_7"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_11") {dim=1}
     481 |  # node_Cast_481
            %"val_382"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     482 |  # node_Constant_482
            %"val_383"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     483 |  # node_Reshape_483
            %"val_384"<?,?> ⬅️ ::Reshape(%"val_382", %"val_383") {allowzero=0}
     484 |  # node_Cast_484
            %"val_385"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     485 |  # node_Constant_485
            %"val_386"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     486 |  # node_Reshape_486
            %"val_387"<?,?> ⬅️ ::Reshape(%"val_385", %"val_386") {allowzero=0}
     487 |  # node_Cast_487
            %"val_388"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     488 |  # node_Constant_488
            %"val_389"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     489 |  # node_Reshape_489
            %"val_390"<?,?> ⬅️ ::Reshape(%"val_388", %"val_389") {allowzero=0}
     490 |  # node_Constant_490
            %"val_391"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     491 |  # node_Slice_491
            %"slice_8"<FLOAT,[96]> ⬅️ ::Slice(%"tencoder.1.dconv.layers.1.6.scale", %"val_384", %"val_387", %"val_390", %"val_391")
     492 |  # node_aten_unsqueeze_492
            %"unsqueeze_7"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_8") {dim=1}
     493 |  # node_Mul_493
            %"mul_13"<FLOAT,[1,96,27563]> ⬅️ ::Mul(%"unsqueeze_7", %"glu_7")
     494 |  # node_aten_add_494
            %"add_18"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_17", %"mul_13") {alpha=1.0}
     495 |  # node_Conv_495
            %"convolution_17"<FLOAT,[1,192,27563]> ⬅️ ::Conv(%"add_18", %"tencoder.1.rewrite.weight", %"tencoder.1.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     496 |  # node_aten_glu_496
            %"glu_8"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_17") {dim=1}
     497 |  # node_Conv_497
            %"convolution_18"<FLOAT,[1,96,128,431]> ⬅️ ::Conv(%"add_16", %"encoder.1.conv.weight", %"encoder.1.conv.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[2, 0, 2, 0], strides=[4, 1]}
     498 |  # node__aten_gelu_approximate_none_498
            %"gelu_9"<FLOAT,[1,96,128,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_18")
     499 |  # node_Transpose_499
            %"permute_3"<FLOAT,[1,128,96,431]> ⬅️ ::Transpose(%"gelu_9") {perm=[0, 2, 1, 3]}
     500 |  # node_Constant_500
            %"val_392"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([128,  96, 431]), name=None)}
     501 |  # node_Cast_501
            %"val_393"<?,?> ⬅️ ::Cast(%"val_392") {to=7}
     502 |  # node_Reshape_502
            %"view_2"<FLOAT,[128,96,431]> ⬅️ ::Reshape(%"permute_3", %"val_393") {allowzero=0}
     503 |  # node_Conv_503
            %"convolution_19"<FLOAT,[128,12,431]> ⬅️ ::Conv(%"view_2", %"encoder.1.dconv.layers.0.0.weight", %"encoder.1.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
     504 |  # node_Constant_504
            %"val_394"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     505 |  # node_Reshape_505
            %"val_395"<?,?> ⬅️ ::Reshape(%"val_35", %"val_394") {allowzero=0}
     506 |  # node_Constant_506
            %"val_396"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     507 |  # node_Concat_507
            %"val_397"<?,?> ⬅️ ::Concat(%"val_396", %"val_395", %"val_394") {axis=0}
     508 |  # node_Reshape_508
            %"val_398"<?,?> ⬅️ ::Reshape(%"convolution_19", %"val_397") {allowzero=0}
     509 |  # node_Constant_509
            %"val_399"<?,?> ⬅️ ::Constant() {value_float=1.0}
     510 |  # node_CastLike_510
            %"val_400"<?,?> ⬅️ ::CastLike(%"val_399", %"convolution_19")
     511 |  # node_Expand_511
            %"val_401"<?,?> ⬅️ ::Expand(%"val_400", %"val_395")
     512 |  # node_Constant_512
            %"val_402"<?,?> ⬅️ ::Constant() {value_float=0.0}
     513 |  # node_CastLike_513
            %"val_403"<?,?> ⬅️ ::CastLike(%"val_402", %"convolution_19")
     514 |  # node_Expand_514
            %"val_404"<?,?> ⬅️ ::Expand(%"val_403", %"val_395")
     515 |  # node_InstanceNormalization_515
            %"val_405"<?,?> ⬅️ ::InstanceNormalization(%"val_398", %"val_401", %"val_404") {epsilon=1e-05}
     516 |  # node_Shape_516
            %"val_406"<?,?> ⬅️ ::Shape(%"convolution_19") {start=0}
     517 |  # node_Reshape_517
            %"val_407"<?,?> ⬅️ ::Reshape(%"val_405", %"val_406") {allowzero=0}
     518 |  # node_Constant_518
            %"val_408"<?,?> ⬅️ ::Constant() {value_int=1}
     519 |  # node_Sub_519
            %"val_409"<?,?> ⬅️ ::Sub(%"val_50", %"val_408")
     520 |  # node_Range_520
            %"val_410"<?,?> ⬅️ ::Range(%"val_408", %"val_409", %"val_408")
     521 |  # node_Unsqueeze_521
            %"val_411"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.0.1.weight", %"val_410")
     522 |  # node_Unsqueeze_522
            %"val_412"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.0.1.bias", %"val_410")
     523 |  # node_CastLike_523
            %"val_413"<?,?> ⬅️ ::CastLike(%"val_411", %"val_407")
     524 |  # node_Mul_524
            %"val_414"<?,?> ⬅️ ::Mul(%"val_407", %"val_413")
     525 |  # node_CastLike_525
            %"val_415"<?,?> ⬅️ ::CastLike(%"val_412", %"val_414")
     526 |  # node_Add_526
            %"group_norm_12"<FLOAT,[128,12,431]> ⬅️ ::Add(%"val_414", %"val_415")
     527 |  # node__aten_gelu_approximate_none_527
            %"gelu_10"<FLOAT,[128,12,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_12")
     528 |  # node_Conv_528
            %"convolution_20"<FLOAT,[128,192,431]> ⬅️ ::Conv(%"gelu_10", %"encoder.1.dconv.layers.0.3.weight", %"encoder.1.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     529 |  # node_Constant_529
            %"val_416"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     530 |  # node_Reshape_530
            %"val_417"<?,?> ⬅️ ::Reshape(%"val_35", %"val_416") {allowzero=0}
     531 |  # node_Constant_531
            %"val_418"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     532 |  # node_Concat_532
            %"val_419"<?,?> ⬅️ ::Concat(%"val_418", %"val_417", %"val_416") {axis=0}
     533 |  # node_Reshape_533
            %"val_420"<?,?> ⬅️ ::Reshape(%"convolution_20", %"val_419") {allowzero=0}
     534 |  # node_Constant_534
            %"val_421"<?,?> ⬅️ ::Constant() {value_float=1.0}
     535 |  # node_CastLike_535
            %"val_422"<?,?> ⬅️ ::CastLike(%"val_421", %"convolution_20")
     536 |  # node_Expand_536
            %"val_423"<?,?> ⬅️ ::Expand(%"val_422", %"val_417")
     537 |  # node_Constant_537
            %"val_424"<?,?> ⬅️ ::Constant() {value_float=0.0}
     538 |  # node_CastLike_538
            %"val_425"<?,?> ⬅️ ::CastLike(%"val_424", %"convolution_20")
     539 |  # node_Expand_539
            %"val_426"<?,?> ⬅️ ::Expand(%"val_425", %"val_417")
     540 |  # node_InstanceNormalization_540
            %"val_427"<?,?> ⬅️ ::InstanceNormalization(%"val_420", %"val_423", %"val_426") {epsilon=1e-05}
     541 |  # node_Shape_541
            %"val_428"<?,?> ⬅️ ::Shape(%"convolution_20") {start=0}
     542 |  # node_Reshape_542
            %"val_429"<?,?> ⬅️ ::Reshape(%"val_427", %"val_428") {allowzero=0}
     543 |  # node_Constant_543
            %"val_430"<?,?> ⬅️ ::Constant() {value_int=1}
     544 |  # node_Sub_544
            %"val_431"<?,?> ⬅️ ::Sub(%"val_50", %"val_430")
     545 |  # node_Range_545
            %"val_432"<?,?> ⬅️ ::Range(%"val_430", %"val_431", %"val_430")
     546 |  # node_Unsqueeze_546
            %"val_433"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.0.4.weight", %"val_432")
     547 |  # node_Unsqueeze_547
            %"val_434"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.0.4.bias", %"val_432")
     548 |  # node_CastLike_548
            %"val_435"<?,?> ⬅️ ::CastLike(%"val_433", %"val_429")
     549 |  # node_Mul_549
            %"val_436"<?,?> ⬅️ ::Mul(%"val_429", %"val_435")
     550 |  # node_CastLike_550
            %"val_437"<?,?> ⬅️ ::CastLike(%"val_434", %"val_436")
     551 |  # node_Add_551
            %"group_norm_13"<FLOAT,[128,192,431]> ⬅️ ::Add(%"val_436", %"val_437")
     552 |  # node_aten_glu_552
            %"glu_9"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_13") {dim=1}
     553 |  # node_Cast_553
            %"val_438"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     554 |  # node_Constant_554
            %"val_439"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     555 |  # node_Reshape_555
            %"val_440"<?,?> ⬅️ ::Reshape(%"val_438", %"val_439") {allowzero=0}
     556 |  # node_Cast_556
            %"val_441"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     557 |  # node_Constant_557
            %"val_442"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     558 |  # node_Reshape_558
            %"val_443"<?,?> ⬅️ ::Reshape(%"val_441", %"val_442") {allowzero=0}
     559 |  # node_Cast_559
            %"val_444"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     560 |  # node_Constant_560
            %"val_445"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     561 |  # node_Reshape_561
            %"val_446"<?,?> ⬅️ ::Reshape(%"val_444", %"val_445") {allowzero=0}
     562 |  # node_Constant_562
            %"val_447"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     563 |  # node_Slice_563
            %"slice_9"<FLOAT,[96]> ⬅️ ::Slice(%"encoder.1.dconv.layers.0.6.scale", %"val_440", %"val_443", %"val_446", %"val_447")
     564 |  # node_aten_unsqueeze_564
            %"unsqueeze_8"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_9") {dim=1}
     565 |  # node_Mul_565
            %"mul_14"<FLOAT,[128,96,431]> ⬅️ ::Mul(%"unsqueeze_8", %"glu_9")
     566 |  # node_aten_add_566
            %"add_19"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_2", %"mul_14") {alpha=1.0}
     567 |  # node_Conv_567
            %"convolution_21"<FLOAT,[128,12,431]> ⬅️ ::Conv(%"add_19", %"encoder.1.dconv.layers.1.0.weight", %"encoder.1.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     568 |  # node_Constant_568
            %"val_448"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     569 |  # node_Reshape_569
            %"val_449"<?,?> ⬅️ ::Reshape(%"val_35", %"val_448") {allowzero=0}
     570 |  # node_Constant_570
            %"val_450"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     571 |  # node_Concat_571
            %"val_451"<?,?> ⬅️ ::Concat(%"val_450", %"val_449", %"val_448") {axis=0}
     572 |  # node_Reshape_572
            %"val_452"<?,?> ⬅️ ::Reshape(%"convolution_21", %"val_451") {allowzero=0}
     573 |  # node_Constant_573
            %"val_453"<?,?> ⬅️ ::Constant() {value_float=1.0}
     574 |  # node_CastLike_574
            %"val_454"<?,?> ⬅️ ::CastLike(%"val_453", %"convolution_21")
     575 |  # node_Expand_575
            %"val_455"<?,?> ⬅️ ::Expand(%"val_454", %"val_449")
     576 |  # node_Constant_576
            %"val_456"<?,?> ⬅️ ::Constant() {value_float=0.0}
     577 |  # node_CastLike_577
            %"val_457"<?,?> ⬅️ ::CastLike(%"val_456", %"convolution_21")
     578 |  # node_Expand_578
            %"val_458"<?,?> ⬅️ ::Expand(%"val_457", %"val_449")
     579 |  # node_InstanceNormalization_579
            %"val_459"<?,?> ⬅️ ::InstanceNormalization(%"val_452", %"val_455", %"val_458") {epsilon=1e-05}
     580 |  # node_Shape_580
            %"val_460"<?,?> ⬅️ ::Shape(%"convolution_21") {start=0}
     581 |  # node_Reshape_581
            %"val_461"<?,?> ⬅️ ::Reshape(%"val_459", %"val_460") {allowzero=0}
     582 |  # node_Constant_582
            %"val_462"<?,?> ⬅️ ::Constant() {value_int=1}
     583 |  # node_Sub_583
            %"val_463"<?,?> ⬅️ ::Sub(%"val_50", %"val_462")
     584 |  # node_Range_584
            %"val_464"<?,?> ⬅️ ::Range(%"val_462", %"val_463", %"val_462")
     585 |  # node_Unsqueeze_585
            %"val_465"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.1.1.weight", %"val_464")
     586 |  # node_Unsqueeze_586
            %"val_466"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.1.1.bias", %"val_464")
     587 |  # node_CastLike_587
            %"val_467"<?,?> ⬅️ ::CastLike(%"val_465", %"val_461")
     588 |  # node_Mul_588
            %"val_468"<?,?> ⬅️ ::Mul(%"val_461", %"val_467")
     589 |  # node_CastLike_589
            %"val_469"<?,?> ⬅️ ::CastLike(%"val_466", %"val_468")
     590 |  # node_Add_590
            %"group_norm_14"<FLOAT,[128,12,431]> ⬅️ ::Add(%"val_468", %"val_469")
     591 |  # node__aten_gelu_approximate_none_591
            %"gelu_11"<FLOAT,[128,12,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_14")
     592 |  # node_Conv_592
            %"convolution_22"<FLOAT,[128,192,431]> ⬅️ ::Conv(%"gelu_11", %"encoder.1.dconv.layers.1.3.weight", %"encoder.1.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     593 |  # node_Constant_593
            %"val_470"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     594 |  # node_Reshape_594
            %"val_471"<?,?> ⬅️ ::Reshape(%"val_35", %"val_470") {allowzero=0}
     595 |  # node_Constant_595
            %"val_472"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     596 |  # node_Concat_596
            %"val_473"<?,?> ⬅️ ::Concat(%"val_472", %"val_471", %"val_470") {axis=0}
     597 |  # node_Reshape_597
            %"val_474"<?,?> ⬅️ ::Reshape(%"convolution_22", %"val_473") {allowzero=0}
     598 |  # node_Constant_598
            %"val_475"<?,?> ⬅️ ::Constant() {value_float=1.0}
     599 |  # node_CastLike_599
            %"val_476"<?,?> ⬅️ ::CastLike(%"val_475", %"convolution_22")
     600 |  # node_Expand_600
            %"val_477"<?,?> ⬅️ ::Expand(%"val_476", %"val_471")
     601 |  # node_Constant_601
            %"val_478"<?,?> ⬅️ ::Constant() {value_float=0.0}
     602 |  # node_CastLike_602
            %"val_479"<?,?> ⬅️ ::CastLike(%"val_478", %"convolution_22")
     603 |  # node_Expand_603
            %"val_480"<?,?> ⬅️ ::Expand(%"val_479", %"val_471")
     604 |  # node_InstanceNormalization_604
            %"val_481"<?,?> ⬅️ ::InstanceNormalization(%"val_474", %"val_477", %"val_480") {epsilon=1e-05}
     605 |  # node_Shape_605
            %"val_482"<?,?> ⬅️ ::Shape(%"convolution_22") {start=0}
     606 |  # node_Reshape_606
            %"val_483"<?,?> ⬅️ ::Reshape(%"val_481", %"val_482") {allowzero=0}
     607 |  # node_Constant_607
            %"val_484"<?,?> ⬅️ ::Constant() {value_int=1}
     608 |  # node_Sub_608
            %"val_485"<?,?> ⬅️ ::Sub(%"val_50", %"val_484")
     609 |  # node_Range_609
            %"val_486"<?,?> ⬅️ ::Range(%"val_484", %"val_485", %"val_484")
     610 |  # node_Unsqueeze_610
            %"val_487"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.1.4.weight", %"val_486")
     611 |  # node_Unsqueeze_611
            %"val_488"<?,?> ⬅️ ::Unsqueeze(%"encoder.1.dconv.layers.1.4.bias", %"val_486")
     612 |  # node_CastLike_612
            %"val_489"<?,?> ⬅️ ::CastLike(%"val_487", %"val_483")
     613 |  # node_Mul_613
            %"val_490"<?,?> ⬅️ ::Mul(%"val_483", %"val_489")
     614 |  # node_CastLike_614
            %"val_491"<?,?> ⬅️ ::CastLike(%"val_488", %"val_490")
     615 |  # node_Add_615
            %"group_norm_15"<FLOAT,[128,192,431]> ⬅️ ::Add(%"val_490", %"val_491")
     616 |  # node_aten_glu_616
            %"glu_10"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_15") {dim=1}
     617 |  # node_Cast_617
            %"val_492"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     618 |  # node_Constant_618
            %"val_493"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     619 |  # node_Reshape_619
            %"val_494"<?,?> ⬅️ ::Reshape(%"val_492", %"val_493") {allowzero=0}
     620 |  # node_Cast_620
            %"val_495"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     621 |  # node_Constant_621
            %"val_496"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     622 |  # node_Reshape_622
            %"val_497"<?,?> ⬅️ ::Reshape(%"val_495", %"val_496") {allowzero=0}
     623 |  # node_Cast_623
            %"val_498"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     624 |  # node_Constant_624
            %"val_499"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     625 |  # node_Reshape_625
            %"val_500"<?,?> ⬅️ ::Reshape(%"val_498", %"val_499") {allowzero=0}
     626 |  # node_Constant_626
            %"val_501"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     627 |  # node_Slice_627
            %"slice_10"<FLOAT,[96]> ⬅️ ::Slice(%"encoder.1.dconv.layers.1.6.scale", %"val_494", %"val_497", %"val_500", %"val_501")
     628 |  # node_aten_unsqueeze_628
            %"unsqueeze_9"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_10") {dim=1}
     629 |  # node_Mul_629
            %"mul_15"<FLOAT,[128,96,431]> ⬅️ ::Mul(%"unsqueeze_9", %"glu_10")
     630 |  # node_aten_add_630
            %"add_20"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_19", %"mul_15") {alpha=1.0}
     631 |  # node_Constant_631
            %"val_502"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 128,  96, 431]), name=None)}
     632 |  # node_Cast_632
            %"val_503"<?,?> ⬅️ ::Cast(%"val_502") {to=7}
     633 |  # node_Reshape_633
            %"view_3"<FLOAT,[1,128,96,431]> ⬅️ ::Reshape(%"add_20", %"val_503") {allowzero=0}
     634 |  # node_Transpose_634
            %"permute_4"<FLOAT,[1,96,128,431]> ⬅️ ::Transpose(%"view_3") {perm=[0, 2, 1, 3]}
     635 |  # node_Conv_635
            %"convolution_23"<FLOAT,[1,192,128,431]> ⬅️ ::Conv(%"permute_4", %"encoder.1.rewrite.weight", %"encoder.1.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[0, 0, 0, 0], strides=[1, 1]}
     636 |  # node_aten_glu_636
            %"glu_11"<FLOAT,[1,96,128,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_23") {dim=1}
     637 |  # node_Constant_637
            %"val_504"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([0, 1]), name=None)}
     638 |  # node_aten_constant_pad_nd_638
            %"constant_pad_nd_1"<FLOAT,[1,96,27564]> ⬅️ pkg.onnxscript.torch_lib::aten_constant_pad_nd(%"glu_8", %"val_504") {value=0.0}
     639 |  # node_Conv_639
            %"convolution_24"<FLOAT,[1,192,6891]> ⬅️ ::Conv(%"constant_pad_nd_1", %"tencoder.2.conv.weight", %"tencoder.2.conv.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[2, 2], strides=[4]}
     640 |  # node__aten_gelu_approximate_none_640
            %"gelu_12"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_24")
     641 |  # node_Conv_641
            %"convolution_25"<FLOAT,[1,24,6891]> ⬅️ ::Conv(%"gelu_12", %"tencoder.2.dconv.layers.0.0.weight", %"tencoder.2.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
     642 |  # node_Constant_642
            %"val_505"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     643 |  # node_Reshape_643
            %"val_506"<?,?> ⬅️ ::Reshape(%"val_35", %"val_505") {allowzero=0}
     644 |  # node_Constant_644
            %"val_507"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     645 |  # node_Concat_645
            %"val_508"<?,?> ⬅️ ::Concat(%"val_507", %"val_506", %"val_505") {axis=0}
     646 |  # node_Reshape_646
            %"val_509"<?,?> ⬅️ ::Reshape(%"convolution_25", %"val_508") {allowzero=0}
     647 |  # node_Constant_647
            %"val_510"<?,?> ⬅️ ::Constant() {value_float=1.0}
     648 |  # node_CastLike_648
            %"val_511"<?,?> ⬅️ ::CastLike(%"val_510", %"convolution_25")
     649 |  # node_Expand_649
            %"val_512"<?,?> ⬅️ ::Expand(%"val_511", %"val_506")
     650 |  # node_Constant_650
            %"val_513"<?,?> ⬅️ ::Constant() {value_float=0.0}
     651 |  # node_CastLike_651
            %"val_514"<?,?> ⬅️ ::CastLike(%"val_513", %"convolution_25")
     652 |  # node_Expand_652
            %"val_515"<?,?> ⬅️ ::Expand(%"val_514", %"val_506")
     653 |  # node_InstanceNormalization_653
            %"val_516"<?,?> ⬅️ ::InstanceNormalization(%"val_509", %"val_512", %"val_515") {epsilon=1e-05}
     654 |  # node_Shape_654
            %"val_517"<?,?> ⬅️ ::Shape(%"convolution_25") {start=0}
     655 |  # node_Reshape_655
            %"val_518"<?,?> ⬅️ ::Reshape(%"val_516", %"val_517") {allowzero=0}
     656 |  # node_Constant_656
            %"val_519"<?,?> ⬅️ ::Constant() {value_int=1}
     657 |  # node_Sub_657
            %"val_520"<?,?> ⬅️ ::Sub(%"val_50", %"val_519")
     658 |  # node_Range_658
            %"val_521"<?,?> ⬅️ ::Range(%"val_519", %"val_520", %"val_519")
     659 |  # node_Unsqueeze_659
            %"val_522"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.0.1.weight", %"val_521")
     660 |  # node_Unsqueeze_660
            %"val_523"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.0.1.bias", %"val_521")
     661 |  # node_CastLike_661
            %"val_524"<?,?> ⬅️ ::CastLike(%"val_522", %"val_518")
     662 |  # node_Mul_662
            %"val_525"<?,?> ⬅️ ::Mul(%"val_518", %"val_524")
     663 |  # node_CastLike_663
            %"val_526"<?,?> ⬅️ ::CastLike(%"val_523", %"val_525")
     664 |  # node_Add_664
            %"group_norm_16"<FLOAT,[1,24,6891]> ⬅️ ::Add(%"val_525", %"val_526")
     665 |  # node__aten_gelu_approximate_none_665
            %"gelu_13"<FLOAT,[1,24,6891]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_16")
     666 |  # node_Conv_666
            %"convolution_26"<FLOAT,[1,384,6891]> ⬅️ ::Conv(%"gelu_13", %"tencoder.2.dconv.layers.0.3.weight", %"tencoder.2.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     667 |  # node_Constant_667
            %"val_527"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     668 |  # node_Reshape_668
            %"val_528"<?,?> ⬅️ ::Reshape(%"val_35", %"val_527") {allowzero=0}
     669 |  # node_Constant_669
            %"val_529"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     670 |  # node_Concat_670
            %"val_530"<?,?> ⬅️ ::Concat(%"val_529", %"val_528", %"val_527") {axis=0}
     671 |  # node_Reshape_671
            %"val_531"<?,?> ⬅️ ::Reshape(%"convolution_26", %"val_530") {allowzero=0}
     672 |  # node_Constant_672
            %"val_532"<?,?> ⬅️ ::Constant() {value_float=1.0}
     673 |  # node_CastLike_673
            %"val_533"<?,?> ⬅️ ::CastLike(%"val_532", %"convolution_26")
     674 |  # node_Expand_674
            %"val_534"<?,?> ⬅️ ::Expand(%"val_533", %"val_528")
     675 |  # node_Constant_675
            %"val_535"<?,?> ⬅️ ::Constant() {value_float=0.0}
     676 |  # node_CastLike_676
            %"val_536"<?,?> ⬅️ ::CastLike(%"val_535", %"convolution_26")
     677 |  # node_Expand_677
            %"val_537"<?,?> ⬅️ ::Expand(%"val_536", %"val_528")
     678 |  # node_InstanceNormalization_678
            %"val_538"<?,?> ⬅️ ::InstanceNormalization(%"val_531", %"val_534", %"val_537") {epsilon=1e-05}
     679 |  # node_Shape_679
            %"val_539"<?,?> ⬅️ ::Shape(%"convolution_26") {start=0}
     680 |  # node_Reshape_680
            %"val_540"<?,?> ⬅️ ::Reshape(%"val_538", %"val_539") {allowzero=0}
     681 |  # node_Constant_681
            %"val_541"<?,?> ⬅️ ::Constant() {value_int=1}
     682 |  # node_Sub_682
            %"val_542"<?,?> ⬅️ ::Sub(%"val_50", %"val_541")
     683 |  # node_Range_683
            %"val_543"<?,?> ⬅️ ::Range(%"val_541", %"val_542", %"val_541")
     684 |  # node_Unsqueeze_684
            %"val_544"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.0.4.weight", %"val_543")
     685 |  # node_Unsqueeze_685
            %"val_545"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.0.4.bias", %"val_543")
     686 |  # node_CastLike_686
            %"val_546"<?,?> ⬅️ ::CastLike(%"val_544", %"val_540")
     687 |  # node_Mul_687
            %"val_547"<?,?> ⬅️ ::Mul(%"val_540", %"val_546")
     688 |  # node_CastLike_688
            %"val_548"<?,?> ⬅️ ::CastLike(%"val_545", %"val_547")
     689 |  # node_Add_689
            %"group_norm_17"<FLOAT,[1,384,6891]> ⬅️ ::Add(%"val_547", %"val_548")
     690 |  # node_aten_glu_690
            %"glu_12"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_17") {dim=1}
     691 |  # node_Cast_691
            %"val_549"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     692 |  # node_Constant_692
            %"val_550"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     693 |  # node_Reshape_693
            %"val_551"<?,?> ⬅️ ::Reshape(%"val_549", %"val_550") {allowzero=0}
     694 |  # node_Cast_694
            %"val_552"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     695 |  # node_Constant_695
            %"val_553"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     696 |  # node_Reshape_696
            %"val_554"<?,?> ⬅️ ::Reshape(%"val_552", %"val_553") {allowzero=0}
     697 |  # node_Cast_697
            %"val_555"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     698 |  # node_Constant_698
            %"val_556"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     699 |  # node_Reshape_699
            %"val_557"<?,?> ⬅️ ::Reshape(%"val_555", %"val_556") {allowzero=0}
     700 |  # node_Constant_700
            %"val_558"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     701 |  # node_Slice_701
            %"slice_11"<FLOAT,[192]> ⬅️ ::Slice(%"tencoder.2.dconv.layers.0.6.scale", %"val_551", %"val_554", %"val_557", %"val_558")
     702 |  # node_aten_unsqueeze_702
            %"unsqueeze_10"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_11") {dim=1}
     703 |  # node_Mul_703
            %"mul_16"<FLOAT,[1,192,6891]> ⬅️ ::Mul(%"unsqueeze_10", %"glu_12")
     704 |  # node_aten_add_704
            %"add_21"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_12", %"mul_16") {alpha=1.0}
     705 |  # node_Conv_705
            %"convolution_27"<FLOAT,[1,24,6891]> ⬅️ ::Conv(%"add_21", %"tencoder.2.dconv.layers.1.0.weight", %"tencoder.2.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     706 |  # node_Constant_706
            %"val_559"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     707 |  # node_Reshape_707
            %"val_560"<?,?> ⬅️ ::Reshape(%"val_35", %"val_559") {allowzero=0}
     708 |  # node_Constant_708
            %"val_561"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     709 |  # node_Concat_709
            %"val_562"<?,?> ⬅️ ::Concat(%"val_561", %"val_560", %"val_559") {axis=0}
     710 |  # node_Reshape_710
            %"val_563"<?,?> ⬅️ ::Reshape(%"convolution_27", %"val_562") {allowzero=0}
     711 |  # node_Constant_711
            %"val_564"<?,?> ⬅️ ::Constant() {value_float=1.0}
     712 |  # node_CastLike_712
            %"val_565"<?,?> ⬅️ ::CastLike(%"val_564", %"convolution_27")
     713 |  # node_Expand_713
            %"val_566"<?,?> ⬅️ ::Expand(%"val_565", %"val_560")
     714 |  # node_Constant_714
            %"val_567"<?,?> ⬅️ ::Constant() {value_float=0.0}
     715 |  # node_CastLike_715
            %"val_568"<?,?> ⬅️ ::CastLike(%"val_567", %"convolution_27")
     716 |  # node_Expand_716
            %"val_569"<?,?> ⬅️ ::Expand(%"val_568", %"val_560")
     717 |  # node_InstanceNormalization_717
            %"val_570"<?,?> ⬅️ ::InstanceNormalization(%"val_563", %"val_566", %"val_569") {epsilon=1e-05}
     718 |  # node_Shape_718
            %"val_571"<?,?> ⬅️ ::Shape(%"convolution_27") {start=0}
     719 |  # node_Reshape_719
            %"val_572"<?,?> ⬅️ ::Reshape(%"val_570", %"val_571") {allowzero=0}
     720 |  # node_Constant_720
            %"val_573"<?,?> ⬅️ ::Constant() {value_int=1}
     721 |  # node_Sub_721
            %"val_574"<?,?> ⬅️ ::Sub(%"val_50", %"val_573")
     722 |  # node_Range_722
            %"val_575"<?,?> ⬅️ ::Range(%"val_573", %"val_574", %"val_573")
     723 |  # node_Unsqueeze_723
            %"val_576"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.1.1.weight", %"val_575")
     724 |  # node_Unsqueeze_724
            %"val_577"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.1.1.bias", %"val_575")
     725 |  # node_CastLike_725
            %"val_578"<?,?> ⬅️ ::CastLike(%"val_576", %"val_572")
     726 |  # node_Mul_726
            %"val_579"<?,?> ⬅️ ::Mul(%"val_572", %"val_578")
     727 |  # node_CastLike_727
            %"val_580"<?,?> ⬅️ ::CastLike(%"val_577", %"val_579")
     728 |  # node_Add_728
            %"group_norm_18"<FLOAT,[1,24,6891]> ⬅️ ::Add(%"val_579", %"val_580")
     729 |  # node__aten_gelu_approximate_none_729
            %"gelu_14"<FLOAT,[1,24,6891]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_18")
     730 |  # node_Conv_730
            %"convolution_28"<FLOAT,[1,384,6891]> ⬅️ ::Conv(%"gelu_14", %"tencoder.2.dconv.layers.1.3.weight", %"tencoder.2.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     731 |  # node_Constant_731
            %"val_581"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     732 |  # node_Reshape_732
            %"val_582"<?,?> ⬅️ ::Reshape(%"val_35", %"val_581") {allowzero=0}
     733 |  # node_Constant_733
            %"val_583"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     734 |  # node_Concat_734
            %"val_584"<?,?> ⬅️ ::Concat(%"val_583", %"val_582", %"val_581") {axis=0}
     735 |  # node_Reshape_735
            %"val_585"<?,?> ⬅️ ::Reshape(%"convolution_28", %"val_584") {allowzero=0}
     736 |  # node_Constant_736
            %"val_586"<?,?> ⬅️ ::Constant() {value_float=1.0}
     737 |  # node_CastLike_737
            %"val_587"<?,?> ⬅️ ::CastLike(%"val_586", %"convolution_28")
     738 |  # node_Expand_738
            %"val_588"<?,?> ⬅️ ::Expand(%"val_587", %"val_582")
     739 |  # node_Constant_739
            %"val_589"<?,?> ⬅️ ::Constant() {value_float=0.0}
     740 |  # node_CastLike_740
            %"val_590"<?,?> ⬅️ ::CastLike(%"val_589", %"convolution_28")
     741 |  # node_Expand_741
            %"val_591"<?,?> ⬅️ ::Expand(%"val_590", %"val_582")
     742 |  # node_InstanceNormalization_742
            %"val_592"<?,?> ⬅️ ::InstanceNormalization(%"val_585", %"val_588", %"val_591") {epsilon=1e-05}
     743 |  # node_Shape_743
            %"val_593"<?,?> ⬅️ ::Shape(%"convolution_28") {start=0}
     744 |  # node_Reshape_744
            %"val_594"<?,?> ⬅️ ::Reshape(%"val_592", %"val_593") {allowzero=0}
     745 |  # node_Constant_745
            %"val_595"<?,?> ⬅️ ::Constant() {value_int=1}
     746 |  # node_Sub_746
            %"val_596"<?,?> ⬅️ ::Sub(%"val_50", %"val_595")
     747 |  # node_Range_747
            %"val_597"<?,?> ⬅️ ::Range(%"val_595", %"val_596", %"val_595")
     748 |  # node_Unsqueeze_748
            %"val_598"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.1.4.weight", %"val_597")
     749 |  # node_Unsqueeze_749
            %"val_599"<?,?> ⬅️ ::Unsqueeze(%"tencoder.2.dconv.layers.1.4.bias", %"val_597")
     750 |  # node_CastLike_750
            %"val_600"<?,?> ⬅️ ::CastLike(%"val_598", %"val_594")
     751 |  # node_Mul_751
            %"val_601"<?,?> ⬅️ ::Mul(%"val_594", %"val_600")
     752 |  # node_CastLike_752
            %"val_602"<?,?> ⬅️ ::CastLike(%"val_599", %"val_601")
     753 |  # node_Add_753
            %"group_norm_19"<FLOAT,[1,384,6891]> ⬅️ ::Add(%"val_601", %"val_602")
     754 |  # node_aten_glu_754
            %"glu_13"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_19") {dim=1}
     755 |  # node_Cast_755
            %"val_603"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     756 |  # node_Constant_756
            %"val_604"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     757 |  # node_Reshape_757
            %"val_605"<?,?> ⬅️ ::Reshape(%"val_603", %"val_604") {allowzero=0}
     758 |  # node_Cast_758
            %"val_606"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     759 |  # node_Constant_759
            %"val_607"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     760 |  # node_Reshape_760
            %"val_608"<?,?> ⬅️ ::Reshape(%"val_606", %"val_607") {allowzero=0}
     761 |  # node_Cast_761
            %"val_609"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     762 |  # node_Constant_762
            %"val_610"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     763 |  # node_Reshape_763
            %"val_611"<?,?> ⬅️ ::Reshape(%"val_609", %"val_610") {allowzero=0}
     764 |  # node_Constant_764
            %"val_612"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     765 |  # node_Slice_765
            %"slice_12"<FLOAT,[192]> ⬅️ ::Slice(%"tencoder.2.dconv.layers.1.6.scale", %"val_605", %"val_608", %"val_611", %"val_612")
     766 |  # node_aten_unsqueeze_766
            %"unsqueeze_11"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_12") {dim=1}
     767 |  # node_Mul_767
            %"mul_17"<FLOAT,[1,192,6891]> ⬅️ ::Mul(%"unsqueeze_11", %"glu_13")
     768 |  # node_aten_add_768
            %"add_22"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_21", %"mul_17") {alpha=1.0}
     769 |  # node_Conv_769
            %"convolution_29"<FLOAT,[1,384,6891]> ⬅️ ::Conv(%"add_22", %"tencoder.2.rewrite.weight", %"tencoder.2.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     770 |  # node_aten_glu_770
            %"glu_14"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_29") {dim=1}
     771 |  # node_Conv_771
            %"convolution_30"<FLOAT,[1,192,32,431]> ⬅️ ::Conv(%"glu_11", %"encoder.2.conv.weight", %"encoder.2.conv.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[2, 0, 2, 0], strides=[4, 1]}
     772 |  # node__aten_gelu_approximate_none_772
            %"gelu_15"<FLOAT,[1,192,32,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_30")
     773 |  # node_Transpose_773
            %"permute_5"<FLOAT,[1,32,192,431]> ⬅️ ::Transpose(%"gelu_15") {perm=[0, 2, 1, 3]}
     774 |  # node_Constant_774
            %"val_613"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ 32, 192, 431]), name=None)}
     775 |  # node_Cast_775
            %"val_614"<?,?> ⬅️ ::Cast(%"val_613") {to=7}
     776 |  # node_Reshape_776
            %"view_4"<FLOAT,[32,192,431]> ⬅️ ::Reshape(%"permute_5", %"val_614") {allowzero=0}
     777 |  # node_Conv_777
            %"convolution_31"<FLOAT,[32,24,431]> ⬅️ ::Conv(%"view_4", %"encoder.2.dconv.layers.0.0.weight", %"encoder.2.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
     778 |  # node_Constant_778
            %"val_615"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     779 |  # node_Reshape_779
            %"val_616"<?,?> ⬅️ ::Reshape(%"val_35", %"val_615") {allowzero=0}
     780 |  # node_Constant_780
            %"val_617"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     781 |  # node_Concat_781
            %"val_618"<?,?> ⬅️ ::Concat(%"val_617", %"val_616", %"val_615") {axis=0}
     782 |  # node_Reshape_782
            %"val_619"<?,?> ⬅️ ::Reshape(%"convolution_31", %"val_618") {allowzero=0}
     783 |  # node_Constant_783
            %"val_620"<?,?> ⬅️ ::Constant() {value_float=1.0}
     784 |  # node_CastLike_784
            %"val_621"<?,?> ⬅️ ::CastLike(%"val_620", %"convolution_31")
     785 |  # node_Expand_785
            %"val_622"<?,?> ⬅️ ::Expand(%"val_621", %"val_616")
     786 |  # node_Constant_786
            %"val_623"<?,?> ⬅️ ::Constant() {value_float=0.0}
     787 |  # node_CastLike_787
            %"val_624"<?,?> ⬅️ ::CastLike(%"val_623", %"convolution_31")
     788 |  # node_Expand_788
            %"val_625"<?,?> ⬅️ ::Expand(%"val_624", %"val_616")
     789 |  # node_InstanceNormalization_789
            %"val_626"<?,?> ⬅️ ::InstanceNormalization(%"val_619", %"val_622", %"val_625") {epsilon=1e-05}
     790 |  # node_Shape_790
            %"val_627"<?,?> ⬅️ ::Shape(%"convolution_31") {start=0}
     791 |  # node_Reshape_791
            %"val_628"<?,?> ⬅️ ::Reshape(%"val_626", %"val_627") {allowzero=0}
     792 |  # node_Constant_792
            %"val_629"<?,?> ⬅️ ::Constant() {value_int=1}
     793 |  # node_Sub_793
            %"val_630"<?,?> ⬅️ ::Sub(%"val_50", %"val_629")
     794 |  # node_Range_794
            %"val_631"<?,?> ⬅️ ::Range(%"val_629", %"val_630", %"val_629")
     795 |  # node_Unsqueeze_795
            %"val_632"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.0.1.weight", %"val_631")
     796 |  # node_Unsqueeze_796
            %"val_633"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.0.1.bias", %"val_631")
     797 |  # node_CastLike_797
            %"val_634"<?,?> ⬅️ ::CastLike(%"val_632", %"val_628")
     798 |  # node_Mul_798
            %"val_635"<?,?> ⬅️ ::Mul(%"val_628", %"val_634")
     799 |  # node_CastLike_799
            %"val_636"<?,?> ⬅️ ::CastLike(%"val_633", %"val_635")
     800 |  # node_Add_800
            %"group_norm_20"<FLOAT,[32,24,431]> ⬅️ ::Add(%"val_635", %"val_636")
     801 |  # node__aten_gelu_approximate_none_801
            %"gelu_16"<FLOAT,[32,24,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_20")
     802 |  # node_Conv_802
            %"convolution_32"<FLOAT,[32,384,431]> ⬅️ ::Conv(%"gelu_16", %"encoder.2.dconv.layers.0.3.weight", %"encoder.2.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     803 |  # node_Constant_803
            %"val_637"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     804 |  # node_Reshape_804
            %"val_638"<?,?> ⬅️ ::Reshape(%"val_35", %"val_637") {allowzero=0}
     805 |  # node_Constant_805
            %"val_639"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     806 |  # node_Concat_806
            %"val_640"<?,?> ⬅️ ::Concat(%"val_639", %"val_638", %"val_637") {axis=0}
     807 |  # node_Reshape_807
            %"val_641"<?,?> ⬅️ ::Reshape(%"convolution_32", %"val_640") {allowzero=0}
     808 |  # node_Constant_808
            %"val_642"<?,?> ⬅️ ::Constant() {value_float=1.0}
     809 |  # node_CastLike_809
            %"val_643"<?,?> ⬅️ ::CastLike(%"val_642", %"convolution_32")
     810 |  # node_Expand_810
            %"val_644"<?,?> ⬅️ ::Expand(%"val_643", %"val_638")
     811 |  # node_Constant_811
            %"val_645"<?,?> ⬅️ ::Constant() {value_float=0.0}
     812 |  # node_CastLike_812
            %"val_646"<?,?> ⬅️ ::CastLike(%"val_645", %"convolution_32")
     813 |  # node_Expand_813
            %"val_647"<?,?> ⬅️ ::Expand(%"val_646", %"val_638")
     814 |  # node_InstanceNormalization_814
            %"val_648"<?,?> ⬅️ ::InstanceNormalization(%"val_641", %"val_644", %"val_647") {epsilon=1e-05}
     815 |  # node_Shape_815
            %"val_649"<?,?> ⬅️ ::Shape(%"convolution_32") {start=0}
     816 |  # node_Reshape_816
            %"val_650"<?,?> ⬅️ ::Reshape(%"val_648", %"val_649") {allowzero=0}
     817 |  # node_Constant_817
            %"val_651"<?,?> ⬅️ ::Constant() {value_int=1}
     818 |  # node_Sub_818
            %"val_652"<?,?> ⬅️ ::Sub(%"val_50", %"val_651")
     819 |  # node_Range_819
            %"val_653"<?,?> ⬅️ ::Range(%"val_651", %"val_652", %"val_651")
     820 |  # node_Unsqueeze_820
            %"val_654"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.0.4.weight", %"val_653")
     821 |  # node_Unsqueeze_821
            %"val_655"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.0.4.bias", %"val_653")
     822 |  # node_CastLike_822
            %"val_656"<?,?> ⬅️ ::CastLike(%"val_654", %"val_650")
     823 |  # node_Mul_823
            %"val_657"<?,?> ⬅️ ::Mul(%"val_650", %"val_656")
     824 |  # node_CastLike_824
            %"val_658"<?,?> ⬅️ ::CastLike(%"val_655", %"val_657")
     825 |  # node_Add_825
            %"group_norm_21"<FLOAT,[32,384,431]> ⬅️ ::Add(%"val_657", %"val_658")
     826 |  # node_aten_glu_826
            %"glu_15"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_21") {dim=1}
     827 |  # node_Cast_827
            %"val_659"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     828 |  # node_Constant_828
            %"val_660"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     829 |  # node_Reshape_829
            %"val_661"<?,?> ⬅️ ::Reshape(%"val_659", %"val_660") {allowzero=0}
     830 |  # node_Cast_830
            %"val_662"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     831 |  # node_Constant_831
            %"val_663"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     832 |  # node_Reshape_832
            %"val_664"<?,?> ⬅️ ::Reshape(%"val_662", %"val_663") {allowzero=0}
     833 |  # node_Cast_833
            %"val_665"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     834 |  # node_Constant_834
            %"val_666"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     835 |  # node_Reshape_835
            %"val_667"<?,?> ⬅️ ::Reshape(%"val_665", %"val_666") {allowzero=0}
     836 |  # node_Constant_836
            %"val_668"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     837 |  # node_Slice_837
            %"slice_13"<FLOAT,[192]> ⬅️ ::Slice(%"encoder.2.dconv.layers.0.6.scale", %"val_661", %"val_664", %"val_667", %"val_668")
     838 |  # node_aten_unsqueeze_838
            %"unsqueeze_12"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_13") {dim=1}
     839 |  # node_Mul_839
            %"mul_18"<FLOAT,[32,192,431]> ⬅️ ::Mul(%"unsqueeze_12", %"glu_15")
     840 |  # node_aten_add_840
            %"add_23"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_4", %"mul_18") {alpha=1.0}
     841 |  # node_Conv_841
            %"convolution_33"<FLOAT,[32,24,431]> ⬅️ ::Conv(%"add_23", %"encoder.2.dconv.layers.1.0.weight", %"encoder.2.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     842 |  # node_Constant_842
            %"val_669"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     843 |  # node_Reshape_843
            %"val_670"<?,?> ⬅️ ::Reshape(%"val_35", %"val_669") {allowzero=0}
     844 |  # node_Constant_844
            %"val_671"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     845 |  # node_Concat_845
            %"val_672"<?,?> ⬅️ ::Concat(%"val_671", %"val_670", %"val_669") {axis=0}
     846 |  # node_Reshape_846
            %"val_673"<?,?> ⬅️ ::Reshape(%"convolution_33", %"val_672") {allowzero=0}
     847 |  # node_Constant_847
            %"val_674"<?,?> ⬅️ ::Constant() {value_float=1.0}
     848 |  # node_CastLike_848
            %"val_675"<?,?> ⬅️ ::CastLike(%"val_674", %"convolution_33")
     849 |  # node_Expand_849
            %"val_676"<?,?> ⬅️ ::Expand(%"val_675", %"val_670")
     850 |  # node_Constant_850
            %"val_677"<?,?> ⬅️ ::Constant() {value_float=0.0}
     851 |  # node_CastLike_851
            %"val_678"<?,?> ⬅️ ::CastLike(%"val_677", %"convolution_33")
     852 |  # node_Expand_852
            %"val_679"<?,?> ⬅️ ::Expand(%"val_678", %"val_670")
     853 |  # node_InstanceNormalization_853
            %"val_680"<?,?> ⬅️ ::InstanceNormalization(%"val_673", %"val_676", %"val_679") {epsilon=1e-05}
     854 |  # node_Shape_854
            %"val_681"<?,?> ⬅️ ::Shape(%"convolution_33") {start=0}
     855 |  # node_Reshape_855
            %"val_682"<?,?> ⬅️ ::Reshape(%"val_680", %"val_681") {allowzero=0}
     856 |  # node_Constant_856
            %"val_683"<?,?> ⬅️ ::Constant() {value_int=1}
     857 |  # node_Sub_857
            %"val_684"<?,?> ⬅️ ::Sub(%"val_50", %"val_683")
     858 |  # node_Range_858
            %"val_685"<?,?> ⬅️ ::Range(%"val_683", %"val_684", %"val_683")
     859 |  # node_Unsqueeze_859
            %"val_686"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.1.1.weight", %"val_685")
     860 |  # node_Unsqueeze_860
            %"val_687"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.1.1.bias", %"val_685")
     861 |  # node_CastLike_861
            %"val_688"<?,?> ⬅️ ::CastLike(%"val_686", %"val_682")
     862 |  # node_Mul_862
            %"val_689"<?,?> ⬅️ ::Mul(%"val_682", %"val_688")
     863 |  # node_CastLike_863
            %"val_690"<?,?> ⬅️ ::CastLike(%"val_687", %"val_689")
     864 |  # node_Add_864
            %"group_norm_22"<FLOAT,[32,24,431]> ⬅️ ::Add(%"val_689", %"val_690")
     865 |  # node__aten_gelu_approximate_none_865
            %"gelu_17"<FLOAT,[32,24,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_22")
     866 |  # node_Conv_866
            %"convolution_34"<FLOAT,[32,384,431]> ⬅️ ::Conv(%"gelu_17", %"encoder.2.dconv.layers.1.3.weight", %"encoder.2.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     867 |  # node_Constant_867
            %"val_691"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     868 |  # node_Reshape_868
            %"val_692"<?,?> ⬅️ ::Reshape(%"val_35", %"val_691") {allowzero=0}
     869 |  # node_Constant_869
            %"val_693"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     870 |  # node_Concat_870
            %"val_694"<?,?> ⬅️ ::Concat(%"val_693", %"val_692", %"val_691") {axis=0}
     871 |  # node_Reshape_871
            %"val_695"<?,?> ⬅️ ::Reshape(%"convolution_34", %"val_694") {allowzero=0}
     872 |  # node_Constant_872
            %"val_696"<?,?> ⬅️ ::Constant() {value_float=1.0}
     873 |  # node_CastLike_873
            %"val_697"<?,?> ⬅️ ::CastLike(%"val_696", %"convolution_34")
     874 |  # node_Expand_874
            %"val_698"<?,?> ⬅️ ::Expand(%"val_697", %"val_692")
     875 |  # node_Constant_875
            %"val_699"<?,?> ⬅️ ::Constant() {value_float=0.0}
     876 |  # node_CastLike_876
            %"val_700"<?,?> ⬅️ ::CastLike(%"val_699", %"convolution_34")
     877 |  # node_Expand_877
            %"val_701"<?,?> ⬅️ ::Expand(%"val_700", %"val_692")
     878 |  # node_InstanceNormalization_878
            %"val_702"<?,?> ⬅️ ::InstanceNormalization(%"val_695", %"val_698", %"val_701") {epsilon=1e-05}
     879 |  # node_Shape_879
            %"val_703"<?,?> ⬅️ ::Shape(%"convolution_34") {start=0}
     880 |  # node_Reshape_880
            %"val_704"<?,?> ⬅️ ::Reshape(%"val_702", %"val_703") {allowzero=0}
     881 |  # node_Constant_881
            %"val_705"<?,?> ⬅️ ::Constant() {value_int=1}
     882 |  # node_Sub_882
            %"val_706"<?,?> ⬅️ ::Sub(%"val_50", %"val_705")
     883 |  # node_Range_883
            %"val_707"<?,?> ⬅️ ::Range(%"val_705", %"val_706", %"val_705")
     884 |  # node_Unsqueeze_884
            %"val_708"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.1.4.weight", %"val_707")
     885 |  # node_Unsqueeze_885
            %"val_709"<?,?> ⬅️ ::Unsqueeze(%"encoder.2.dconv.layers.1.4.bias", %"val_707")
     886 |  # node_CastLike_886
            %"val_710"<?,?> ⬅️ ::CastLike(%"val_708", %"val_704")
     887 |  # node_Mul_887
            %"val_711"<?,?> ⬅️ ::Mul(%"val_704", %"val_710")
     888 |  # node_CastLike_888
            %"val_712"<?,?> ⬅️ ::CastLike(%"val_709", %"val_711")
     889 |  # node_Add_889
            %"group_norm_23"<FLOAT,[32,384,431]> ⬅️ ::Add(%"val_711", %"val_712")
     890 |  # node_aten_glu_890
            %"glu_16"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_23") {dim=1}
     891 |  # node_Cast_891
            %"val_713"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     892 |  # node_Constant_892
            %"val_714"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     893 |  # node_Reshape_893
            %"val_715"<?,?> ⬅️ ::Reshape(%"val_713", %"val_714") {allowzero=0}
     894 |  # node_Cast_894
            %"val_716"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     895 |  # node_Constant_895
            %"val_717"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     896 |  # node_Reshape_896
            %"val_718"<?,?> ⬅️ ::Reshape(%"val_716", %"val_717") {allowzero=0}
     897 |  # node_Cast_897
            %"val_719"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     898 |  # node_Constant_898
            %"val_720"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     899 |  # node_Reshape_899
            %"val_721"<?,?> ⬅️ ::Reshape(%"val_719", %"val_720") {allowzero=0}
     900 |  # node_Constant_900
            %"val_722"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     901 |  # node_Slice_901
            %"slice_14"<FLOAT,[192]> ⬅️ ::Slice(%"encoder.2.dconv.layers.1.6.scale", %"val_715", %"val_718", %"val_721", %"val_722")
     902 |  # node_aten_unsqueeze_902
            %"unsqueeze_13"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_14") {dim=1}
     903 |  # node_Mul_903
            %"mul_19"<FLOAT,[32,192,431]> ⬅️ ::Mul(%"unsqueeze_13", %"glu_16")
     904 |  # node_aten_add_904
            %"add_24"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_23", %"mul_19") {alpha=1.0}
     905 |  # node_Constant_905
            %"val_723"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,  32, 192, 431]), name=None)}
     906 |  # node_Cast_906
            %"val_724"<?,?> ⬅️ ::Cast(%"val_723") {to=7}
     907 |  # node_Reshape_907
            %"view_5"<FLOAT,[1,32,192,431]> ⬅️ ::Reshape(%"add_24", %"val_724") {allowzero=0}
     908 |  # node_Transpose_908
            %"permute_6"<FLOAT,[1,192,32,431]> ⬅️ ::Transpose(%"view_5") {perm=[0, 2, 1, 3]}
     909 |  # node_Conv_909
            %"convolution_35"<FLOAT,[1,384,32,431]> ⬅️ ::Conv(%"permute_6", %"encoder.2.rewrite.weight", %"encoder.2.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[0, 0, 0, 0], strides=[1, 1]}
     910 |  # node_aten_glu_910
            %"glu_17"<FLOAT,[1,192,32,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_35") {dim=1}
     911 |  # node_aten_constant_pad_nd_911
            %"constant_pad_nd_2"<FLOAT,[1,192,6892]> ⬅️ pkg.onnxscript.torch_lib::aten_constant_pad_nd(%"glu_14", %"val_504") {value=0.0}
     912 |  # node_Conv_912
            %"convolution_36"<FLOAT,[1,384,1723]> ⬅️ ::Conv(%"constant_pad_nd_2", %"tencoder.3.conv.weight", %"tencoder.3.conv.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[2, 2], strides=[4]}
     913 |  # node__aten_gelu_approximate_none_913
            %"gelu_18"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_36")
     914 |  # node_Conv_914
            %"convolution_37"<FLOAT,[1,48,1723]> ⬅️ ::Conv(%"gelu_18", %"tencoder.3.dconv.layers.0.0.weight", %"tencoder.3.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
     915 |  # node_Constant_915
            %"val_725"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     916 |  # node_Reshape_916
            %"val_726"<?,?> ⬅️ ::Reshape(%"val_35", %"val_725") {allowzero=0}
     917 |  # node_Constant_917
            %"val_727"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     918 |  # node_Concat_918
            %"val_728"<?,?> ⬅️ ::Concat(%"val_727", %"val_726", %"val_725") {axis=0}
     919 |  # node_Reshape_919
            %"val_729"<?,?> ⬅️ ::Reshape(%"convolution_37", %"val_728") {allowzero=0}
     920 |  # node_Constant_920
            %"val_730"<?,?> ⬅️ ::Constant() {value_float=1.0}
     921 |  # node_CastLike_921
            %"val_731"<?,?> ⬅️ ::CastLike(%"val_730", %"convolution_37")
     922 |  # node_Expand_922
            %"val_732"<?,?> ⬅️ ::Expand(%"val_731", %"val_726")
     923 |  # node_Constant_923
            %"val_733"<?,?> ⬅️ ::Constant() {value_float=0.0}
     924 |  # node_CastLike_924
            %"val_734"<?,?> ⬅️ ::CastLike(%"val_733", %"convolution_37")
     925 |  # node_Expand_925
            %"val_735"<?,?> ⬅️ ::Expand(%"val_734", %"val_726")
     926 |  # node_InstanceNormalization_926
            %"val_736"<?,?> ⬅️ ::InstanceNormalization(%"val_729", %"val_732", %"val_735") {epsilon=1e-05}
     927 |  # node_Shape_927
            %"val_737"<?,?> ⬅️ ::Shape(%"convolution_37") {start=0}
     928 |  # node_Reshape_928
            %"val_738"<?,?> ⬅️ ::Reshape(%"val_736", %"val_737") {allowzero=0}
     929 |  # node_Constant_929
            %"val_739"<?,?> ⬅️ ::Constant() {value_int=1}
     930 |  # node_Sub_930
            %"val_740"<?,?> ⬅️ ::Sub(%"val_50", %"val_739")
     931 |  # node_Range_931
            %"val_741"<?,?> ⬅️ ::Range(%"val_739", %"val_740", %"val_739")
     932 |  # node_Unsqueeze_932
            %"val_742"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.0.1.weight", %"val_741")
     933 |  # node_Unsqueeze_933
            %"val_743"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.0.1.bias", %"val_741")
     934 |  # node_CastLike_934
            %"val_744"<?,?> ⬅️ ::CastLike(%"val_742", %"val_738")
     935 |  # node_Mul_935
            %"val_745"<?,?> ⬅️ ::Mul(%"val_738", %"val_744")
     936 |  # node_CastLike_936
            %"val_746"<?,?> ⬅️ ::CastLike(%"val_743", %"val_745")
     937 |  # node_Add_937
            %"group_norm_24"<FLOAT,[1,48,1723]> ⬅️ ::Add(%"val_745", %"val_746")
     938 |  # node__aten_gelu_approximate_none_938
            %"gelu_19"<FLOAT,[1,48,1723]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_24")
     939 |  # node_Conv_939
            %"convolution_38"<FLOAT,[1,768,1723]> ⬅️ ::Conv(%"gelu_19", %"tencoder.3.dconv.layers.0.3.weight", %"tencoder.3.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
     940 |  # node_Constant_940
            %"val_747"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     941 |  # node_Reshape_941
            %"val_748"<?,?> ⬅️ ::Reshape(%"val_35", %"val_747") {allowzero=0}
     942 |  # node_Constant_942
            %"val_749"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     943 |  # node_Concat_943
            %"val_750"<?,?> ⬅️ ::Concat(%"val_749", %"val_748", %"val_747") {axis=0}
     944 |  # node_Reshape_944
            %"val_751"<?,?> ⬅️ ::Reshape(%"convolution_38", %"val_750") {allowzero=0}
     945 |  # node_Constant_945
            %"val_752"<?,?> ⬅️ ::Constant() {value_float=1.0}
     946 |  # node_CastLike_946
            %"val_753"<?,?> ⬅️ ::CastLike(%"val_752", %"convolution_38")
     947 |  # node_Expand_947
            %"val_754"<?,?> ⬅️ ::Expand(%"val_753", %"val_748")
     948 |  # node_Constant_948
            %"val_755"<?,?> ⬅️ ::Constant() {value_float=0.0}
     949 |  # node_CastLike_949
            %"val_756"<?,?> ⬅️ ::CastLike(%"val_755", %"convolution_38")
     950 |  # node_Expand_950
            %"val_757"<?,?> ⬅️ ::Expand(%"val_756", %"val_748")
     951 |  # node_InstanceNormalization_951
            %"val_758"<?,?> ⬅️ ::InstanceNormalization(%"val_751", %"val_754", %"val_757") {epsilon=1e-05}
     952 |  # node_Shape_952
            %"val_759"<?,?> ⬅️ ::Shape(%"convolution_38") {start=0}
     953 |  # node_Reshape_953
            %"val_760"<?,?> ⬅️ ::Reshape(%"val_758", %"val_759") {allowzero=0}
     954 |  # node_Constant_954
            %"val_761"<?,?> ⬅️ ::Constant() {value_int=1}
     955 |  # node_Sub_955
            %"val_762"<?,?> ⬅️ ::Sub(%"val_50", %"val_761")
     956 |  # node_Range_956
            %"val_763"<?,?> ⬅️ ::Range(%"val_761", %"val_762", %"val_761")
     957 |  # node_Unsqueeze_957
            %"val_764"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.0.4.weight", %"val_763")
     958 |  # node_Unsqueeze_958
            %"val_765"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.0.4.bias", %"val_763")
     959 |  # node_CastLike_959
            %"val_766"<?,?> ⬅️ ::CastLike(%"val_764", %"val_760")
     960 |  # node_Mul_960
            %"val_767"<?,?> ⬅️ ::Mul(%"val_760", %"val_766")
     961 |  # node_CastLike_961
            %"val_768"<?,?> ⬅️ ::CastLike(%"val_765", %"val_767")
     962 |  # node_Add_962
            %"group_norm_25"<FLOAT,[1,768,1723]> ⬅️ ::Add(%"val_767", %"val_768")
     963 |  # node_aten_glu_963
            %"glu_18"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_25") {dim=1}
     964 |  # node_Cast_964
            %"val_769"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     965 |  # node_Constant_965
            %"val_770"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     966 |  # node_Reshape_966
            %"val_771"<?,?> ⬅️ ::Reshape(%"val_769", %"val_770") {allowzero=0}
     967 |  # node_Cast_967
            %"val_772"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
     968 |  # node_Constant_968
            %"val_773"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     969 |  # node_Reshape_969
            %"val_774"<?,?> ⬅️ ::Reshape(%"val_772", %"val_773") {allowzero=0}
     970 |  # node_Cast_970
            %"val_775"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
     971 |  # node_Constant_971
            %"val_776"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     972 |  # node_Reshape_972
            %"val_777"<?,?> ⬅️ ::Reshape(%"val_775", %"val_776") {allowzero=0}
     973 |  # node_Constant_973
            %"val_778"<?,?> ⬅️ ::Constant() {value_ints=[1]}
     974 |  # node_Slice_974
            %"slice_15"<FLOAT,[384]> ⬅️ ::Slice(%"tencoder.3.dconv.layers.0.6.scale", %"val_771", %"val_774", %"val_777", %"val_778")
     975 |  # node_aten_unsqueeze_975
            %"unsqueeze_14"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_15") {dim=1}
     976 |  # node_Mul_976
            %"mul_20"<FLOAT,[1,384,1723]> ⬅️ ::Mul(%"unsqueeze_14", %"glu_18")
     977 |  # node_aten_add_977
            %"add_25"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_18", %"mul_20") {alpha=1.0}
     978 |  # node_Conv_978
            %"convolution_39"<FLOAT,[1,48,1723]> ⬅️ ::Conv(%"add_25", %"tencoder.3.dconv.layers.1.0.weight", %"tencoder.3.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
     979 |  # node_Constant_979
            %"val_779"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     980 |  # node_Reshape_980
            %"val_780"<?,?> ⬅️ ::Reshape(%"val_35", %"val_779") {allowzero=0}
     981 |  # node_Constant_981
            %"val_781"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     982 |  # node_Concat_982
            %"val_782"<?,?> ⬅️ ::Concat(%"val_781", %"val_780", %"val_779") {axis=0}
     983 |  # node_Reshape_983
            %"val_783"<?,?> ⬅️ ::Reshape(%"convolution_39", %"val_782") {allowzero=0}
     984 |  # node_Constant_984
            %"val_784"<?,?> ⬅️ ::Constant() {value_float=1.0}
     985 |  # node_CastLike_985
            %"val_785"<?,?> ⬅️ ::CastLike(%"val_784", %"convolution_39")
     986 |  # node_Expand_986
            %"val_786"<?,?> ⬅️ ::Expand(%"val_785", %"val_780")
     987 |  # node_Constant_987
            %"val_787"<?,?> ⬅️ ::Constant() {value_float=0.0}
     988 |  # node_CastLike_988
            %"val_788"<?,?> ⬅️ ::CastLike(%"val_787", %"convolution_39")
     989 |  # node_Expand_989
            %"val_789"<?,?> ⬅️ ::Expand(%"val_788", %"val_780")
     990 |  # node_InstanceNormalization_990
            %"val_790"<?,?> ⬅️ ::InstanceNormalization(%"val_783", %"val_786", %"val_789") {epsilon=1e-05}
     991 |  # node_Shape_991
            %"val_791"<?,?> ⬅️ ::Shape(%"convolution_39") {start=0}
     992 |  # node_Reshape_992
            %"val_792"<?,?> ⬅️ ::Reshape(%"val_790", %"val_791") {allowzero=0}
     993 |  # node_Constant_993
            %"val_793"<?,?> ⬅️ ::Constant() {value_int=1}
     994 |  # node_Sub_994
            %"val_794"<?,?> ⬅️ ::Sub(%"val_50", %"val_793")
     995 |  # node_Range_995
            %"val_795"<?,?> ⬅️ ::Range(%"val_793", %"val_794", %"val_793")
     996 |  # node_Unsqueeze_996
            %"val_796"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.1.1.weight", %"val_795")
     997 |  # node_Unsqueeze_997
            %"val_797"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.1.1.bias", %"val_795")
     998 |  # node_CastLike_998
            %"val_798"<?,?> ⬅️ ::CastLike(%"val_796", %"val_792")
     999 |  # node_Mul_999
            %"val_799"<?,?> ⬅️ ::Mul(%"val_792", %"val_798")
    1000 |  # node_CastLike_1000
            %"val_800"<?,?> ⬅️ ::CastLike(%"val_797", %"val_799")
    1001 |  # node_Add_1001
            %"group_norm_26"<FLOAT,[1,48,1723]> ⬅️ ::Add(%"val_799", %"val_800")
    1002 |  # node__aten_gelu_approximate_none_1002
            %"gelu_20"<FLOAT,[1,48,1723]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_26")
    1003 |  # node_Conv_1003
            %"convolution_40"<FLOAT,[1,768,1723]> ⬅️ ::Conv(%"gelu_20", %"tencoder.3.dconv.layers.1.3.weight", %"tencoder.3.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    1004 |  # node_Constant_1004
            %"val_801"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1005 |  # node_Reshape_1005
            %"val_802"<?,?> ⬅️ ::Reshape(%"val_35", %"val_801") {allowzero=0}
    1006 |  # node_Constant_1006
            %"val_803"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1007 |  # node_Concat_1007
            %"val_804"<?,?> ⬅️ ::Concat(%"val_803", %"val_802", %"val_801") {axis=0}
    1008 |  # node_Reshape_1008
            %"val_805"<?,?> ⬅️ ::Reshape(%"convolution_40", %"val_804") {allowzero=0}
    1009 |  # node_Constant_1009
            %"val_806"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1010 |  # node_CastLike_1010
            %"val_807"<?,?> ⬅️ ::CastLike(%"val_806", %"convolution_40")
    1011 |  # node_Expand_1011
            %"val_808"<?,?> ⬅️ ::Expand(%"val_807", %"val_802")
    1012 |  # node_Constant_1012
            %"val_809"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1013 |  # node_CastLike_1013
            %"val_810"<?,?> ⬅️ ::CastLike(%"val_809", %"convolution_40")
    1014 |  # node_Expand_1014
            %"val_811"<?,?> ⬅️ ::Expand(%"val_810", %"val_802")
    1015 |  # node_InstanceNormalization_1015
            %"val_812"<?,?> ⬅️ ::InstanceNormalization(%"val_805", %"val_808", %"val_811") {epsilon=1e-05}
    1016 |  # node_Shape_1016
            %"val_813"<?,?> ⬅️ ::Shape(%"convolution_40") {start=0}
    1017 |  # node_Reshape_1017
            %"val_814"<?,?> ⬅️ ::Reshape(%"val_812", %"val_813") {allowzero=0}
    1018 |  # node_Constant_1018
            %"val_815"<?,?> ⬅️ ::Constant() {value_int=1}
    1019 |  # node_Sub_1019
            %"val_816"<?,?> ⬅️ ::Sub(%"val_50", %"val_815")
    1020 |  # node_Range_1020
            %"val_817"<?,?> ⬅️ ::Range(%"val_815", %"val_816", %"val_815")
    1021 |  # node_Unsqueeze_1021
            %"val_818"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.1.4.weight", %"val_817")
    1022 |  # node_Unsqueeze_1022
            %"val_819"<?,?> ⬅️ ::Unsqueeze(%"tencoder.3.dconv.layers.1.4.bias", %"val_817")
    1023 |  # node_CastLike_1023
            %"val_820"<?,?> ⬅️ ::CastLike(%"val_818", %"val_814")
    1024 |  # node_Mul_1024
            %"val_821"<?,?> ⬅️ ::Mul(%"val_814", %"val_820")
    1025 |  # node_CastLike_1025
            %"val_822"<?,?> ⬅️ ::CastLike(%"val_819", %"val_821")
    1026 |  # node_Add_1026
            %"group_norm_27"<FLOAT,[1,768,1723]> ⬅️ ::Add(%"val_821", %"val_822")
    1027 |  # node_aten_glu_1027
            %"glu_19"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_27") {dim=1}
    1028 |  # node_Cast_1028
            %"val_823"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1029 |  # node_Constant_1029
            %"val_824"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1030 |  # node_Reshape_1030
            %"val_825"<?,?> ⬅️ ::Reshape(%"val_823", %"val_824") {allowzero=0}
    1031 |  # node_Cast_1031
            %"val_826"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1032 |  # node_Constant_1032
            %"val_827"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1033 |  # node_Reshape_1033
            %"val_828"<?,?> ⬅️ ::Reshape(%"val_826", %"val_827") {allowzero=0}
    1034 |  # node_Cast_1034
            %"val_829"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1035 |  # node_Constant_1035
            %"val_830"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1036 |  # node_Reshape_1036
            %"val_831"<?,?> ⬅️ ::Reshape(%"val_829", %"val_830") {allowzero=0}
    1037 |  # node_Constant_1037
            %"val_832"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1038 |  # node_Slice_1038
            %"slice_16"<FLOAT,[384]> ⬅️ ::Slice(%"tencoder.3.dconv.layers.1.6.scale", %"val_825", %"val_828", %"val_831", %"val_832")
    1039 |  # node_aten_unsqueeze_1039
            %"unsqueeze_15"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_16") {dim=1}
    1040 |  # node_Mul_1040
            %"mul_21"<FLOAT,[1,384,1723]> ⬅️ ::Mul(%"unsqueeze_15", %"glu_19")
    1041 |  # node_aten_add_1041
            %"add_26"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_25", %"mul_21") {alpha=1.0}
    1042 |  # node_Conv_1042
            %"convolution_41"<FLOAT,[1,768,1723]> ⬅️ ::Conv(%"add_26", %"tencoder.3.rewrite.weight", %"tencoder.3.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    1043 |  # node_aten_glu_1043
            %"glu_20"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_41") {dim=1}
    1044 |  # node_Conv_1044
            %"convolution_42"<FLOAT,[1,384,8,431]> ⬅️ ::Conv(%"glu_17", %"encoder.3.conv.weight", %"encoder.3.conv.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[2, 0, 2, 0], strides=[4, 1]}
    1045 |  # node__aten_gelu_approximate_none_1045
            %"gelu_21"<FLOAT,[1,384,8,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"convolution_42")
    1046 |  # node_Transpose_1046
            %"permute_7"<FLOAT,[1,8,384,431]> ⬅️ ::Transpose(%"gelu_21") {perm=[0, 2, 1, 3]}
    1047 |  # node_Constant_1047
            %"val_833"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  8, 384, 431]), name=None)}
    1048 |  # node_Cast_1048
            %"val_834"<?,?> ⬅️ ::Cast(%"val_833") {to=7}
    1049 |  # node_Reshape_1049
            %"view_6"<FLOAT,[8,384,431]> ⬅️ ::Reshape(%"permute_7", %"val_834") {allowzero=0}
    1050 |  # node_Conv_1050
            %"convolution_43"<FLOAT,[8,48,431]> ⬅️ ::Conv(%"view_6", %"encoder.3.dconv.layers.0.0.weight", %"encoder.3.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    1051 |  # node_Constant_1051
            %"val_835"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1052 |  # node_Reshape_1052
            %"val_836"<?,?> ⬅️ ::Reshape(%"val_35", %"val_835") {allowzero=0}
    1053 |  # node_Constant_1053
            %"val_837"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1054 |  # node_Concat_1054
            %"val_838"<?,?> ⬅️ ::Concat(%"val_837", %"val_836", %"val_835") {axis=0}
    1055 |  # node_Reshape_1055
            %"val_839"<?,?> ⬅️ ::Reshape(%"convolution_43", %"val_838") {allowzero=0}
    1056 |  # node_Constant_1056
            %"val_840"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1057 |  # node_CastLike_1057
            %"val_841"<?,?> ⬅️ ::CastLike(%"val_840", %"convolution_43")
    1058 |  # node_Expand_1058
            %"val_842"<?,?> ⬅️ ::Expand(%"val_841", %"val_836")
    1059 |  # node_Constant_1059
            %"val_843"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1060 |  # node_CastLike_1060
            %"val_844"<?,?> ⬅️ ::CastLike(%"val_843", %"convolution_43")
    1061 |  # node_Expand_1061
            %"val_845"<?,?> ⬅️ ::Expand(%"val_844", %"val_836")
    1062 |  # node_InstanceNormalization_1062
            %"val_846"<?,?> ⬅️ ::InstanceNormalization(%"val_839", %"val_842", %"val_845") {epsilon=1e-05}
    1063 |  # node_Shape_1063
            %"val_847"<?,?> ⬅️ ::Shape(%"convolution_43") {start=0}
    1064 |  # node_Reshape_1064
            %"val_848"<?,?> ⬅️ ::Reshape(%"val_846", %"val_847") {allowzero=0}
    1065 |  # node_Constant_1065
            %"val_849"<?,?> ⬅️ ::Constant() {value_int=1}
    1066 |  # node_Sub_1066
            %"val_850"<?,?> ⬅️ ::Sub(%"val_50", %"val_849")
    1067 |  # node_Range_1067
            %"val_851"<?,?> ⬅️ ::Range(%"val_849", %"val_850", %"val_849")
    1068 |  # node_Unsqueeze_1068
            %"val_852"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.0.1.weight", %"val_851")
    1069 |  # node_Unsqueeze_1069
            %"val_853"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.0.1.bias", %"val_851")
    1070 |  # node_CastLike_1070
            %"val_854"<?,?> ⬅️ ::CastLike(%"val_852", %"val_848")
    1071 |  # node_Mul_1071
            %"val_855"<?,?> ⬅️ ::Mul(%"val_848", %"val_854")
    1072 |  # node_CastLike_1072
            %"val_856"<?,?> ⬅️ ::CastLike(%"val_853", %"val_855")
    1073 |  # node_Add_1073
            %"group_norm_28"<FLOAT,[8,48,431]> ⬅️ ::Add(%"val_855", %"val_856")
    1074 |  # node__aten_gelu_approximate_none_1074
            %"gelu_22"<FLOAT,[8,48,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_28")
    1075 |  # node_Conv_1075
            %"convolution_44"<FLOAT,[8,768,431]> ⬅️ ::Conv(%"gelu_22", %"encoder.3.dconv.layers.0.3.weight", %"encoder.3.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    1076 |  # node_Constant_1076
            %"val_857"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1077 |  # node_Reshape_1077
            %"val_858"<?,?> ⬅️ ::Reshape(%"val_35", %"val_857") {allowzero=0}
    1078 |  # node_Constant_1078
            %"val_859"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1079 |  # node_Concat_1079
            %"val_860"<?,?> ⬅️ ::Concat(%"val_859", %"val_858", %"val_857") {axis=0}
    1080 |  # node_Reshape_1080
            %"val_861"<?,?> ⬅️ ::Reshape(%"convolution_44", %"val_860") {allowzero=0}
    1081 |  # node_Constant_1081
            %"val_862"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1082 |  # node_CastLike_1082
            %"val_863"<?,?> ⬅️ ::CastLike(%"val_862", %"convolution_44")
    1083 |  # node_Expand_1083
            %"val_864"<?,?> ⬅️ ::Expand(%"val_863", %"val_858")
    1084 |  # node_Constant_1084
            %"val_865"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1085 |  # node_CastLike_1085
            %"val_866"<?,?> ⬅️ ::CastLike(%"val_865", %"convolution_44")
    1086 |  # node_Expand_1086
            %"val_867"<?,?> ⬅️ ::Expand(%"val_866", %"val_858")
    1087 |  # node_InstanceNormalization_1087
            %"val_868"<?,?> ⬅️ ::InstanceNormalization(%"val_861", %"val_864", %"val_867") {epsilon=1e-05}
    1088 |  # node_Shape_1088
            %"val_869"<?,?> ⬅️ ::Shape(%"convolution_44") {start=0}
    1089 |  # node_Reshape_1089
            %"val_870"<?,?> ⬅️ ::Reshape(%"val_868", %"val_869") {allowzero=0}
    1090 |  # node_Constant_1090
            %"val_871"<?,?> ⬅️ ::Constant() {value_int=1}
    1091 |  # node_Sub_1091
            %"val_872"<?,?> ⬅️ ::Sub(%"val_50", %"val_871")
    1092 |  # node_Range_1092
            %"val_873"<?,?> ⬅️ ::Range(%"val_871", %"val_872", %"val_871")
    1093 |  # node_Unsqueeze_1093
            %"val_874"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.0.4.weight", %"val_873")
    1094 |  # node_Unsqueeze_1094
            %"val_875"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.0.4.bias", %"val_873")
    1095 |  # node_CastLike_1095
            %"val_876"<?,?> ⬅️ ::CastLike(%"val_874", %"val_870")
    1096 |  # node_Mul_1096
            %"val_877"<?,?> ⬅️ ::Mul(%"val_870", %"val_876")
    1097 |  # node_CastLike_1097
            %"val_878"<?,?> ⬅️ ::CastLike(%"val_875", %"val_877")
    1098 |  # node_Add_1098
            %"group_norm_29"<FLOAT,[8,768,431]> ⬅️ ::Add(%"val_877", %"val_878")
    1099 |  # node_aten_glu_1099
            %"glu_21"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_29") {dim=1}
    1100 |  # node_Cast_1100
            %"val_879"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1101 |  # node_Constant_1101
            %"val_880"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1102 |  # node_Reshape_1102
            %"val_881"<?,?> ⬅️ ::Reshape(%"val_879", %"val_880") {allowzero=0}
    1103 |  # node_Cast_1103
            %"val_882"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1104 |  # node_Constant_1104
            %"val_883"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1105 |  # node_Reshape_1105
            %"val_884"<?,?> ⬅️ ::Reshape(%"val_882", %"val_883") {allowzero=0}
    1106 |  # node_Cast_1106
            %"val_885"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1107 |  # node_Constant_1107
            %"val_886"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1108 |  # node_Reshape_1108
            %"val_887"<?,?> ⬅️ ::Reshape(%"val_885", %"val_886") {allowzero=0}
    1109 |  # node_Constant_1109
            %"val_888"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1110 |  # node_Slice_1110
            %"slice_17"<FLOAT,[384]> ⬅️ ::Slice(%"encoder.3.dconv.layers.0.6.scale", %"val_881", %"val_884", %"val_887", %"val_888")
    1111 |  # node_aten_unsqueeze_1111
            %"unsqueeze_16"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_17") {dim=1}
    1112 |  # node_Mul_1112
            %"mul_22"<FLOAT,[8,384,431]> ⬅️ ::Mul(%"unsqueeze_16", %"glu_21")
    1113 |  # node_aten_add_1113
            %"add_27"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_6", %"mul_22") {alpha=1.0}
    1114 |  # node_Conv_1114
            %"convolution_45"<FLOAT,[8,48,431]> ⬅️ ::Conv(%"add_27", %"encoder.3.dconv.layers.1.0.weight", %"encoder.3.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    1115 |  # node_Constant_1115
            %"val_889"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1116 |  # node_Reshape_1116
            %"val_890"<?,?> ⬅️ ::Reshape(%"val_35", %"val_889") {allowzero=0}
    1117 |  # node_Constant_1117
            %"val_891"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1118 |  # node_Concat_1118
            %"val_892"<?,?> ⬅️ ::Concat(%"val_891", %"val_890", %"val_889") {axis=0}
    1119 |  # node_Reshape_1119
            %"val_893"<?,?> ⬅️ ::Reshape(%"convolution_45", %"val_892") {allowzero=0}
    1120 |  # node_Constant_1120
            %"val_894"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1121 |  # node_CastLike_1121
            %"val_895"<?,?> ⬅️ ::CastLike(%"val_894", %"convolution_45")
    1122 |  # node_Expand_1122
            %"val_896"<?,?> ⬅️ ::Expand(%"val_895", %"val_890")
    1123 |  # node_Constant_1123
            %"val_897"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1124 |  # node_CastLike_1124
            %"val_898"<?,?> ⬅️ ::CastLike(%"val_897", %"convolution_45")
    1125 |  # node_Expand_1125
            %"val_899"<?,?> ⬅️ ::Expand(%"val_898", %"val_890")
    1126 |  # node_InstanceNormalization_1126
            %"val_900"<?,?> ⬅️ ::InstanceNormalization(%"val_893", %"val_896", %"val_899") {epsilon=1e-05}
    1127 |  # node_Shape_1127
            %"val_901"<?,?> ⬅️ ::Shape(%"convolution_45") {start=0}
    1128 |  # node_Reshape_1128
            %"val_902"<?,?> ⬅️ ::Reshape(%"val_900", %"val_901") {allowzero=0}
    1129 |  # node_Constant_1129
            %"val_903"<?,?> ⬅️ ::Constant() {value_int=1}
    1130 |  # node_Sub_1130
            %"val_904"<?,?> ⬅️ ::Sub(%"val_50", %"val_903")
    1131 |  # node_Range_1131
            %"val_905"<?,?> ⬅️ ::Range(%"val_903", %"val_904", %"val_903")
    1132 |  # node_Unsqueeze_1132
            %"val_906"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.1.1.weight", %"val_905")
    1133 |  # node_Unsqueeze_1133
            %"val_907"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.1.1.bias", %"val_905")
    1134 |  # node_CastLike_1134
            %"val_908"<?,?> ⬅️ ::CastLike(%"val_906", %"val_902")
    1135 |  # node_Mul_1135
            %"val_909"<?,?> ⬅️ ::Mul(%"val_902", %"val_908")
    1136 |  # node_CastLike_1136
            %"val_910"<?,?> ⬅️ ::CastLike(%"val_907", %"val_909")
    1137 |  # node_Add_1137
            %"group_norm_30"<FLOAT,[8,48,431]> ⬅️ ::Add(%"val_909", %"val_910")
    1138 |  # node__aten_gelu_approximate_none_1138
            %"gelu_23"<FLOAT,[8,48,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_30")
    1139 |  # node_Conv_1139
            %"convolution_46"<FLOAT,[8,768,431]> ⬅️ ::Conv(%"gelu_23", %"encoder.3.dconv.layers.1.3.weight", %"encoder.3.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    1140 |  # node_Constant_1140
            %"val_911"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1141 |  # node_Reshape_1141
            %"val_912"<?,?> ⬅️ ::Reshape(%"val_35", %"val_911") {allowzero=0}
    1142 |  # node_Constant_1142
            %"val_913"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1143 |  # node_Concat_1143
            %"val_914"<?,?> ⬅️ ::Concat(%"val_913", %"val_912", %"val_911") {axis=0}
    1144 |  # node_Reshape_1144
            %"val_915"<?,?> ⬅️ ::Reshape(%"convolution_46", %"val_914") {allowzero=0}
    1145 |  # node_Constant_1145
            %"val_916"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1146 |  # node_CastLike_1146
            %"val_917"<?,?> ⬅️ ::CastLike(%"val_916", %"convolution_46")
    1147 |  # node_Expand_1147
            %"val_918"<?,?> ⬅️ ::Expand(%"val_917", %"val_912")
    1148 |  # node_Constant_1148
            %"val_919"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1149 |  # node_CastLike_1149
            %"val_920"<?,?> ⬅️ ::CastLike(%"val_919", %"convolution_46")
    1150 |  # node_Expand_1150
            %"val_921"<?,?> ⬅️ ::Expand(%"val_920", %"val_912")
    1151 |  # node_InstanceNormalization_1151
            %"val_922"<?,?> ⬅️ ::InstanceNormalization(%"val_915", %"val_918", %"val_921") {epsilon=1e-05}
    1152 |  # node_Shape_1152
            %"val_923"<?,?> ⬅️ ::Shape(%"convolution_46") {start=0}
    1153 |  # node_Reshape_1153
            %"val_924"<?,?> ⬅️ ::Reshape(%"val_922", %"val_923") {allowzero=0}
    1154 |  # node_Constant_1154
            %"val_925"<?,?> ⬅️ ::Constant() {value_int=1}
    1155 |  # node_Sub_1155
            %"val_926"<?,?> ⬅️ ::Sub(%"val_50", %"val_925")
    1156 |  # node_Range_1156
            %"val_927"<?,?> ⬅️ ::Range(%"val_925", %"val_926", %"val_925")
    1157 |  # node_Unsqueeze_1157
            %"val_928"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.1.4.weight", %"val_927")
    1158 |  # node_Unsqueeze_1158
            %"val_929"<?,?> ⬅️ ::Unsqueeze(%"encoder.3.dconv.layers.1.4.bias", %"val_927")
    1159 |  # node_CastLike_1159
            %"val_930"<?,?> ⬅️ ::CastLike(%"val_928", %"val_924")
    1160 |  # node_Mul_1160
            %"val_931"<?,?> ⬅️ ::Mul(%"val_924", %"val_930")
    1161 |  # node_CastLike_1161
            %"val_932"<?,?> ⬅️ ::CastLike(%"val_929", %"val_931")
    1162 |  # node_Add_1162
            %"group_norm_31"<FLOAT,[8,768,431]> ⬅️ ::Add(%"val_931", %"val_932")
    1163 |  # node_aten_glu_1163
            %"glu_22"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_31") {dim=1}
    1164 |  # node_Cast_1164
            %"val_933"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1165 |  # node_Constant_1165
            %"val_934"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1166 |  # node_Reshape_1166
            %"val_935"<?,?> ⬅️ ::Reshape(%"val_933", %"val_934") {allowzero=0}
    1167 |  # node_Cast_1167
            %"val_936"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1168 |  # node_Constant_1168
            %"val_937"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1169 |  # node_Reshape_1169
            %"val_938"<?,?> ⬅️ ::Reshape(%"val_936", %"val_937") {allowzero=0}
    1170 |  # node_Cast_1170
            %"val_939"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1171 |  # node_Constant_1171
            %"val_940"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1172 |  # node_Reshape_1172
            %"val_941"<?,?> ⬅️ ::Reshape(%"val_939", %"val_940") {allowzero=0}
    1173 |  # node_Constant_1173
            %"val_942"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1174 |  # node_Slice_1174
            %"slice_18"<FLOAT,[384]> ⬅️ ::Slice(%"encoder.3.dconv.layers.1.6.scale", %"val_935", %"val_938", %"val_941", %"val_942")
    1175 |  # node_aten_unsqueeze_1175
            %"unsqueeze_17"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_18") {dim=1}
    1176 |  # node_Mul_1176
            %"mul_23"<FLOAT,[8,384,431]> ⬅️ ::Mul(%"unsqueeze_17", %"glu_22")
    1177 |  # node_aten_add_1177
            %"add_28"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_27", %"mul_23") {alpha=1.0}
    1178 |  # node_Constant_1178
            %"val_943"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1,   8, 384, 431]), name=None)}
    1179 |  # node_Cast_1179
            %"val_944"<?,?> ⬅️ ::Cast(%"val_943") {to=7}
    1180 |  # node_Reshape_1180
            %"view_7"<FLOAT,[1,8,384,431]> ⬅️ ::Reshape(%"add_28", %"val_944") {allowzero=0}
    1181 |  # node_Transpose_1181
            %"permute_8"<FLOAT,[1,384,8,431]> ⬅️ ::Transpose(%"view_7") {perm=[0, 2, 1, 3]}
    1182 |  # node_Conv_1182
            %"convolution_47"<FLOAT,[1,768,8,431]> ⬅️ ::Conv(%"permute_8", %"encoder.3.rewrite.weight", %"encoder.3.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[0, 0, 0, 0], strides=[1, 1]}
    1183 |  # node_aten_glu_1183
            %"glu_23"<FLOAT,[1,384,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_47") {dim=1}
    1184 |  # node_Constant_1184
            %"val_945"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([   1,  384, 3448]), name=None)}
    1185 |  # node_Cast_1185
            %"val_946"<?,?> ⬅️ ::Cast(%"val_945") {to=7}
    1186 |  # node_Reshape_1186
            %"view_8"<FLOAT,[1,384,3448]> ⬅️ ::Reshape(%"glu_23", %"val_946") {allowzero=0}
    1187 |  # node_Conv_1187
            %"convolution_48"<FLOAT,[1,512,3448]> ⬅️ ::Conv(%"view_8", %"channel_upsampler.weight", %"channel_upsampler.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    1188 |  # node_Constant_1188
            %"val_947"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 512,   8, 431]), name=None)}
    1189 |  # node_Cast_1189
            %"val_948"<?,?> ⬅️ ::Cast(%"val_947") {to=7}
    1190 |  # node_Reshape_1190
            %"view_9"<FLOAT,[1,512,8,431]> ⬅️ ::Reshape(%"convolution_48", %"val_948") {allowzero=0}
    1191 |  # node_Conv_1191
            %"convolution_49"<FLOAT,[1,512,1723]> ⬅️ ::Conv(%"glu_20", %"channel_upsampler_t.weight", %"channel_upsampler_t.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    1192 |  # node_Constant_1192
            %"val_949"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([512,   8, 431]), name=None)}
    1193 |  # node_Cast_1193
            %"val_950"<?,?> ⬅️ ::Cast(%"val_949") {to=7}
    1194 |  # node_Constant_1194
            %"val_951"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1195 |  # node_Cast_1195
            %"val_952"<?,?> ⬅️ ::Cast(%"val_951") {to=1}
    1196 |  # node_Expand_1196
            %"zeros"<FLOAT,[512,8,431]> ⬅️ ::Expand(%"val_952", %"val_950")
    1197 |  # node_Constant_1197
            %"val_953"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(0., dtype=float32), name=None)}
    1198 |  # node_Constant_1198
            %"val_954"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(256., dtype=float32), name=None)}
    1199 |  # node_Constant_1199
            %"val_955"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(2., dtype=float32), name=None)}
    1200 |  # node_Range_1200
            %"arange_1"<FLOAT,[128]> ⬅️ ::Range(%"val_953", %"val_954", %"val_955")
    1201 |  # node_Constant_1201
            %"val_956"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(-0.03597789, dtype=float32), name=None)}
    1202 |  # node_Mul_1202
            %"mul_24"<FLOAT,[128]> ⬅️ ::Mul(%"arange_1", %"val_956")
    1203 |  # node_aten_exp_1203
            %"exp"<FLOAT,[128]> ⬅️ pkg.onnxscript.torch_lib::aten_exp(%"mul_24")
    1204 |  # node_Constant_1204
            %"val_957"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(431., dtype=float32), name=None)}
    1205 |  # node_Range_1205
            %"arange_2"<FLOAT,[431]> ⬅️ ::Range(%"val_953", %"val_957", %"val_14")
    1206 |  # node_aten_unsqueeze_1206
            %"unsqueeze_18"<FLOAT,[431,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"arange_2") {dim=1}
    1207 |  # node_Constant_1207
            %"val_958"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(8., dtype=float32), name=None)}
    1208 |  # node_Range_1208
            %"arange_3"<FLOAT,[8]> ⬅️ ::Range(%"val_953", %"val_958", %"val_14")
    1209 |  # node_aten_unsqueeze_1209
            %"unsqueeze_19"<FLOAT,[8,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"arange_3") {dim=1}
    1210 |  # node_Mul_1210
            %"mul_25"<FLOAT,[431,128]> ⬅️ ::Mul(%"unsqueeze_18", %"exp")
    1211 |  # node_Sin_1211
            %"sin"<FLOAT,[431,128]> ⬅️ ::Sin(%"mul_25")
    1212 |  # node_Transpose_1212
            %"transpose"<FLOAT,[128,431]> ⬅️ ::Transpose(%"sin") {perm=[1, 0]}
    1213 |  # node_aten_unsqueeze_1213
            %"unsqueeze_20"<FLOAT,[128,1,431]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"transpose") {dim=1}
    1214 |  # node_Constant_1214
            %"val_959"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1, 8, 1]), name=None)}
    1215 |  # node_aten_repeat_1215
            %"repeat"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_repeat(%"unsqueeze_20", %"val_959")
    1216 |  # node_Cast_1216
            %"val_960"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1217 |  # node_Constant_1217
            %"val_961"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1218 |  # node_Reshape_1218
            %"val_962"<?,?> ⬅️ ::Reshape(%"val_960", %"val_961") {allowzero=0}
    1219 |  # node_Constant_1219
            %"val_963"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(256), name=None)}
    1220 |  # node_Cast_1220
            %"val_964"<?,?> ⬅️ ::Cast(%"val_963") {to=7}
    1221 |  # node_Constant_1221
            %"val_965"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1222 |  # node_Reshape_1222
            %"val_966"<?,?> ⬅️ ::Reshape(%"val_964", %"val_965") {allowzero=0}
    1223 |  # node_Cast_1223
            %"val_967"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1224 |  # node_Constant_1224
            %"val_968"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1225 |  # node_Reshape_1225
            %"val_969"<?,?> ⬅️ ::Reshape(%"val_967", %"val_968") {allowzero=0}
    1226 |  # node_Cast_1226
            %"val_970"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1227 |  # node_Constant_1227
            %"val_971"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1228 |  # node_Reshape_1228
            %"val_972"<?,?> ⬅️ ::Reshape(%"val_970", %"val_971") {allowzero=0}
    1229 |  # node_Slice_1229
            %"slice_19"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"zeros", %"val_962", %"val_966", %"val_969", %"val_972")
    1230 |  # node_Cast_1230
            %"val_973"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1231 |  # node_Constant_1231
            %"val_974"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1232 |  # node_Reshape_1232
            %"val_975"<?,?> ⬅️ ::Reshape(%"val_973", %"val_974") {allowzero=0}
    1233 |  # node_Cast_1233
            %"val_976"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1234 |  # node_Constant_1234
            %"val_977"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1235 |  # node_Reshape_1235
            %"val_978"<?,?> ⬅️ ::Reshape(%"val_976", %"val_977") {allowzero=0}
    1236 |  # node_Cast_1236
            %"val_979"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1237 |  # node_Constant_1237
            %"val_980"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1238 |  # node_Reshape_1238
            %"val_981"<?,?> ⬅️ ::Reshape(%"val_979", %"val_980") {allowzero=0}
    1239 |  # node_Constant_1239
            %"val_982"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1240 |  # node_Slice_1240
            %"slice_20"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_19", %"val_975", %"val_978", %"val_981", %"val_982")
    1241 |  # node_Cast_1241
            %"val_983"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1242 |  # node_Constant_1242
            %"val_984"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1243 |  # node_Reshape_1243
            %"val_985"<?,?> ⬅️ ::Reshape(%"val_983", %"val_984") {allowzero=0}
    1244 |  # node_Cast_1244
            %"val_986"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1245 |  # node_Constant_1245
            %"val_987"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1246 |  # node_Reshape_1246
            %"val_988"<?,?> ⬅️ ::Reshape(%"val_986", %"val_987") {allowzero=0}
    1247 |  # node_Cast_1247
            %"val_989"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1248 |  # node_Constant_1248
            %"val_990"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1249 |  # node_Reshape_1249
            %"val_991"<?,?> ⬅️ ::Reshape(%"val_989", %"val_990") {allowzero=0}
    1250 |  # node_Constant_1250
            %"val_992"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1251 |  # node_Slice_1251
            %"slice_21"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_20", %"val_985", %"val_988", %"val_991", %"val_992")
    1252 |  # node_aten_copy_1252
            %"copy"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_copy(%"slice_21", %"repeat") {non_blocking=False}
    1253 |  # node_Cast_1253
            %"val_993"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1254 |  # node_Constant_1254
            %"val_994"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1255 |  # node_Reshape_1255
            %"val_995"<?,?> ⬅️ ::Reshape(%"val_993", %"val_994") {allowzero=0}
    1256 |  # node_Cast_1256
            %"val_996"<?,?> ⬅️ ::Cast(%"val_963") {to=7}
    1257 |  # node_Constant_1257
            %"val_997"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1258 |  # node_Reshape_1258
            %"val_998"<?,?> ⬅️ ::Reshape(%"val_996", %"val_997") {allowzero=0}
    1259 |  # node_Cast_1259
            %"val_999"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1260 |  # node_Constant_1260
            %"val_1000"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1261 |  # node_Reshape_1261
            %"val_1001"<?,?> ⬅️ ::Reshape(%"val_999", %"val_1000") {allowzero=0}
    1262 |  # node_Cast_1262
            %"val_1002"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1263 |  # node_Constant_1263
            %"val_1003"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1264 |  # node_Reshape_1264
            %"val_1004"<?,?> ⬅️ ::Reshape(%"val_1002", %"val_1003") {allowzero=0}
    1265 |  # node_Slice_1265
            %"slice_22"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"zeros", %"val_995", %"val_998", %"val_1001", %"val_1004")
    1266 |  # node_Cast_1266
            %"val_1005"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1267 |  # node_Constant_1267
            %"val_1006"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1268 |  # node_Reshape_1268
            %"val_1007"<?,?> ⬅️ ::Reshape(%"val_1005", %"val_1006") {allowzero=0}
    1269 |  # node_Cast_1269
            %"val_1008"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1270 |  # node_Constant_1270
            %"val_1009"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1271 |  # node_Reshape_1271
            %"val_1010"<?,?> ⬅️ ::Reshape(%"val_1008", %"val_1009") {allowzero=0}
    1272 |  # node_Cast_1272
            %"val_1011"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1273 |  # node_Constant_1273
            %"val_1012"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1274 |  # node_Reshape_1274
            %"val_1013"<?,?> ⬅️ ::Reshape(%"val_1011", %"val_1012") {allowzero=0}
    1275 |  # node_Constant_1275
            %"val_1014"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1276 |  # node_Slice_1276
            %"slice_23"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_22", %"val_1007", %"val_1010", %"val_1013", %"val_1014")
    1277 |  # node_Constant_1277
            %"val_1015"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1278 |  # node_Shape_1278
            %"val_1016"<?,?> ⬅️ ::Shape(%"slice_23") {start=0}
    1279 |  # node_Gather_1279
            %"val_1017"<?,?> ⬅️ ::Gather(%"val_1016", %"val_276") {axis=0}
    1280 |  # node_Range_1280
            %"val_1018"<?,?> ⬅️ ::Range(%"val_80", %"val_1017", %"val_35")
    1281 |  # node_Unsqueeze_1281
            %"val_1019"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1015")
    1282 |  # node_Unsqueeze_1282
            %"val_1020"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1015")
    1283 |  # node_Unsqueeze_1283
            %"val_1021"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1015")
    1284 |  # node_Slice_1284
            %"val_1022"<?,?> ⬅️ ::Slice(%"val_1018", %"val_1019", %"val_1020", %"val_1015", %"val_1021")
    1285 |  # node_Constant_1285
            %"val_1023"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(-1), name=None)}
    1286 |  # node_Unsqueeze_1286
            %"val_1024"<?,?> ⬅️ ::Unsqueeze(%"val_1022", %"val_1023")
    1287 |  # node_Transpose_1287
            %"val_1025"<?,?> ⬅️ ::Transpose(%"copy") {perm=[2, 1, 0]}
    1288 |  # node_Transpose_1288
            %"val_1026"<?,?> ⬅️ ::Transpose(%"slice_23") {perm=[2, 1, 0]}
    1289 |  # node_ScatterND_1289
            %"val_1027"<?,?> ⬅️ ::ScatterND(%"val_1026", %"val_1024", %"val_1025") {reduction=none}
    1290 |  # node_Transpose_1290
            %"slice_scatter"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1027") {perm=[2, 1, 0]}
    1291 |  # node_Constant_1291
            %"val_1028"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1292 |  # node_Shape_1292
            %"val_1029"<?,?> ⬅️ ::Shape(%"slice_22") {start=0}
    1293 |  # node_Gather_1293
            %"val_1030"<?,?> ⬅️ ::Gather(%"val_1029", %"val_35") {axis=0}
    1294 |  # node_Range_1294
            %"val_1031"<?,?> ⬅️ ::Range(%"val_80", %"val_1030", %"val_35")
    1295 |  # node_Unsqueeze_1295
            %"val_1032"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1028")
    1296 |  # node_Unsqueeze_1296
            %"val_1033"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1028")
    1297 |  # node_Unsqueeze_1297
            %"val_1034"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1028")
    1298 |  # node_Slice_1298
            %"val_1035"<?,?> ⬅️ ::Slice(%"val_1031", %"val_1032", %"val_1033", %"val_1028", %"val_1034")
    1299 |  # node_Unsqueeze_1299
            %"val_1036"<?,?> ⬅️ ::Unsqueeze(%"val_1035", %"val_1023")
    1300 |  # node_Transpose_1300
            %"val_1037"<?,?> ⬅️ ::Transpose(%"slice_scatter") {perm=[1, 0, 2]}
    1301 |  # node_Transpose_1301
            %"val_1038"<?,?> ⬅️ ::Transpose(%"slice_22") {perm=[1, 0, 2]}
    1302 |  # node_ScatterND_1302
            %"val_1039"<?,?> ⬅️ ::ScatterND(%"val_1038", %"val_1036", %"val_1037") {reduction=none}
    1303 |  # node_Transpose_1303
            %"slice_scatter_1"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1039") {perm=[1, 0, 2]}
    1304 |  # node_Constant_1304
            %"val_1040"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1305 |  # node_Shape_1305
            %"val_1041"<?,?> ⬅️ ::Shape(%"zeros") {start=0}
    1306 |  # node_Gather_1306
            %"val_1042"<?,?> ⬅️ ::Gather(%"val_1041", %"val_80") {axis=0}
    1307 |  # node_Range_1307
            %"val_1043"<?,?> ⬅️ ::Range(%"val_80", %"val_1042", %"val_35")
    1308 |  # node_Unsqueeze_1308
            %"val_1044"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1040")
    1309 |  # node_Unsqueeze_1309
            %"val_1045"<?,?> ⬅️ ::Unsqueeze(%"val_963", %"val_1040")
    1310 |  # node_Unsqueeze_1310
            %"val_1046"<?,?> ⬅️ ::Unsqueeze(%"val_276", %"val_1040")
    1311 |  # node_Slice_1311
            %"val_1047"<?,?> ⬅️ ::Slice(%"val_1043", %"val_1044", %"val_1045", %"val_1040", %"val_1046")
    1312 |  # node_Unsqueeze_1312
            %"val_1048"<?,?> ⬅️ ::Unsqueeze(%"val_1047", %"val_1023")
    1313 |  # node_ScatterND_1313
            %"slice_scatter_2"<FLOAT,[512,8,431]> ⬅️ ::ScatterND(%"zeros", %"val_1048", %"slice_scatter_1") {reduction=none}
    1314 |  # node_Mul_1314
            %"mul_26"<FLOAT,[431,128]> ⬅️ ::Mul(%"unsqueeze_18", %"exp")
    1315 |  # node_Cos_1315
            %"cos"<FLOAT,[431,128]> ⬅️ ::Cos(%"mul_26")
    1316 |  # node_Transpose_1316
            %"transpose_1"<FLOAT,[128,431]> ⬅️ ::Transpose(%"cos") {perm=[1, 0]}
    1317 |  # node_aten_unsqueeze_1317
            %"unsqueeze_21"<FLOAT,[128,1,431]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"transpose_1") {dim=1}
    1318 |  # node_aten_repeat_1318
            %"repeat_1"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_repeat(%"unsqueeze_21", %"val_959")
    1319 |  # node_Cast_1319
            %"val_1049"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1320 |  # node_Constant_1320
            %"val_1050"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1321 |  # node_Reshape_1321
            %"val_1051"<?,?> ⬅️ ::Reshape(%"val_1049", %"val_1050") {allowzero=0}
    1322 |  # node_Cast_1322
            %"val_1052"<?,?> ⬅️ ::Cast(%"val_963") {to=7}
    1323 |  # node_Constant_1323
            %"val_1053"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1324 |  # node_Reshape_1324
            %"val_1054"<?,?> ⬅️ ::Reshape(%"val_1052", %"val_1053") {allowzero=0}
    1325 |  # node_Cast_1325
            %"val_1055"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1326 |  # node_Constant_1326
            %"val_1056"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1327 |  # node_Reshape_1327
            %"val_1057"<?,?> ⬅️ ::Reshape(%"val_1055", %"val_1056") {allowzero=0}
    1328 |  # node_Cast_1328
            %"val_1058"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1329 |  # node_Constant_1329
            %"val_1059"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1330 |  # node_Reshape_1330
            %"val_1060"<?,?> ⬅️ ::Reshape(%"val_1058", %"val_1059") {allowzero=0}
    1331 |  # node_Slice_1331
            %"slice_24"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_scatter_2", %"val_1051", %"val_1054", %"val_1057", %"val_1060")
    1332 |  # node_Cast_1332
            %"val_1061"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1333 |  # node_Constant_1333
            %"val_1062"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1334 |  # node_Reshape_1334
            %"val_1063"<?,?> ⬅️ ::Reshape(%"val_1061", %"val_1062") {allowzero=0}
    1335 |  # node_Cast_1335
            %"val_1064"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1336 |  # node_Constant_1336
            %"val_1065"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1337 |  # node_Reshape_1337
            %"val_1066"<?,?> ⬅️ ::Reshape(%"val_1064", %"val_1065") {allowzero=0}
    1338 |  # node_Cast_1338
            %"val_1067"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1339 |  # node_Constant_1339
            %"val_1068"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1340 |  # node_Reshape_1340
            %"val_1069"<?,?> ⬅️ ::Reshape(%"val_1067", %"val_1068") {allowzero=0}
    1341 |  # node_Constant_1341
            %"val_1070"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1342 |  # node_Slice_1342
            %"slice_25"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_24", %"val_1063", %"val_1066", %"val_1069", %"val_1070")
    1343 |  # node_Cast_1343
            %"val_1071"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1344 |  # node_Constant_1344
            %"val_1072"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1345 |  # node_Reshape_1345
            %"val_1073"<?,?> ⬅️ ::Reshape(%"val_1071", %"val_1072") {allowzero=0}
    1346 |  # node_Cast_1346
            %"val_1074"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1347 |  # node_Constant_1347
            %"val_1075"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1348 |  # node_Reshape_1348
            %"val_1076"<?,?> ⬅️ ::Reshape(%"val_1074", %"val_1075") {allowzero=0}
    1349 |  # node_Cast_1349
            %"val_1077"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1350 |  # node_Constant_1350
            %"val_1078"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1351 |  # node_Reshape_1351
            %"val_1079"<?,?> ⬅️ ::Reshape(%"val_1077", %"val_1078") {allowzero=0}
    1352 |  # node_Constant_1352
            %"val_1080"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1353 |  # node_Slice_1353
            %"slice_26"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_25", %"val_1073", %"val_1076", %"val_1079", %"val_1080")
    1354 |  # node_aten_copy_1354
            %"copy_1"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_copy(%"slice_26", %"repeat_1") {non_blocking=False}
    1355 |  # node_Cast_1355
            %"val_1081"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1356 |  # node_Constant_1356
            %"val_1082"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1357 |  # node_Reshape_1357
            %"val_1083"<?,?> ⬅️ ::Reshape(%"val_1081", %"val_1082") {allowzero=0}
    1358 |  # node_Cast_1358
            %"val_1084"<?,?> ⬅️ ::Cast(%"val_963") {to=7}
    1359 |  # node_Constant_1359
            %"val_1085"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1360 |  # node_Reshape_1360
            %"val_1086"<?,?> ⬅️ ::Reshape(%"val_1084", %"val_1085") {allowzero=0}
    1361 |  # node_Cast_1361
            %"val_1087"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1362 |  # node_Constant_1362
            %"val_1088"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1363 |  # node_Reshape_1363
            %"val_1089"<?,?> ⬅️ ::Reshape(%"val_1087", %"val_1088") {allowzero=0}
    1364 |  # node_Cast_1364
            %"val_1090"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1365 |  # node_Constant_1365
            %"val_1091"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1366 |  # node_Reshape_1366
            %"val_1092"<?,?> ⬅️ ::Reshape(%"val_1090", %"val_1091") {allowzero=0}
    1367 |  # node_Slice_1367
            %"slice_27"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_scatter_2", %"val_1083", %"val_1086", %"val_1089", %"val_1092")
    1368 |  # node_Cast_1368
            %"val_1093"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1369 |  # node_Constant_1369
            %"val_1094"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1370 |  # node_Reshape_1370
            %"val_1095"<?,?> ⬅️ ::Reshape(%"val_1093", %"val_1094") {allowzero=0}
    1371 |  # node_Cast_1371
            %"val_1096"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1372 |  # node_Constant_1372
            %"val_1097"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1373 |  # node_Reshape_1373
            %"val_1098"<?,?> ⬅️ ::Reshape(%"val_1096", %"val_1097") {allowzero=0}
    1374 |  # node_Cast_1374
            %"val_1099"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1375 |  # node_Constant_1375
            %"val_1100"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1376 |  # node_Reshape_1376
            %"val_1101"<?,?> ⬅️ ::Reshape(%"val_1099", %"val_1100") {allowzero=0}
    1377 |  # node_Constant_1377
            %"val_1102"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1378 |  # node_Slice_1378
            %"slice_28"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_27", %"val_1095", %"val_1098", %"val_1101", %"val_1102")
    1379 |  # node_Constant_1379
            %"val_1103"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1380 |  # node_Shape_1380
            %"val_1104"<?,?> ⬅️ ::Shape(%"slice_28") {start=0}
    1381 |  # node_Gather_1381
            %"val_1105"<?,?> ⬅️ ::Gather(%"val_1104", %"val_276") {axis=0}
    1382 |  # node_Range_1382
            %"val_1106"<?,?> ⬅️ ::Range(%"val_80", %"val_1105", %"val_35")
    1383 |  # node_Unsqueeze_1383
            %"val_1107"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1103")
    1384 |  # node_Unsqueeze_1384
            %"val_1108"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1103")
    1385 |  # node_Unsqueeze_1385
            %"val_1109"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1103")
    1386 |  # node_Slice_1386
            %"val_1110"<?,?> ⬅️ ::Slice(%"val_1106", %"val_1107", %"val_1108", %"val_1103", %"val_1109")
    1387 |  # node_Unsqueeze_1387
            %"val_1111"<?,?> ⬅️ ::Unsqueeze(%"val_1110", %"val_1023")
    1388 |  # node_Transpose_1388
            %"val_1112"<?,?> ⬅️ ::Transpose(%"copy_1") {perm=[2, 1, 0]}
    1389 |  # node_Transpose_1389
            %"val_1113"<?,?> ⬅️ ::Transpose(%"slice_28") {perm=[2, 1, 0]}
    1390 |  # node_ScatterND_1390
            %"val_1114"<?,?> ⬅️ ::ScatterND(%"val_1113", %"val_1111", %"val_1112") {reduction=none}
    1391 |  # node_Transpose_1391
            %"slice_scatter_3"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1114") {perm=[2, 1, 0]}
    1392 |  # node_Constant_1392
            %"val_1115"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1393 |  # node_Shape_1393
            %"val_1116"<?,?> ⬅️ ::Shape(%"slice_27") {start=0}
    1394 |  # node_Gather_1394
            %"val_1117"<?,?> ⬅️ ::Gather(%"val_1116", %"val_35") {axis=0}
    1395 |  # node_Range_1395
            %"val_1118"<?,?> ⬅️ ::Range(%"val_80", %"val_1117", %"val_35")
    1396 |  # node_Unsqueeze_1396
            %"val_1119"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1115")
    1397 |  # node_Unsqueeze_1397
            %"val_1120"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1115")
    1398 |  # node_Unsqueeze_1398
            %"val_1121"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1115")
    1399 |  # node_Slice_1399
            %"val_1122"<?,?> ⬅️ ::Slice(%"val_1118", %"val_1119", %"val_1120", %"val_1115", %"val_1121")
    1400 |  # node_Unsqueeze_1400
            %"val_1123"<?,?> ⬅️ ::Unsqueeze(%"val_1122", %"val_1023")
    1401 |  # node_Transpose_1401
            %"val_1124"<?,?> ⬅️ ::Transpose(%"slice_scatter_3") {perm=[1, 0, 2]}
    1402 |  # node_Transpose_1402
            %"val_1125"<?,?> ⬅️ ::Transpose(%"slice_27") {perm=[1, 0, 2]}
    1403 |  # node_ScatterND_1403
            %"val_1126"<?,?> ⬅️ ::ScatterND(%"val_1125", %"val_1123", %"val_1124") {reduction=none}
    1404 |  # node_Transpose_1404
            %"slice_scatter_4"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1126") {perm=[1, 0, 2]}
    1405 |  # node_Constant_1405
            %"val_1127"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1406 |  # node_Shape_1406
            %"val_1128"<?,?> ⬅️ ::Shape(%"slice_scatter_2") {start=0}
    1407 |  # node_Gather_1407
            %"val_1129"<?,?> ⬅️ ::Gather(%"val_1128", %"val_80") {axis=0}
    1408 |  # node_Range_1408
            %"val_1130"<?,?> ⬅️ ::Range(%"val_80", %"val_1129", %"val_35")
    1409 |  # node_Unsqueeze_1409
            %"val_1131"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1127")
    1410 |  # node_Unsqueeze_1410
            %"val_1132"<?,?> ⬅️ ::Unsqueeze(%"val_963", %"val_1127")
    1411 |  # node_Unsqueeze_1411
            %"val_1133"<?,?> ⬅️ ::Unsqueeze(%"val_276", %"val_1127")
    1412 |  # node_Slice_1412
            %"val_1134"<?,?> ⬅️ ::Slice(%"val_1130", %"val_1131", %"val_1132", %"val_1127", %"val_1133")
    1413 |  # node_Unsqueeze_1413
            %"val_1135"<?,?> ⬅️ ::Unsqueeze(%"val_1134", %"val_1023")
    1414 |  # node_ScatterND_1414
            %"slice_scatter_5"<FLOAT,[512,8,431]> ⬅️ ::ScatterND(%"slice_scatter_2", %"val_1135", %"slice_scatter_4") {reduction=none}
    1415 |  # node_Mul_1415
            %"mul_27"<FLOAT,[8,128]> ⬅️ ::Mul(%"unsqueeze_19", %"exp")
    1416 |  # node_Sin_1416
            %"sin_1"<FLOAT,[8,128]> ⬅️ ::Sin(%"mul_27")
    1417 |  # node_Transpose_1417
            %"transpose_2"<FLOAT,[128,8]> ⬅️ ::Transpose(%"sin_1") {perm=[1, 0]}
    1418 |  # node_aten_unsqueeze_1418
            %"unsqueeze_22"<FLOAT,[128,8,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"transpose_2") {dim=2}
    1419 |  # node_Constant_1419
            %"val_1136"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([  1,   1, 431]), name=None)}
    1420 |  # node_aten_repeat_1420
            %"repeat_2"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_repeat(%"unsqueeze_22", %"val_1136")
    1421 |  # node_Cast_1421
            %"val_1137"<?,?> ⬅️ ::Cast(%"val_963") {to=7}
    1422 |  # node_Constant_1422
            %"val_1138"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1423 |  # node_Reshape_1423
            %"val_1139"<?,?> ⬅️ ::Reshape(%"val_1137", %"val_1138") {allowzero=0}
    1424 |  # node_Cast_1424
            %"val_1140"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1425 |  # node_Constant_1425
            %"val_1141"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1426 |  # node_Reshape_1426
            %"val_1142"<?,?> ⬅️ ::Reshape(%"val_1140", %"val_1141") {allowzero=0}
    1427 |  # node_Cast_1427
            %"val_1143"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1428 |  # node_Constant_1428
            %"val_1144"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1429 |  # node_Reshape_1429
            %"val_1145"<?,?> ⬅️ ::Reshape(%"val_1143", %"val_1144") {allowzero=0}
    1430 |  # node_Cast_1430
            %"val_1146"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1431 |  # node_Constant_1431
            %"val_1147"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1432 |  # node_Reshape_1432
            %"val_1148"<?,?> ⬅️ ::Reshape(%"val_1146", %"val_1147") {allowzero=0}
    1433 |  # node_Slice_1433
            %"slice_29"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_scatter_5", %"val_1139", %"val_1142", %"val_1145", %"val_1148")
    1434 |  # node_Cast_1434
            %"val_1149"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1435 |  # node_Constant_1435
            %"val_1150"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1436 |  # node_Reshape_1436
            %"val_1151"<?,?> ⬅️ ::Reshape(%"val_1149", %"val_1150") {allowzero=0}
    1437 |  # node_Cast_1437
            %"val_1152"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1438 |  # node_Constant_1438
            %"val_1153"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1439 |  # node_Reshape_1439
            %"val_1154"<?,?> ⬅️ ::Reshape(%"val_1152", %"val_1153") {allowzero=0}
    1440 |  # node_Cast_1440
            %"val_1155"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1441 |  # node_Constant_1441
            %"val_1156"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1442 |  # node_Reshape_1442
            %"val_1157"<?,?> ⬅️ ::Reshape(%"val_1155", %"val_1156") {allowzero=0}
    1443 |  # node_Constant_1443
            %"val_1158"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1444 |  # node_Slice_1444
            %"slice_30"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_29", %"val_1151", %"val_1154", %"val_1157", %"val_1158")
    1445 |  # node_Cast_1445
            %"val_1159"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1446 |  # node_Constant_1446
            %"val_1160"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1447 |  # node_Reshape_1447
            %"val_1161"<?,?> ⬅️ ::Reshape(%"val_1159", %"val_1160") {allowzero=0}
    1448 |  # node_Cast_1448
            %"val_1162"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1449 |  # node_Constant_1449
            %"val_1163"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1450 |  # node_Reshape_1450
            %"val_1164"<?,?> ⬅️ ::Reshape(%"val_1162", %"val_1163") {allowzero=0}
    1451 |  # node_Cast_1451
            %"val_1165"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1452 |  # node_Constant_1452
            %"val_1166"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1453 |  # node_Reshape_1453
            %"val_1167"<?,?> ⬅️ ::Reshape(%"val_1165", %"val_1166") {allowzero=0}
    1454 |  # node_Constant_1454
            %"val_1168"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1455 |  # node_Slice_1455
            %"slice_31"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_30", %"val_1161", %"val_1164", %"val_1167", %"val_1168")
    1456 |  # node_aten_copy_1456
            %"copy_2"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_copy(%"slice_31", %"repeat_2") {non_blocking=False}
    1457 |  # node_Cast_1457
            %"val_1169"<?,?> ⬅️ ::Cast(%"val_963") {to=7}
    1458 |  # node_Constant_1458
            %"val_1170"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1459 |  # node_Reshape_1459
            %"val_1171"<?,?> ⬅️ ::Reshape(%"val_1169", %"val_1170") {allowzero=0}
    1460 |  # node_Cast_1460
            %"val_1172"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1461 |  # node_Constant_1461
            %"val_1173"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1462 |  # node_Reshape_1462
            %"val_1174"<?,?> ⬅️ ::Reshape(%"val_1172", %"val_1173") {allowzero=0}
    1463 |  # node_Cast_1463
            %"val_1175"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1464 |  # node_Constant_1464
            %"val_1176"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1465 |  # node_Reshape_1465
            %"val_1177"<?,?> ⬅️ ::Reshape(%"val_1175", %"val_1176") {allowzero=0}
    1466 |  # node_Cast_1466
            %"val_1178"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1467 |  # node_Constant_1467
            %"val_1179"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1468 |  # node_Reshape_1468
            %"val_1180"<?,?> ⬅️ ::Reshape(%"val_1178", %"val_1179") {allowzero=0}
    1469 |  # node_Slice_1469
            %"slice_32"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_scatter_5", %"val_1171", %"val_1174", %"val_1177", %"val_1180")
    1470 |  # node_Cast_1470
            %"val_1181"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1471 |  # node_Constant_1471
            %"val_1182"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1472 |  # node_Reshape_1472
            %"val_1183"<?,?> ⬅️ ::Reshape(%"val_1181", %"val_1182") {allowzero=0}
    1473 |  # node_Cast_1473
            %"val_1184"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1474 |  # node_Constant_1474
            %"val_1185"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1475 |  # node_Reshape_1475
            %"val_1186"<?,?> ⬅️ ::Reshape(%"val_1184", %"val_1185") {allowzero=0}
    1476 |  # node_Cast_1476
            %"val_1187"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1477 |  # node_Constant_1477
            %"val_1188"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1478 |  # node_Reshape_1478
            %"val_1189"<?,?> ⬅️ ::Reshape(%"val_1187", %"val_1188") {allowzero=0}
    1479 |  # node_Constant_1479
            %"val_1190"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1480 |  # node_Slice_1480
            %"slice_33"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_32", %"val_1183", %"val_1186", %"val_1189", %"val_1190")
    1481 |  # node_Constant_1481
            %"val_1191"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1482 |  # node_Shape_1482
            %"val_1192"<?,?> ⬅️ ::Shape(%"slice_33") {start=0}
    1483 |  # node_Gather_1483
            %"val_1193"<?,?> ⬅️ ::Gather(%"val_1192", %"val_276") {axis=0}
    1484 |  # node_Range_1484
            %"val_1194"<?,?> ⬅️ ::Range(%"val_80", %"val_1193", %"val_35")
    1485 |  # node_Unsqueeze_1485
            %"val_1195"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1191")
    1486 |  # node_Unsqueeze_1486
            %"val_1196"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1191")
    1487 |  # node_Unsqueeze_1487
            %"val_1197"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1191")
    1488 |  # node_Slice_1488
            %"val_1198"<?,?> ⬅️ ::Slice(%"val_1194", %"val_1195", %"val_1196", %"val_1191", %"val_1197")
    1489 |  # node_Unsqueeze_1489
            %"val_1199"<?,?> ⬅️ ::Unsqueeze(%"val_1198", %"val_1023")
    1490 |  # node_Transpose_1490
            %"val_1200"<?,?> ⬅️ ::Transpose(%"copy_2") {perm=[2, 1, 0]}
    1491 |  # node_Transpose_1491
            %"val_1201"<?,?> ⬅️ ::Transpose(%"slice_33") {perm=[2, 1, 0]}
    1492 |  # node_ScatterND_1492
            %"val_1202"<?,?> ⬅️ ::ScatterND(%"val_1201", %"val_1199", %"val_1200") {reduction=none}
    1493 |  # node_Transpose_1493
            %"slice_scatter_6"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1202") {perm=[2, 1, 0]}
    1494 |  # node_Constant_1494
            %"val_1203"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1495 |  # node_Shape_1495
            %"val_1204"<?,?> ⬅️ ::Shape(%"slice_32") {start=0}
    1496 |  # node_Gather_1496
            %"val_1205"<?,?> ⬅️ ::Gather(%"val_1204", %"val_35") {axis=0}
    1497 |  # node_Range_1497
            %"val_1206"<?,?> ⬅️ ::Range(%"val_80", %"val_1205", %"val_35")
    1498 |  # node_Unsqueeze_1498
            %"val_1207"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1203")
    1499 |  # node_Unsqueeze_1499
            %"val_1208"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1203")
    1500 |  # node_Unsqueeze_1500
            %"val_1209"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1203")
    1501 |  # node_Slice_1501
            %"val_1210"<?,?> ⬅️ ::Slice(%"val_1206", %"val_1207", %"val_1208", %"val_1203", %"val_1209")
    1502 |  # node_Unsqueeze_1502
            %"val_1211"<?,?> ⬅️ ::Unsqueeze(%"val_1210", %"val_1023")
    1503 |  # node_Transpose_1503
            %"val_1212"<?,?> ⬅️ ::Transpose(%"slice_scatter_6") {perm=[1, 0, 2]}
    1504 |  # node_Transpose_1504
            %"val_1213"<?,?> ⬅️ ::Transpose(%"slice_32") {perm=[1, 0, 2]}
    1505 |  # node_ScatterND_1505
            %"val_1214"<?,?> ⬅️ ::ScatterND(%"val_1213", %"val_1211", %"val_1212") {reduction=none}
    1506 |  # node_Transpose_1506
            %"slice_scatter_7"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1214") {perm=[1, 0, 2]}
    1507 |  # node_Constant_1507
            %"val_1215"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1508 |  # node_Shape_1508
            %"val_1216"<?,?> ⬅️ ::Shape(%"slice_scatter_5") {start=0}
    1509 |  # node_Gather_1509
            %"val_1217"<?,?> ⬅️ ::Gather(%"val_1216", %"val_80") {axis=0}
    1510 |  # node_Range_1510
            %"val_1218"<?,?> ⬅️ ::Range(%"val_80", %"val_1217", %"val_35")
    1511 |  # node_Unsqueeze_1511
            %"val_1219"<?,?> ⬅️ ::Unsqueeze(%"val_963", %"val_1215")
    1512 |  # node_Unsqueeze_1512
            %"val_1220"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1215")
    1513 |  # node_Unsqueeze_1513
            %"val_1221"<?,?> ⬅️ ::Unsqueeze(%"val_276", %"val_1215")
    1514 |  # node_Slice_1514
            %"val_1222"<?,?> ⬅️ ::Slice(%"val_1218", %"val_1219", %"val_1220", %"val_1215", %"val_1221")
    1515 |  # node_Unsqueeze_1515
            %"val_1223"<?,?> ⬅️ ::Unsqueeze(%"val_1222", %"val_1023")
    1516 |  # node_ScatterND_1516
            %"slice_scatter_8"<FLOAT,[512,8,431]> ⬅️ ::ScatterND(%"slice_scatter_5", %"val_1223", %"slice_scatter_7") {reduction=none}
    1517 |  # node_Mul_1517
            %"mul_28"<FLOAT,[8,128]> ⬅️ ::Mul(%"unsqueeze_19", %"exp")
    1518 |  # node_Cos_1518
            %"cos_1"<FLOAT,[8,128]> ⬅️ ::Cos(%"mul_28")
    1519 |  # node_Transpose_1519
            %"transpose_3"<FLOAT,[128,8]> ⬅️ ::Transpose(%"cos_1") {perm=[1, 0]}
    1520 |  # node_aten_unsqueeze_1520
            %"unsqueeze_23"<FLOAT,[128,8,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"transpose_3") {dim=2}
    1521 |  # node_aten_repeat_1521
            %"repeat_3"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_repeat(%"unsqueeze_23", %"val_1136")
    1522 |  # node_Constant_1522
            %"val_1224"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(257), name=None)}
    1523 |  # node_Cast_1523
            %"val_1225"<?,?> ⬅️ ::Cast(%"val_1224") {to=7}
    1524 |  # node_Constant_1524
            %"val_1226"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1525 |  # node_Reshape_1525
            %"val_1227"<?,?> ⬅️ ::Reshape(%"val_1225", %"val_1226") {allowzero=0}
    1526 |  # node_Cast_1526
            %"val_1228"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1527 |  # node_Constant_1527
            %"val_1229"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1528 |  # node_Reshape_1528
            %"val_1230"<?,?> ⬅️ ::Reshape(%"val_1228", %"val_1229") {allowzero=0}
    1529 |  # node_Cast_1529
            %"val_1231"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1530 |  # node_Constant_1530
            %"val_1232"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1531 |  # node_Reshape_1531
            %"val_1233"<?,?> ⬅️ ::Reshape(%"val_1231", %"val_1232") {allowzero=0}
    1532 |  # node_Cast_1532
            %"val_1234"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1533 |  # node_Constant_1533
            %"val_1235"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1534 |  # node_Reshape_1534
            %"val_1236"<?,?> ⬅️ ::Reshape(%"val_1234", %"val_1235") {allowzero=0}
    1535 |  # node_Slice_1535
            %"slice_34"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_scatter_8", %"val_1227", %"val_1230", %"val_1233", %"val_1236")
    1536 |  # node_Cast_1536
            %"val_1237"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1537 |  # node_Constant_1537
            %"val_1238"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1538 |  # node_Reshape_1538
            %"val_1239"<?,?> ⬅️ ::Reshape(%"val_1237", %"val_1238") {allowzero=0}
    1539 |  # node_Cast_1539
            %"val_1240"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1540 |  # node_Constant_1540
            %"val_1241"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1541 |  # node_Reshape_1541
            %"val_1242"<?,?> ⬅️ ::Reshape(%"val_1240", %"val_1241") {allowzero=0}
    1542 |  # node_Cast_1542
            %"val_1243"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1543 |  # node_Constant_1543
            %"val_1244"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1544 |  # node_Reshape_1544
            %"val_1245"<?,?> ⬅️ ::Reshape(%"val_1243", %"val_1244") {allowzero=0}
    1545 |  # node_Constant_1545
            %"val_1246"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1546 |  # node_Slice_1546
            %"slice_35"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_34", %"val_1239", %"val_1242", %"val_1245", %"val_1246")
    1547 |  # node_Cast_1547
            %"val_1247"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1548 |  # node_Constant_1548
            %"val_1248"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1549 |  # node_Reshape_1549
            %"val_1249"<?,?> ⬅️ ::Reshape(%"val_1247", %"val_1248") {allowzero=0}
    1550 |  # node_Cast_1550
            %"val_1250"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1551 |  # node_Constant_1551
            %"val_1251"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1552 |  # node_Reshape_1552
            %"val_1252"<?,?> ⬅️ ::Reshape(%"val_1250", %"val_1251") {allowzero=0}
    1553 |  # node_Cast_1553
            %"val_1253"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1554 |  # node_Constant_1554
            %"val_1254"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1555 |  # node_Reshape_1555
            %"val_1255"<?,?> ⬅️ ::Reshape(%"val_1253", %"val_1254") {allowzero=0}
    1556 |  # node_Constant_1556
            %"val_1256"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1557 |  # node_Slice_1557
            %"slice_36"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_35", %"val_1249", %"val_1252", %"val_1255", %"val_1256")
    1558 |  # node_aten_copy_1558
            %"copy_3"<FLOAT,[128,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_copy(%"slice_36", %"repeat_3") {non_blocking=False}
    1559 |  # node_Cast_1559
            %"val_1257"<?,?> ⬅️ ::Cast(%"val_1224") {to=7}
    1560 |  # node_Constant_1560
            %"val_1258"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1561 |  # node_Reshape_1561
            %"val_1259"<?,?> ⬅️ ::Reshape(%"val_1257", %"val_1258") {allowzero=0}
    1562 |  # node_Cast_1562
            %"val_1260"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1563 |  # node_Constant_1563
            %"val_1261"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1564 |  # node_Reshape_1564
            %"val_1262"<?,?> ⬅️ ::Reshape(%"val_1260", %"val_1261") {allowzero=0}
    1565 |  # node_Cast_1565
            %"val_1263"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1566 |  # node_Constant_1566
            %"val_1264"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1567 |  # node_Reshape_1567
            %"val_1265"<?,?> ⬅️ ::Reshape(%"val_1263", %"val_1264") {allowzero=0}
    1568 |  # node_Cast_1568
            %"val_1266"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    1569 |  # node_Constant_1569
            %"val_1267"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1570 |  # node_Reshape_1570
            %"val_1268"<?,?> ⬅️ ::Reshape(%"val_1266", %"val_1267") {allowzero=0}
    1571 |  # node_Slice_1571
            %"slice_37"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_scatter_8", %"val_1259", %"val_1262", %"val_1265", %"val_1268")
    1572 |  # node_Cast_1572
            %"val_1269"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1573 |  # node_Constant_1573
            %"val_1270"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1574 |  # node_Reshape_1574
            %"val_1271"<?,?> ⬅️ ::Reshape(%"val_1269", %"val_1270") {allowzero=0}
    1575 |  # node_Cast_1575
            %"val_1272"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1576 |  # node_Constant_1576
            %"val_1273"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1577 |  # node_Reshape_1577
            %"val_1274"<?,?> ⬅️ ::Reshape(%"val_1272", %"val_1273") {allowzero=0}
    1578 |  # node_Cast_1578
            %"val_1275"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1579 |  # node_Constant_1579
            %"val_1276"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1580 |  # node_Reshape_1580
            %"val_1277"<?,?> ⬅️ ::Reshape(%"val_1275", %"val_1276") {allowzero=0}
    1581 |  # node_Constant_1581
            %"val_1278"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1582 |  # node_Slice_1582
            %"slice_38"<FLOAT,[128,8,431]> ⬅️ ::Slice(%"slice_37", %"val_1271", %"val_1274", %"val_1277", %"val_1278")
    1583 |  # node_Constant_1583
            %"val_1279"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1584 |  # node_Shape_1584
            %"val_1280"<?,?> ⬅️ ::Shape(%"slice_38") {start=0}
    1585 |  # node_Gather_1585
            %"val_1281"<?,?> ⬅️ ::Gather(%"val_1280", %"val_276") {axis=0}
    1586 |  # node_Range_1586
            %"val_1282"<?,?> ⬅️ ::Range(%"val_80", %"val_1281", %"val_35")
    1587 |  # node_Unsqueeze_1587
            %"val_1283"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1279")
    1588 |  # node_Unsqueeze_1588
            %"val_1284"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1279")
    1589 |  # node_Unsqueeze_1589
            %"val_1285"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1279")
    1590 |  # node_Slice_1590
            %"val_1286"<?,?> ⬅️ ::Slice(%"val_1282", %"val_1283", %"val_1284", %"val_1279", %"val_1285")
    1591 |  # node_Unsqueeze_1591
            %"val_1287"<?,?> ⬅️ ::Unsqueeze(%"val_1286", %"val_1023")
    1592 |  # node_Transpose_1592
            %"val_1288"<?,?> ⬅️ ::Transpose(%"copy_3") {perm=[2, 1, 0]}
    1593 |  # node_Transpose_1593
            %"val_1289"<?,?> ⬅️ ::Transpose(%"slice_38") {perm=[2, 1, 0]}
    1594 |  # node_ScatterND_1594
            %"val_1290"<?,?> ⬅️ ::ScatterND(%"val_1289", %"val_1287", %"val_1288") {reduction=none}
    1595 |  # node_Transpose_1595
            %"slice_scatter_9"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1290") {perm=[2, 1, 0]}
    1596 |  # node_Constant_1596
            %"val_1291"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1597 |  # node_Shape_1597
            %"val_1292"<?,?> ⬅️ ::Shape(%"slice_37") {start=0}
    1598 |  # node_Gather_1598
            %"val_1293"<?,?> ⬅️ ::Gather(%"val_1292", %"val_35") {axis=0}
    1599 |  # node_Range_1599
            %"val_1294"<?,?> ⬅️ ::Range(%"val_80", %"val_1293", %"val_35")
    1600 |  # node_Unsqueeze_1600
            %"val_1295"<?,?> ⬅️ ::Unsqueeze(%"val_80", %"val_1291")
    1601 |  # node_Unsqueeze_1601
            %"val_1296"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1291")
    1602 |  # node_Unsqueeze_1602
            %"val_1297"<?,?> ⬅️ ::Unsqueeze(%"val_35", %"val_1291")
    1603 |  # node_Slice_1603
            %"val_1298"<?,?> ⬅️ ::Slice(%"val_1294", %"val_1295", %"val_1296", %"val_1291", %"val_1297")
    1604 |  # node_Unsqueeze_1604
            %"val_1299"<?,?> ⬅️ ::Unsqueeze(%"val_1298", %"val_1023")
    1605 |  # node_Transpose_1605
            %"val_1300"<?,?> ⬅️ ::Transpose(%"slice_scatter_9") {perm=[1, 0, 2]}
    1606 |  # node_Transpose_1606
            %"val_1301"<?,?> ⬅️ ::Transpose(%"slice_37") {perm=[1, 0, 2]}
    1607 |  # node_ScatterND_1607
            %"val_1302"<?,?> ⬅️ ::ScatterND(%"val_1301", %"val_1299", %"val_1300") {reduction=none}
    1608 |  # node_Transpose_1608
            %"slice_scatter_10"<FLOAT,[128,8,431]> ⬅️ ::Transpose(%"val_1302") {perm=[1, 0, 2]}
    1609 |  # node_Constant_1609
            %"val_1303"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1610 |  # node_Shape_1610
            %"val_1304"<?,?> ⬅️ ::Shape(%"slice_scatter_8") {start=0}
    1611 |  # node_Gather_1611
            %"val_1305"<?,?> ⬅️ ::Gather(%"val_1304", %"val_80") {axis=0}
    1612 |  # node_Range_1612
            %"val_1306"<?,?> ⬅️ ::Range(%"val_80", %"val_1305", %"val_35")
    1613 |  # node_Unsqueeze_1613
            %"val_1307"<?,?> ⬅️ ::Unsqueeze(%"val_1224", %"val_1303")
    1614 |  # node_Unsqueeze_1614
            %"val_1308"<?,?> ⬅️ ::Unsqueeze(%"val_84", %"val_1303")
    1615 |  # node_Unsqueeze_1615
            %"val_1309"<?,?> ⬅️ ::Unsqueeze(%"val_276", %"val_1303")
    1616 |  # node_Slice_1616
            %"val_1310"<?,?> ⬅️ ::Slice(%"val_1306", %"val_1307", %"val_1308", %"val_1303", %"val_1309")
    1617 |  # node_Unsqueeze_1617
            %"val_1311"<?,?> ⬅️ ::Unsqueeze(%"val_1310", %"val_1023")
    1618 |  # node_ScatterND_1618
            %"slice_scatter_11"<FLOAT,[512,8,431]> ⬅️ ::ScatterND(%"slice_scatter_8", %"val_1311", %"slice_scatter_10") {reduction=none}
    1619 |  # node_aten_unsqueeze_1619
            %"unsqueeze_24"<FLOAT,[1,512,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_scatter_11") {dim=0}
    1620 |  # node_Cast_1620
            %"val_1312"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    1621 |  # node_Constant_1621
            %"val_1313"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1622 |  # node_Reshape_1622
            %"val_1314"<?,?> ⬅️ ::Reshape(%"val_1312", %"val_1313") {allowzero=0}
    1623 |  # node_Cast_1623
            %"val_1315"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    1624 |  # node_Constant_1624
            %"val_1316"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1625 |  # node_Reshape_1625
            %"val_1317"<?,?> ⬅️ ::Reshape(%"val_1315", %"val_1316") {allowzero=0}
    1626 |  # node_Cast_1626
            %"val_1318"<?,?> ⬅️ ::Cast(%"val_35") {to=7}
    1627 |  # node_Constant_1627
            %"val_1319"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1628 |  # node_Reshape_1628
            %"val_1320"<?,?> ⬅️ ::Reshape(%"val_1318", %"val_1319") {allowzero=0}
    1629 |  # node_Constant_1629
            %"val_1321"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    1630 |  # node_Slice_1630
            %"slice_39"<FLOAT,[1,512,8,431]> ⬅️ ::Slice(%"unsqueeze_24", %"val_1314", %"val_1317", %"val_1320", %"val_1321")
    1631 |  # node_Cast_1631
            %"_to_copy"<FLOAT,[1,512,8,431]> ⬅️ ::Cast(%"slice_39") {to=FLOAT}
    1632 |  # node_Transpose_1632
            %"permute_9"<FLOAT,[1,431,8,512]> ⬅️ ::Transpose(%"_to_copy") {perm=[0, 3, 2, 1]}
    1633 |  # node_Identity_1633
            %"clone_1"<FLOAT,[1,431,8,512]> ⬅️ ::Identity(%"permute_9")
    1634 |  # node_Constant_1634
            %"val_1322"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([   1, 3448,  512]), name=None)}
    1635 |  # node_Cast_1635
            %"val_1323"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    1636 |  # node_Reshape_1636
            %"_unsafe_view_1"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"clone_1", %"val_1323") {allowzero=0}
    1637 |  # node_Transpose_1637
            %"permute_10"<FLOAT,[1,431,8,512]> ⬅️ ::Transpose(%"view_9") {perm=[0, 3, 2, 1]}
    1638 |  # node_Identity_1638
            %"clone_2"<FLOAT,[1,431,8,512]> ⬅️ ::Identity(%"permute_10")
    1639 |  # node_Cast_1639
            %"val_1324"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    1640 |  # node_Reshape_1640
            %"_unsafe_view_2"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"clone_2", %"val_1324") {allowzero=0}
    1641 |  # node_LayerNormalization_1641
            %"getitem"<FLOAT,[1,3448,512]>, %"native_layer_norm__1"<FLOAT,[1,3448,1]>, %"native_layer_norm__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"_unsafe_view_2", %"crosstransformer.norm_in.weight", %"crosstransformer.norm_in.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1642 |  # node_Mul_1642
            %"mul_29"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"_unsafe_view_1", %"val_14")
    1643 |  # node_aten_add_1643
            %"add_29"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"getitem", %"mul_29") {alpha=1.0}
    1644 |  # node_Transpose_1644
            %"permute_11"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"convolution_49") {perm=[0, 2, 1]}
    1645 |  # node_Constant_1645
            %"val_1325"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(1723), name=None)}
    1646 |  # node_Range_1646
            %"arange_4"<INT64,[1723]> ⬅️ ::Range(%"val_80", %"val_1325", %"val_35")
    1647 |  # node_Constant_1647
            %"val_1326"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([-1,  1,  1]), name=None)}
    1648 |  # node_Cast_1648
            %"val_1327"<?,?> ⬅️ ::Cast(%"val_1326") {to=7}
    1649 |  # node_Reshape_1649
            %"view_10"<INT64,[1723,1,1]> ⬅️ ::Reshape(%"arange_4", %"val_1327") {allowzero=0}
    1650 |  # node_aten_add_1650
            %"add_30"<INT64,[1723,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_10", %"val_80") {alpha=1.0}
    1651 |  # node_Range_1651
            %"arange_5"<INT64,[256]> ⬅️ ::Range(%"val_80", %"val_963", %"val_35")
    1652 |  # node_Constant_1652
            %"val_1328"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([ 1,  1, -1]), name=None)}
    1653 |  # node_Cast_1653
            %"val_1329"<?,?> ⬅️ ::Cast(%"val_1328") {to=7}
    1654 |  # node_Reshape_1654
            %"view_11"<INT64,[1,1,256]> ⬅️ ::Reshape(%"arange_5", %"val_1329") {allowzero=0}
    1655 |  # node_Cast_1655
            %"convert_element_type_default"<FLOAT,[1,1,256]> ⬅️ ::Cast(%"view_11") {to=FLOAT}
    1656 |  # node_Constant_1656
            %"val_1330"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(255), name=None)}
    1657 |  # node_Cast_1657
            %"scalar_tensor_default_1"<FLOAT,[]> ⬅️ ::Cast(%"val_1330") {to=FLOAT}
    1658 |  # node_aten_div_1658
            %"div_2"<FLOAT,[1,1,256]> ⬅️ pkg.onnxscript.torch_lib::aten_div(%"convert_element_type_default", %"scalar_tensor_default_1")
    1659 |  # node_Constant_1659
            %"val_1331"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(10000., dtype=float32), name=None)}
    1660 |  # node_aten_pow_1660
            %"pow_1"<FLOAT,[1,1,256]> ⬅️ pkg.onnxscript.torch_lib::aten_pow(%"val_1331", %"div_2")
    1661 |  # node_Cast_1661
            %"convert_element_type_default_1"<FLOAT,[1723,1,1]> ⬅️ ::Cast(%"add_30") {to=FLOAT}
    1662 |  # node_aten_div_1662
            %"div_3"<FLOAT,[1723,1,256]> ⬅️ pkg.onnxscript.torch_lib::aten_div(%"convert_element_type_default_1", %"pow_1")
    1663 |  # node_Cos_1663
            %"cos_2"<FLOAT,[1723,1,256]> ⬅️ ::Cos(%"div_3")
    1664 |  # node_Sin_1664
            %"sin_2"<FLOAT,[1723,1,256]> ⬅️ ::Sin(%"div_3")
    1665 |  # node_Concat_1665
            %"cat"<FLOAT,[1723,1,512]> ⬅️ ::Concat(%"cos_2", %"sin_2") {axis=-1}
    1666 |  # node_Transpose_1666
            %"permute_12"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"cat") {perm=[1, 0, 2]}
    1667 |  # node_LayerNormalization_1667
            %"getitem_3"<FLOAT,[1,1723,512]>, %"native_layer_norm_1__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_1__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"permute_11", %"crosstransformer.norm_in_t.weight", %"crosstransformer.norm_in_t.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1668 |  # node_Mul_1668
            %"mul_30"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"permute_12", %"val_14")
    1669 |  # node_aten_add_1669
            %"add_31"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"getitem_3", %"mul_30") {alpha=1.0}
    1670 |  # node_LayerNormalization_1670
            %"getitem_6"<FLOAT,[1,3448,512]>, %"native_layer_norm_2__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_2__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"add_29", %"crosstransformer.layers.0.norm1.weight", %"crosstransformer.layers.0.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1671 |  # node_Transpose_1671
            %"transpose_4"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_6") {perm=[1, 0, 2]}
    1672 |  # node_Constant_1672
            %"val_1332"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([3448,  512]), name=None)}
    1673 |  # node_Cast_1673
            %"val_1333"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    1674 |  # node_Reshape_1674
            %"view_12"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_4", %"val_1333") {allowzero=0}
    1675 |  # node_Transpose_1675
            %"t_1"<FLOAT,[512,1536]> ⬅️ ::Transpose(%"crosstransformer.layers.0.self_attn.in_proj_weight") {perm=[1, 0]}
    1676 |  # node_Gemm_1676
            %"addmm"<FLOAT,[3448,1536]> ⬅️ ::Gemm(%"view_12", %"t_1", %"crosstransformer.layers.0.self_attn.in_proj_bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1677 |  # node_Constant_1677
            %"val_1334"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([3448,    1, 1536]), name=None)}
    1678 |  # node_Cast_1678
            %"val_1335"<?,?> ⬅️ ::Cast(%"val_1334") {to=7}
    1679 |  # node_Reshape_1679
            %"view_13"<FLOAT,[3448,1,1536]> ⬅️ ::Reshape(%"addmm", %"val_1335") {allowzero=0}
    1680 |  # node_Constant_1680
            %"val_1336"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([3448,    1,    3,  512]), name=None)}
    1681 |  # node_Cast_1681
            %"val_1337"<?,?> ⬅️ ::Cast(%"val_1336") {to=7}
    1682 |  # node_Reshape_1682
            %"view_14"<FLOAT,[3448,1,3,512]> ⬅️ ::Reshape(%"view_13", %"val_1337") {allowzero=0}
    1683 |  # node_aten_unsqueeze_1683
            %"unsqueeze_25"<FLOAT,[1,3448,1,3,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_14") {dim=0}
    1684 |  # node_Transpose_1684
            %"transpose_5"<FLOAT,[3,3448,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_25") {perm=[3, 1, 2, 0, 4]}
    1685 |  # node_aten_squeeze_dim_1685
            %"squeeze"<FLOAT,[3,3448,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_5") {dim=-2}
    1686 |  # node_Identity_1686
            %"clone_3"<FLOAT,[3,3448,1,512]> ⬅️ ::Identity(%"squeeze")
    1687 |  # node_Gather_1687
            %"select"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_3", %"val_80") {axis=0}
    1688 |  # node_Gather_1688
            %"select_1"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_3", %"val_35") {axis=0}
    1689 |  # node_Gather_1689
            %"select_2"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_3", %"val_276") {axis=0}
    1690 |  # node_Constant_1690
            %"val_1338"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([3448,    8,   64]), name=None)}
    1691 |  # node_Cast_1691
            %"val_1339"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    1692 |  # node_Reshape_1692
            %"view_15"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select", %"val_1339") {allowzero=0}
    1693 |  # node_Transpose_1693
            %"transpose_6"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_15") {perm=[1, 0, 2]}
    1694 |  # node_Cast_1694
            %"val_1340"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    1695 |  # node_Reshape_1695
            %"view_16"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_1", %"val_1340") {allowzero=0}
    1696 |  # node_Transpose_1696
            %"transpose_7"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_16") {perm=[1, 0, 2]}
    1697 |  # node_Cast_1697
            %"val_1341"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    1698 |  # node_Reshape_1698
            %"view_17"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_2", %"val_1341") {allowzero=0}
    1699 |  # node_Transpose_1699
            %"transpose_8"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_17") {perm=[1, 0, 2]}
    1700 |  # node_Constant_1700
            %"val_1342"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([   1,    8, 3448,   64]), name=None)}
    1701 |  # node_Cast_1701
            %"val_1343"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    1702 |  # node_Reshape_1702
            %"view_18"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_6", %"val_1343") {allowzero=0}
    1703 |  # node_Cast_1703
            %"val_1344"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    1704 |  # node_Reshape_1704
            %"view_19"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_7", %"val_1344") {allowzero=0}
    1705 |  # node_Cast_1705
            %"val_1345"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    1706 |  # node_Reshape_1706
            %"view_20"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_8", %"val_1345") {allowzero=0}
    1707 |  # node_Shape_1707
            %"val_1346"<?,?> ⬅️ ::Shape(%"view_18") {start=0}
    1708 |  # node_Constant_1708
            %"val_1347"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1709 |  # node_Gather_1709
            %"val_1348"<?,?> ⬅️ ::Gather(%"val_1346", %"val_1347") {axis=0}
    1710 |  # node_CastLike_1710
            %"val_1349"<?,?> ⬅️ ::CastLike(%"val_1348", %"view_18")
    1711 |  # node_Constant_1711
            %"val_1350"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1712 |  # node_CastLike_1712
            %"val_1351"<?,?> ⬅️ ::CastLike(%"val_1350", %"view_18")
    1713 |  # node_Sqrt_1713
            %"val_1352"<?,?> ⬅️ ::Sqrt(%"val_1349")
    1714 |  # node_Div_1714
            %"val_1353"<?,?> ⬅️ ::Div(%"val_1351", %"val_1352")
    1715 |  # node_CastLike_1715
            %"val_1354"<?,?> ⬅️ ::CastLike(%"val_1353", %"view_18")
    1716 |  # node_Shape_1716
            %"val_1355"<?,?> ⬅️ ::Shape(%"view_19") {start=0}
    1717 |  # node_Constant_1717
            %"val_1356"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    1718 |  # node_Constant_1718
            %"val_1357"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([-1]), name=None)}
    1719 |  # node_Slice_1719
            %"val_1358"<?,?> ⬅️ ::Slice(%"val_1355", %"val_1357", %"val_1356", None, None)
    1720 |  # node_Constant_1720
            %"val_1359"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([-2]), name=None)}
    1721 |  # node_Slice_1721
            %"val_1360"<?,?> ⬅️ ::Slice(%"val_1355", %"val_1359", %"val_1357", None, None)
    1722 |  # node_Constant_1722
            %"val_1361"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    1723 |  # node_Slice_1723
            %"val_1362"<?,?> ⬅️ ::Slice(%"val_1355", %"val_1361", %"val_1359", None, None)
    1724 |  # node_Constant_1724
            %"val_1363"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1725 |  # node_Concat_1725
            %"val_1364"<?,?> ⬅️ ::Concat(%"val_1363", %"val_1360", %"val_1358") {axis=0}
    1726 |  # node_Reshape_1726
            %"val_1365"<?,?> ⬅️ ::Reshape(%"view_19", %"val_1364") {allowzero=0}
    1727 |  # node_Transpose_1727
            %"val_1366"<?,?> ⬅️ ::Transpose(%"val_1365") {perm=[0, 2, 1]}
    1728 |  # node_Concat_1728
            %"val_1367"<?,?> ⬅️ ::Concat(%"val_1362", %"val_1358", %"val_1360") {axis=0}
    1729 |  # node_Reshape_1729
            %"val_1368"<?,?> ⬅️ ::Reshape(%"val_1366", %"val_1367") {allowzero=0}
    1730 |  # node_Sqrt_1730
            %"val_1369"<?,?> ⬅️ ::Sqrt(%"val_1354")
    1731 |  # node_Mul_1731
            %"val_1370"<?,?> ⬅️ ::Mul(%"view_18", %"val_1369")
    1732 |  # node_Sqrt_1732
            %"val_1371"<?,?> ⬅️ ::Sqrt(%"val_1354")
    1733 |  # node_CastLike_1733
            %"val_1372"<?,?> ⬅️ ::CastLike(%"val_1371", %"val_1368")
    1734 |  # node_Mul_1734
            %"val_1373"<?,?> ⬅️ ::Mul(%"val_1368", %"val_1372")
    1735 |  # node_MatMul_1735
            %"val_1374"<?,?> ⬅️ ::MatMul(%"val_1370", %"val_1373")
    1736 |  # node_Softmax_1736
            %"val_1375"<?,?> ⬅️ ::Softmax(%"val_1374") {axis=-1}
    1737 |  # node_Dropout_1737
            %"val_1376"<?,?>, %"val_1377"<?,?> ⬅️ ::Dropout(%"val_1375", %"val_953", None)
    1738 |  # node_MatMul_1738
            %"getitem_9"<FLOAT,[1,8,3448,64]> ⬅️ ::MatMul(%"val_1376", %"view_20")
    1739 |  # node_Shape_1739
            %"val_1378"<?,?> ⬅️ ::Shape(%"view_18") {start=0}
    1740 |  # node_Constant_1740
            %"val_1379"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([0]), name=None)}
    1741 |  # node_Constant_1741
            %"val_1380"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1]), name=None)}
    1742 |  # node_Slice_1742
            %"val_1381"<?,?> ⬅️ ::Slice(%"val_1378", %"val_1379", %"val_1380", None, None)
    1743 |  # node_Constant_1743
            %"val_1382"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([2]), name=None)}
    1744 |  # node_Slice_1744
            %"val_1383"<?,?> ⬅️ ::Slice(%"val_1378", %"val_1380", %"val_1382", None, None)
    1745 |  # node_Slice_1745
            %"val_1384"<?,?> ⬅️ ::Slice(%"val_1378", %"val_1359", %"val_1357", None, None)
    1746 |  # node_Cast_1746
            %"val_1385"<?,?> ⬅️ ::Cast(%"val_1383") {to=1}
    1747 |  # node_Constant_1747
            %"val_1386"<?,?> ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(32., dtype=float32), name=None)}
    1748 |  # node_Div_1748
            %"val_1387"<?,?> ⬅️ ::Div(%"val_1385", %"val_1386")
    1749 |  # node_Ceil_1749
            %"val_1388"<?,?> ⬅️ ::Ceil(%"val_1387")
    1750 |  # node_Mul_1750
            %"val_1389"<?,?> ⬅️ ::Mul(%"val_1388", %"val_1386")
    1751 |  # node_Cast_1751
            %"val_1390"<?,?> ⬅️ ::Cast(%"val_1389") {to=7}
    1752 |  # node_Concat_1752
            %"val_1391"<?,?> ⬅️ ::Concat(%"val_1381", %"val_1384", %"val_1390") {axis=0}
    1753 |  # node_Expand_1753
            %"_scaled_dot_product_flash_attention_for_cpu__1"<FLOAT,[1,8,3448]> ⬅️ ::Expand(%"val_953", %"val_1391")
    1754 |  # node_Transpose_1754
            %"permute_13"<FLOAT,[3448,1,8,64]> ⬅️ ::Transpose(%"getitem_9") {perm=[2, 0, 1, 3]}
    1755 |  # node_Cast_1755
            %"val_1392"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    1756 |  # node_Reshape_1756
            %"view_21"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"permute_13", %"val_1392") {allowzero=0}
    1757 |  # node_Transpose_1757
            %"t_2"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers.0.self_attn.out_proj.weight") {perm=[1, 0]}
    1758 |  # node_Gemm_1758
            %"addmm_1"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_21", %"t_2", %"crosstransformer.layers.0.self_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1759 |  # node_Constant_1759
            %"val_1393"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([3448,    1,  512]), name=None)}
    1760 |  # node_Cast_1760
            %"val_1394"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    1761 |  # node_Reshape_1761
            %"view_22"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_1", %"val_1394") {allowzero=0}
    1762 |  # node_Transpose_1762
            %"transpose_9"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"view_22") {perm=[1, 0, 2]}
    1763 |  # node_Identity_1763
            %"clone_4"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"transpose_9")
    1764 |  # node_Mul_1764
            %"mul_31"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.0.gamma_1.scale", %"clone_4")
    1765 |  # node_aten_add_1765
            %"add_32"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_29", %"mul_31") {alpha=1.0}
    1766 |  # node_LayerNormalization_1766
            %"getitem_11"<FLOAT,[1,3448,512]>, %"native_layer_norm_3__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_3__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"add_32", %"crosstransformer.layers.0.norm2.weight", %"crosstransformer.layers.0.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1767 |  # node_Cast_1767
            %"val_1395"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    1768 |  # node_Reshape_1768
            %"view_23"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"getitem_11", %"val_1395") {allowzero=0}
    1769 |  # node_Transpose_1769
            %"t_3"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers.0.linear1.weight") {perm=[1, 0]}
    1770 |  # node_Gemm_1770
            %"addmm_2"<FLOAT,[3448,2048]> ⬅️ ::Gemm(%"view_23", %"t_3", %"crosstransformer.layers.0.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1771 |  # node_Constant_1771
            %"val_1396"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([   1, 3448, 2048]), name=None)}
    1772 |  # node_Cast_1772
            %"val_1397"<?,?> ⬅️ ::Cast(%"val_1396") {to=7}
    1773 |  # node_Reshape_1773
            %"view_24"<FLOAT,[1,3448,2048]> ⬅️ ::Reshape(%"addmm_2", %"val_1397") {allowzero=0}
    1774 |  # node__aten_gelu_approximate_none_1774
            %"gelu_24"<FLOAT,[1,3448,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_24")
    1775 |  # node_Identity_1775
            %"clone_5"<FLOAT,[1,3448,2048]> ⬅️ ::Identity(%"gelu_24")
    1776 |  # node_Constant_1776
            %"val_1398"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([3448, 2048]), name=None)}
    1777 |  # node_Cast_1777
            %"val_1399"<?,?> ⬅️ ::Cast(%"val_1398") {to=7}
    1778 |  # node_Reshape_1778
            %"view_25"<FLOAT,[3448,2048]> ⬅️ ::Reshape(%"clone_5", %"val_1399") {allowzero=0}
    1779 |  # node_Transpose_1779
            %"t_4"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers.0.linear2.weight") {perm=[1, 0]}
    1780 |  # node_Gemm_1780
            %"addmm_3"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_25", %"t_4", %"crosstransformer.layers.0.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1781 |  # node_Cast_1781
            %"val_1400"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    1782 |  # node_Reshape_1782
            %"view_26"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"addmm_3", %"val_1400") {allowzero=0}
    1783 |  # node_Identity_1783
            %"clone_6"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"view_26")
    1784 |  # node_Mul_1784
            %"mul_32"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.0.gamma_2.scale", %"clone_6")
    1785 |  # node_aten_add_1785
            %"add_33"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_32", %"mul_32") {alpha=1.0}
    1786 |  # node_Transpose_1786
            %"transpose_10"<FLOAT,[1,512,3448]> ⬅️ ::Transpose(%"add_33") {perm=[0, 2, 1]}
    1787 |  # node_Constant_1787
            %"val_1401"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1788 |  # node_Reshape_1788
            %"val_1402"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1401") {allowzero=0}
    1789 |  # node_Constant_1789
            %"val_1403"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1790 |  # node_Concat_1790
            %"val_1404"<?,?> ⬅️ ::Concat(%"val_1403", %"val_1402", %"val_1401") {axis=0}
    1791 |  # node_Reshape_1791
            %"val_1405"<?,?> ⬅️ ::Reshape(%"transpose_10", %"val_1404") {allowzero=0}
    1792 |  # node_Constant_1792
            %"val_1406"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1793 |  # node_CastLike_1793
            %"val_1407"<?,?> ⬅️ ::CastLike(%"val_1406", %"transpose_10")
    1794 |  # node_Expand_1794
            %"val_1408"<?,?> ⬅️ ::Expand(%"val_1407", %"val_1402")
    1795 |  # node_Constant_1795
            %"val_1409"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1796 |  # node_CastLike_1796
            %"val_1410"<?,?> ⬅️ ::CastLike(%"val_1409", %"transpose_10")
    1797 |  # node_Expand_1797
            %"val_1411"<?,?> ⬅️ ::Expand(%"val_1410", %"val_1402")
    1798 |  # node_InstanceNormalization_1798
            %"val_1412"<?,?> ⬅️ ::InstanceNormalization(%"val_1405", %"val_1408", %"val_1411") {epsilon=1e-05}
    1799 |  # node_Shape_1799
            %"val_1413"<?,?> ⬅️ ::Shape(%"transpose_10") {start=0}
    1800 |  # node_Reshape_1800
            %"val_1414"<?,?> ⬅️ ::Reshape(%"val_1412", %"val_1413") {allowzero=0}
    1801 |  # node_Constant_1801
            %"val_1415"<?,?> ⬅️ ::Constant() {value_int=1}
    1802 |  # node_Sub_1802
            %"val_1416"<?,?> ⬅️ ::Sub(%"val_50", %"val_1415")
    1803 |  # node_Range_1803
            %"val_1417"<?,?> ⬅️ ::Range(%"val_1415", %"val_1416", %"val_1415")
    1804 |  # node_Unsqueeze_1804
            %"val_1418"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.0.norm_out.weight", %"val_1417")
    1805 |  # node_Unsqueeze_1805
            %"val_1419"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.0.norm_out.bias", %"val_1417")
    1806 |  # node_CastLike_1806
            %"val_1420"<?,?> ⬅️ ::CastLike(%"val_1418", %"val_1414")
    1807 |  # node_Mul_1807
            %"val_1421"<?,?> ⬅️ ::Mul(%"val_1414", %"val_1420")
    1808 |  # node_CastLike_1808
            %"val_1422"<?,?> ⬅️ ::CastLike(%"val_1419", %"val_1421")
    1809 |  # node_Add_1809
            %"group_norm_32"<FLOAT,[1,512,3448]> ⬅️ ::Add(%"val_1421", %"val_1422")
    1810 |  # node_Transpose_1810
            %"transpose_11"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"group_norm_32") {perm=[0, 2, 1]}
    1811 |  # node_LayerNormalization_1811
            %"getitem_14"<FLOAT,[1,1723,512]>, %"native_layer_norm_4__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_4__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"add_31", %"crosstransformer.layers_t.0.norm1.weight", %"crosstransformer.layers_t.0.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1812 |  # node_Transpose_1812
            %"transpose_12"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_14") {perm=[1, 0, 2]}
    1813 |  # node_Constant_1813
            %"val_1423"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([1723,  512]), name=None)}
    1814 |  # node_Cast_1814
            %"val_1424"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    1815 |  # node_Reshape_1815
            %"view_27"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_12", %"val_1424") {allowzero=0}
    1816 |  # node_Transpose_1816
            %"t_5"<FLOAT,[512,1536]> ⬅️ ::Transpose(%"crosstransformer.layers_t.0.self_attn.in_proj_weight") {perm=[1, 0]}
    1817 |  # node_Gemm_1817
            %"addmm_4"<FLOAT,[1723,1536]> ⬅️ ::Gemm(%"view_27", %"t_5", %"crosstransformer.layers_t.0.self_attn.in_proj_bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1818 |  # node_Constant_1818
            %"val_1425"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1723,    1, 1536]), name=None)}
    1819 |  # node_Cast_1819
            %"val_1426"<?,?> ⬅️ ::Cast(%"val_1425") {to=7}
    1820 |  # node_Reshape_1820
            %"view_28"<FLOAT,[1723,1,1536]> ⬅️ ::Reshape(%"addmm_4", %"val_1426") {allowzero=0}
    1821 |  # node_Constant_1821
            %"val_1427"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([1723,    1,    3,  512]), name=None)}
    1822 |  # node_Cast_1822
            %"val_1428"<?,?> ⬅️ ::Cast(%"val_1427") {to=7}
    1823 |  # node_Reshape_1823
            %"view_29"<FLOAT,[1723,1,3,512]> ⬅️ ::Reshape(%"view_28", %"val_1428") {allowzero=0}
    1824 |  # node_aten_unsqueeze_1824
            %"unsqueeze_26"<FLOAT,[1,1723,1,3,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_29") {dim=0}
    1825 |  # node_Transpose_1825
            %"transpose_13"<FLOAT,[3,1723,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_26") {perm=[3, 1, 2, 0, 4]}
    1826 |  # node_aten_squeeze_dim_1826
            %"squeeze_1"<FLOAT,[3,1723,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_13") {dim=-2}
    1827 |  # node_Identity_1827
            %"clone_7"<FLOAT,[3,1723,1,512]> ⬅️ ::Identity(%"squeeze_1")
    1828 |  # node_Gather_1828
            %"select_3"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_7", %"val_80") {axis=0}
    1829 |  # node_Gather_1829
            %"select_4"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_7", %"val_35") {axis=0}
    1830 |  # node_Gather_1830
            %"select_5"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_7", %"val_276") {axis=0}
    1831 |  # node_Constant_1831
            %"val_1429"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1723,    8,   64]), name=None)}
    1832 |  # node_Cast_1832
            %"val_1430"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    1833 |  # node_Reshape_1833
            %"view_30"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_3", %"val_1430") {allowzero=0}
    1834 |  # node_Transpose_1834
            %"transpose_14"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_30") {perm=[1, 0, 2]}
    1835 |  # node_Cast_1835
            %"val_1431"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    1836 |  # node_Reshape_1836
            %"view_31"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_4", %"val_1431") {allowzero=0}
    1837 |  # node_Transpose_1837
            %"transpose_15"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_31") {perm=[1, 0, 2]}
    1838 |  # node_Cast_1838
            %"val_1432"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    1839 |  # node_Reshape_1839
            %"view_32"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_5", %"val_1432") {allowzero=0}
    1840 |  # node_Transpose_1840
            %"transpose_16"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_32") {perm=[1, 0, 2]}
    1841 |  # node_Constant_1841
            %"val_1433"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([   1,    8, 1723,   64]), name=None)}
    1842 |  # node_Cast_1842
            %"val_1434"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    1843 |  # node_Reshape_1843
            %"view_33"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_14", %"val_1434") {allowzero=0}
    1844 |  # node_Cast_1844
            %"val_1435"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    1845 |  # node_Reshape_1845
            %"view_34"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_15", %"val_1435") {allowzero=0}
    1846 |  # node_Cast_1846
            %"val_1436"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    1847 |  # node_Reshape_1847
            %"view_35"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_16", %"val_1436") {allowzero=0}
    1848 |  # node_Shape_1848
            %"val_1437"<?,?> ⬅️ ::Shape(%"view_33") {start=0}
    1849 |  # node_Constant_1849
            %"val_1438"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1850 |  # node_Gather_1850
            %"val_1439"<?,?> ⬅️ ::Gather(%"val_1437", %"val_1438") {axis=0}
    1851 |  # node_CastLike_1851
            %"val_1440"<?,?> ⬅️ ::CastLike(%"val_1439", %"view_33")
    1852 |  # node_Constant_1852
            %"val_1441"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1853 |  # node_CastLike_1853
            %"val_1442"<?,?> ⬅️ ::CastLike(%"val_1441", %"view_33")
    1854 |  # node_Sqrt_1854
            %"val_1443"<?,?> ⬅️ ::Sqrt(%"val_1440")
    1855 |  # node_Div_1855
            %"val_1444"<?,?> ⬅️ ::Div(%"val_1442", %"val_1443")
    1856 |  # node_CastLike_1856
            %"val_1445"<?,?> ⬅️ ::CastLike(%"val_1444", %"view_33")
    1857 |  # node_Shape_1857
            %"val_1446"<?,?> ⬅️ ::Shape(%"view_34") {start=0}
    1858 |  # node_Constant_1858
            %"val_1447"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    1859 |  # node_Slice_1859
            %"val_1448"<?,?> ⬅️ ::Slice(%"val_1446", %"val_1357", %"val_1447", None, None)
    1860 |  # node_Slice_1860
            %"val_1449"<?,?> ⬅️ ::Slice(%"val_1446", %"val_1359", %"val_1357", None, None)
    1861 |  # node_Constant_1861
            %"val_1450"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    1862 |  # node_Slice_1862
            %"val_1451"<?,?> ⬅️ ::Slice(%"val_1446", %"val_1450", %"val_1359", None, None)
    1863 |  # node_Constant_1863
            %"val_1452"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1864 |  # node_Concat_1864
            %"val_1453"<?,?> ⬅️ ::Concat(%"val_1452", %"val_1449", %"val_1448") {axis=0}
    1865 |  # node_Reshape_1865
            %"val_1454"<?,?> ⬅️ ::Reshape(%"view_34", %"val_1453") {allowzero=0}
    1866 |  # node_Transpose_1866
            %"val_1455"<?,?> ⬅️ ::Transpose(%"val_1454") {perm=[0, 2, 1]}
    1867 |  # node_Concat_1867
            %"val_1456"<?,?> ⬅️ ::Concat(%"val_1451", %"val_1448", %"val_1449") {axis=0}
    1868 |  # node_Reshape_1868
            %"val_1457"<?,?> ⬅️ ::Reshape(%"val_1455", %"val_1456") {allowzero=0}
    1869 |  # node_Sqrt_1869
            %"val_1458"<?,?> ⬅️ ::Sqrt(%"val_1445")
    1870 |  # node_Mul_1870
            %"val_1459"<?,?> ⬅️ ::Mul(%"view_33", %"val_1458")
    1871 |  # node_Sqrt_1871
            %"val_1460"<?,?> ⬅️ ::Sqrt(%"val_1445")
    1872 |  # node_CastLike_1872
            %"val_1461"<?,?> ⬅️ ::CastLike(%"val_1460", %"val_1457")
    1873 |  # node_Mul_1873
            %"val_1462"<?,?> ⬅️ ::Mul(%"val_1457", %"val_1461")
    1874 |  # node_MatMul_1874
            %"val_1463"<?,?> ⬅️ ::MatMul(%"val_1459", %"val_1462")
    1875 |  # node_Softmax_1875
            %"val_1464"<?,?> ⬅️ ::Softmax(%"val_1463") {axis=-1}
    1876 |  # node_Dropout_1876
            %"val_1465"<?,?>, %"val_1466"<?,?> ⬅️ ::Dropout(%"val_1464", %"val_953", None)
    1877 |  # node_MatMul_1877
            %"getitem_17"<FLOAT,[1,8,1723,64]> ⬅️ ::MatMul(%"val_1465", %"view_35")
    1878 |  # node_Shape_1878
            %"val_1467"<?,?> ⬅️ ::Shape(%"view_33") {start=0}
    1879 |  # node_Slice_1879
            %"val_1468"<?,?> ⬅️ ::Slice(%"val_1467", %"val_1379", %"val_1380", None, None)
    1880 |  # node_Slice_1880
            %"val_1469"<?,?> ⬅️ ::Slice(%"val_1467", %"val_1380", %"val_1382", None, None)
    1881 |  # node_Slice_1881
            %"val_1470"<?,?> ⬅️ ::Slice(%"val_1467", %"val_1359", %"val_1357", None, None)
    1882 |  # node_Cast_1882
            %"val_1471"<?,?> ⬅️ ::Cast(%"val_1469") {to=1}
    1883 |  # node_Div_1883
            %"val_1472"<?,?> ⬅️ ::Div(%"val_1471", %"val_1386")
    1884 |  # node_Ceil_1884
            %"val_1473"<?,?> ⬅️ ::Ceil(%"val_1472")
    1885 |  # node_Mul_1885
            %"val_1474"<?,?> ⬅️ ::Mul(%"val_1473", %"val_1386")
    1886 |  # node_Cast_1886
            %"val_1475"<?,?> ⬅️ ::Cast(%"val_1474") {to=7}
    1887 |  # node_Concat_1887
            %"val_1476"<?,?> ⬅️ ::Concat(%"val_1468", %"val_1470", %"val_1475") {axis=0}
    1888 |  # node_Expand_1888
            %"_scaled_dot_product_flash_attention_for_cpu_1__1"<FLOAT,[1,8,1723]> ⬅️ ::Expand(%"val_953", %"val_1476")
    1889 |  # node_Transpose_1889
            %"permute_14"<FLOAT,[1723,1,8,64]> ⬅️ ::Transpose(%"getitem_17") {perm=[2, 0, 1, 3]}
    1890 |  # node_Cast_1890
            %"val_1477"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    1891 |  # node_Reshape_1891
            %"view_36"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"permute_14", %"val_1477") {allowzero=0}
    1892 |  # node_Transpose_1892
            %"t_6"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.0.self_attn.out_proj.weight") {perm=[1, 0]}
    1893 |  # node_Gemm_1893
            %"addmm_5"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_36", %"t_6", %"crosstransformer.layers_t.0.self_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1894 |  # node_Constant_1894
            %"val_1478"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1723,    1,  512]), name=None)}
    1895 |  # node_Cast_1895
            %"val_1479"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    1896 |  # node_Reshape_1896
            %"view_37"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_5", %"val_1479") {allowzero=0}
    1897 |  # node_Transpose_1897
            %"transpose_17"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"view_37") {perm=[1, 0, 2]}
    1898 |  # node_Identity_1898
            %"clone_8"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"transpose_17")
    1899 |  # node_Mul_1899
            %"mul_33"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.0.gamma_1.scale", %"clone_8")
    1900 |  # node_aten_add_1900
            %"add_34"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_31", %"mul_33") {alpha=1.0}
    1901 |  # node_LayerNormalization_1901
            %"getitem_19"<FLOAT,[1,1723,512]>, %"native_layer_norm_5__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_5__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"add_34", %"crosstransformer.layers_t.0.norm2.weight", %"crosstransformer.layers_t.0.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1902 |  # node_Cast_1902
            %"val_1480"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    1903 |  # node_Reshape_1903
            %"view_38"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"getitem_19", %"val_1480") {allowzero=0}
    1904 |  # node_Transpose_1904
            %"t_7"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers_t.0.linear1.weight") {perm=[1, 0]}
    1905 |  # node_Gemm_1905
            %"addmm_6"<FLOAT,[1723,2048]> ⬅️ ::Gemm(%"view_38", %"t_7", %"crosstransformer.layers_t.0.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1906 |  # node_Constant_1906
            %"val_1481"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([   1, 1723, 2048]), name=None)}
    1907 |  # node_Cast_1907
            %"val_1482"<?,?> ⬅️ ::Cast(%"val_1481") {to=7}
    1908 |  # node_Reshape_1908
            %"view_39"<FLOAT,[1,1723,2048]> ⬅️ ::Reshape(%"addmm_6", %"val_1482") {allowzero=0}
    1909 |  # node__aten_gelu_approximate_none_1909
            %"gelu_25"<FLOAT,[1,1723,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_39")
    1910 |  # node_Identity_1910
            %"clone_9"<FLOAT,[1,1723,2048]> ⬅️ ::Identity(%"gelu_25")
    1911 |  # node_Constant_1911
            %"val_1483"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([1723, 2048]), name=None)}
    1912 |  # node_Cast_1912
            %"val_1484"<?,?> ⬅️ ::Cast(%"val_1483") {to=7}
    1913 |  # node_Reshape_1913
            %"view_40"<FLOAT,[1723,2048]> ⬅️ ::Reshape(%"clone_9", %"val_1484") {allowzero=0}
    1914 |  # node_Transpose_1914
            %"t_8"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.0.linear2.weight") {perm=[1, 0]}
    1915 |  # node_Gemm_1915
            %"addmm_7"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_40", %"t_8", %"crosstransformer.layers_t.0.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1916 |  # node_Constant_1916
            %"val_1485"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([   1, 1723,  512]), name=None)}
    1917 |  # node_Cast_1917
            %"val_1486"<?,?> ⬅️ ::Cast(%"val_1485") {to=7}
    1918 |  # node_Reshape_1918
            %"view_41"<FLOAT,[1,1723,512]> ⬅️ ::Reshape(%"addmm_7", %"val_1486") {allowzero=0}
    1919 |  # node_Identity_1919
            %"clone_10"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"view_41")
    1920 |  # node_Mul_1920
            %"mul_34"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.0.gamma_2.scale", %"clone_10")
    1921 |  # node_aten_add_1921
            %"add_35"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_34", %"mul_34") {alpha=1.0}
    1922 |  # node_Transpose_1922
            %"transpose_18"<FLOAT,[1,512,1723]> ⬅️ ::Transpose(%"add_35") {perm=[0, 2, 1]}
    1923 |  # node_Constant_1923
            %"val_1487"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1924 |  # node_Reshape_1924
            %"val_1488"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1487") {allowzero=0}
    1925 |  # node_Constant_1925
            %"val_1489"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    1926 |  # node_Concat_1926
            %"val_1490"<?,?> ⬅️ ::Concat(%"val_1489", %"val_1488", %"val_1487") {axis=0}
    1927 |  # node_Reshape_1927
            %"val_1491"<?,?> ⬅️ ::Reshape(%"transpose_18", %"val_1490") {allowzero=0}
    1928 |  # node_Constant_1928
            %"val_1492"<?,?> ⬅️ ::Constant() {value_float=1.0}
    1929 |  # node_CastLike_1929
            %"val_1493"<?,?> ⬅️ ::CastLike(%"val_1492", %"transpose_18")
    1930 |  # node_Expand_1930
            %"val_1494"<?,?> ⬅️ ::Expand(%"val_1493", %"val_1488")
    1931 |  # node_Constant_1931
            %"val_1495"<?,?> ⬅️ ::Constant() {value_float=0.0}
    1932 |  # node_CastLike_1932
            %"val_1496"<?,?> ⬅️ ::CastLike(%"val_1495", %"transpose_18")
    1933 |  # node_Expand_1933
            %"val_1497"<?,?> ⬅️ ::Expand(%"val_1496", %"val_1488")
    1934 |  # node_InstanceNormalization_1934
            %"val_1498"<?,?> ⬅️ ::InstanceNormalization(%"val_1491", %"val_1494", %"val_1497") {epsilon=1e-05}
    1935 |  # node_Shape_1935
            %"val_1499"<?,?> ⬅️ ::Shape(%"transpose_18") {start=0}
    1936 |  # node_Reshape_1936
            %"val_1500"<?,?> ⬅️ ::Reshape(%"val_1498", %"val_1499") {allowzero=0}
    1937 |  # node_Constant_1937
            %"val_1501"<?,?> ⬅️ ::Constant() {value_int=1}
    1938 |  # node_Sub_1938
            %"val_1502"<?,?> ⬅️ ::Sub(%"val_50", %"val_1501")
    1939 |  # node_Range_1939
            %"val_1503"<?,?> ⬅️ ::Range(%"val_1501", %"val_1502", %"val_1501")
    1940 |  # node_Unsqueeze_1940
            %"val_1504"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.0.norm_out.weight", %"val_1503")
    1941 |  # node_Unsqueeze_1941
            %"val_1505"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.0.norm_out.bias", %"val_1503")
    1942 |  # node_CastLike_1942
            %"val_1506"<?,?> ⬅️ ::CastLike(%"val_1504", %"val_1500")
    1943 |  # node_Mul_1943
            %"val_1507"<?,?> ⬅️ ::Mul(%"val_1500", %"val_1506")
    1944 |  # node_CastLike_1944
            %"val_1508"<?,?> ⬅️ ::CastLike(%"val_1505", %"val_1507")
    1945 |  # node_Add_1945
            %"group_norm_33"<FLOAT,[1,512,1723]> ⬅️ ::Add(%"val_1507", %"val_1508")
    1946 |  # node_Transpose_1946
            %"transpose_19"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"group_norm_33") {perm=[0, 2, 1]}
    1947 |  # node_LayerNormalization_1947
            %"getitem_22"<FLOAT,[1,3448,512]>, %"native_layer_norm_6__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_6__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"transpose_11", %"crosstransformer.layers.1.norm1.weight", %"crosstransformer.layers.1.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1948 |  # node_LayerNormalization_1948
            %"getitem_25"<FLOAT,[1,1723,512]>, %"native_layer_norm_7__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_7__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"transpose_19", %"crosstransformer.layers.1.norm2.weight", %"crosstransformer.layers.1.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    1949 |  # node_Transpose_1949
            %"transpose_20"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_22") {perm=[1, 0, 2]}
    1950 |  # node_Transpose_1950
            %"transpose_21"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_25") {perm=[1, 0, 2]}
    1951 |  # node_Constant_1951
            %"val_1509"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[2]>(array([ 512, 1024]), name=None)}
    1952 |  # node_aten_split_with_sizes_1952
            %"split_with_sizes"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers.1.cross_attn.in_proj_weight", %"val_1509") {dim=0}
    1953 |  # node_aten_getitem_1953
            %"getitem_28"<FLOAT,[512,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes", %"val_80")
    1954 |  # node_aten_getitem_1954
            %"getitem_29"<FLOAT,[1024,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes", %"val_35")
    1955 |  # node_aten_split_with_sizes_1955
            %"split_with_sizes_1"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers.1.cross_attn.in_proj_bias", %"val_1509") {dim=0}
    1956 |  # node_aten_getitem_1956
            %"getitem_30"<FLOAT,[512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_1", %"val_80")
    1957 |  # node_aten_getitem_1957
            %"getitem_31"<FLOAT,[1024]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_1", %"val_35")
    1958 |  # node_Cast_1958
            %"val_1510"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    1959 |  # node_Reshape_1959
            %"view_42"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_20", %"val_1510") {allowzero=0}
    1960 |  # node_Transpose_1960
            %"t_9"<FLOAT,[512,512]> ⬅️ ::Transpose(%"getitem_28") {perm=[1, 0]}
    1961 |  # node_Gemm_1961
            %"addmm_8"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_42", %"t_9", %"getitem_30") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1962 |  # node_Cast_1962
            %"val_1511"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    1963 |  # node_Reshape_1963
            %"view_43"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_8", %"val_1511") {allowzero=0}
    1964 |  # node_Cast_1964
            %"val_1512"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    1965 |  # node_Reshape_1965
            %"view_44"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_21", %"val_1512") {allowzero=0}
    1966 |  # node_Transpose_1966
            %"t_10"<FLOAT,[512,1024]> ⬅️ ::Transpose(%"getitem_29") {perm=[1, 0]}
    1967 |  # node_Gemm_1967
            %"addmm_9"<FLOAT,[1723,1024]> ⬅️ ::Gemm(%"view_44", %"t_10", %"getitem_31") {alpha=1.0, beta=1.0, transA=0, transB=0}
    1968 |  # node_Constant_1968
            %"val_1513"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([1723,    1, 1024]), name=None)}
    1969 |  # node_Cast_1969
            %"val_1514"<?,?> ⬅️ ::Cast(%"val_1513") {to=7}
    1970 |  # node_Reshape_1970
            %"view_45"<FLOAT,[1723,1,1024]> ⬅️ ::Reshape(%"addmm_9", %"val_1514") {allowzero=0}
    1971 |  # node_Constant_1971
            %"val_1515"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([1723,    1,    2,  512]), name=None)}
    1972 |  # node_Cast_1972
            %"val_1516"<?,?> ⬅️ ::Cast(%"val_1515") {to=7}
    1973 |  # node_Reshape_1973
            %"view_46"<FLOAT,[1723,1,2,512]> ⬅️ ::Reshape(%"view_45", %"val_1516") {allowzero=0}
    1974 |  # node_aten_unsqueeze_1974
            %"unsqueeze_27"<FLOAT,[1,1723,1,2,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_46") {dim=0}
    1975 |  # node_Transpose_1975
            %"transpose_22"<FLOAT,[2,1723,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_27") {perm=[3, 1, 2, 0, 4]}
    1976 |  # node_aten_squeeze_dim_1976
            %"squeeze_2"<FLOAT,[2,1723,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_22") {dim=-2}
    1977 |  # node_Identity_1977
            %"clone_11"<FLOAT,[2,1723,1,512]> ⬅️ ::Identity(%"squeeze_2")
    1978 |  # node_Gather_1978
            %"select_6"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_11", %"val_80") {axis=0}
    1979 |  # node_Gather_1979
            %"select_7"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_11", %"val_35") {axis=0}
    1980 |  # node_Cast_1980
            %"val_1517"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    1981 |  # node_Reshape_1981
            %"view_47"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"view_43", %"val_1517") {allowzero=0}
    1982 |  # node_Transpose_1982
            %"transpose_23"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_47") {perm=[1, 0, 2]}
    1983 |  # node_Cast_1983
            %"val_1518"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    1984 |  # node_Reshape_1984
            %"view_48"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_6", %"val_1518") {allowzero=0}
    1985 |  # node_Transpose_1985
            %"transpose_24"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_48") {perm=[1, 0, 2]}
    1986 |  # node_Cast_1986
            %"val_1519"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    1987 |  # node_Reshape_1987
            %"view_49"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_7", %"val_1519") {allowzero=0}
    1988 |  # node_Transpose_1988
            %"transpose_25"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_49") {perm=[1, 0, 2]}
    1989 |  # node_Cast_1989
            %"val_1520"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    1990 |  # node_Reshape_1990
            %"view_50"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_23", %"val_1520") {allowzero=0}
    1991 |  # node_Cast_1991
            %"val_1521"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    1992 |  # node_Reshape_1992
            %"view_51"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_24", %"val_1521") {allowzero=0}
    1993 |  # node_Cast_1993
            %"val_1522"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    1994 |  # node_Reshape_1994
            %"view_52"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_25", %"val_1522") {allowzero=0}
    1995 |  # node_Shape_1995
            %"val_1523"<?,?> ⬅️ ::Shape(%"view_50") {start=0}
    1996 |  # node_Constant_1996
            %"val_1524"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    1997 |  # node_Gather_1997
            %"val_1525"<?,?> ⬅️ ::Gather(%"val_1523", %"val_1524") {axis=0}
    1998 |  # node_CastLike_1998
            %"val_1526"<?,?> ⬅️ ::CastLike(%"val_1525", %"view_50")
    1999 |  # node_Constant_1999
            %"val_1527"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2000 |  # node_CastLike_2000
            %"val_1528"<?,?> ⬅️ ::CastLike(%"val_1527", %"view_50")
    2001 |  # node_Sqrt_2001
            %"val_1529"<?,?> ⬅️ ::Sqrt(%"val_1526")
    2002 |  # node_Div_2002
            %"val_1530"<?,?> ⬅️ ::Div(%"val_1528", %"val_1529")
    2003 |  # node_CastLike_2003
            %"val_1531"<?,?> ⬅️ ::CastLike(%"val_1530", %"view_50")
    2004 |  # node_Shape_2004
            %"val_1532"<?,?> ⬅️ ::Shape(%"view_51") {start=0}
    2005 |  # node_Constant_2005
            %"val_1533"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2006 |  # node_Slice_2006
            %"val_1534"<?,?> ⬅️ ::Slice(%"val_1532", %"val_1357", %"val_1533", None, None)
    2007 |  # node_Slice_2007
            %"val_1535"<?,?> ⬅️ ::Slice(%"val_1532", %"val_1359", %"val_1357", None, None)
    2008 |  # node_Constant_2008
            %"val_1536"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2009 |  # node_Slice_2009
            %"val_1537"<?,?> ⬅️ ::Slice(%"val_1532", %"val_1536", %"val_1359", None, None)
    2010 |  # node_Constant_2010
            %"val_1538"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2011 |  # node_Concat_2011
            %"val_1539"<?,?> ⬅️ ::Concat(%"val_1538", %"val_1535", %"val_1534") {axis=0}
    2012 |  # node_Reshape_2012
            %"val_1540"<?,?> ⬅️ ::Reshape(%"view_51", %"val_1539") {allowzero=0}
    2013 |  # node_Transpose_2013
            %"val_1541"<?,?> ⬅️ ::Transpose(%"val_1540") {perm=[0, 2, 1]}
    2014 |  # node_Concat_2014
            %"val_1542"<?,?> ⬅️ ::Concat(%"val_1537", %"val_1534", %"val_1535") {axis=0}
    2015 |  # node_Reshape_2015
            %"val_1543"<?,?> ⬅️ ::Reshape(%"val_1541", %"val_1542") {allowzero=0}
    2016 |  # node_Sqrt_2016
            %"val_1544"<?,?> ⬅️ ::Sqrt(%"val_1531")
    2017 |  # node_Mul_2017
            %"val_1545"<?,?> ⬅️ ::Mul(%"view_50", %"val_1544")
    2018 |  # node_Sqrt_2018
            %"val_1546"<?,?> ⬅️ ::Sqrt(%"val_1531")
    2019 |  # node_CastLike_2019
            %"val_1547"<?,?> ⬅️ ::CastLike(%"val_1546", %"val_1543")
    2020 |  # node_Mul_2020
            %"val_1548"<?,?> ⬅️ ::Mul(%"val_1543", %"val_1547")
    2021 |  # node_MatMul_2021
            %"val_1549"<?,?> ⬅️ ::MatMul(%"val_1545", %"val_1548")
    2022 |  # node_Softmax_2022
            %"val_1550"<?,?> ⬅️ ::Softmax(%"val_1549") {axis=-1}
    2023 |  # node_Dropout_2023
            %"val_1551"<?,?>, %"val_1552"<?,?> ⬅️ ::Dropout(%"val_1550", %"val_953", None)
    2024 |  # node_MatMul_2024
            %"getitem_32"<FLOAT,[1,8,3448,64]> ⬅️ ::MatMul(%"val_1551", %"view_52")
    2025 |  # node_Shape_2025
            %"val_1553"<?,?> ⬅️ ::Shape(%"view_50") {start=0}
    2026 |  # node_Slice_2026
            %"val_1554"<?,?> ⬅️ ::Slice(%"val_1553", %"val_1379", %"val_1380", None, None)
    2027 |  # node_Slice_2027
            %"val_1555"<?,?> ⬅️ ::Slice(%"val_1553", %"val_1380", %"val_1382", None, None)
    2028 |  # node_Slice_2028
            %"val_1556"<?,?> ⬅️ ::Slice(%"val_1553", %"val_1359", %"val_1357", None, None)
    2029 |  # node_Cast_2029
            %"val_1557"<?,?> ⬅️ ::Cast(%"val_1555") {to=1}
    2030 |  # node_Div_2030
            %"val_1558"<?,?> ⬅️ ::Div(%"val_1557", %"val_1386")
    2031 |  # node_Ceil_2031
            %"val_1559"<?,?> ⬅️ ::Ceil(%"val_1558")
    2032 |  # node_Mul_2032
            %"val_1560"<?,?> ⬅️ ::Mul(%"val_1559", %"val_1386")
    2033 |  # node_Cast_2033
            %"val_1561"<?,?> ⬅️ ::Cast(%"val_1560") {to=7}
    2034 |  # node_Concat_2034
            %"val_1562"<?,?> ⬅️ ::Concat(%"val_1554", %"val_1556", %"val_1561") {axis=0}
    2035 |  # node_Expand_2035
            %"_scaled_dot_product_flash_attention_for_cpu_2__1"<FLOAT,[1,8,3448]> ⬅️ ::Expand(%"val_953", %"val_1562")
    2036 |  # node_Transpose_2036
            %"permute_15"<FLOAT,[3448,1,8,64]> ⬅️ ::Transpose(%"getitem_32") {perm=[2, 0, 1, 3]}
    2037 |  # node_Cast_2037
            %"val_1563"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2038 |  # node_Reshape_2038
            %"view_53"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"permute_15", %"val_1563") {allowzero=0}
    2039 |  # node_Transpose_2039
            %"t_11"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers.1.cross_attn.out_proj.weight") {perm=[1, 0]}
    2040 |  # node_Gemm_2040
            %"addmm_10"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_53", %"t_11", %"crosstransformer.layers.1.cross_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2041 |  # node_Cast_2041
            %"val_1564"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    2042 |  # node_Reshape_2042
            %"view_54"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_10", %"val_1564") {allowzero=0}
    2043 |  # node_Transpose_2043
            %"transpose_26"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"view_54") {perm=[1, 0, 2]}
    2044 |  # node_Identity_2044
            %"clone_12"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"transpose_26")
    2045 |  # node_Mul_2045
            %"mul_35"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.1.gamma_1.scale", %"clone_12")
    2046 |  # node_aten_add_2046
            %"add_36"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_11", %"mul_35") {alpha=1.0}
    2047 |  # node_LayerNormalization_2047
            %"getitem_34"<FLOAT,[1,3448,512]>, %"native_layer_norm_8__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_8__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"add_36", %"crosstransformer.layers.1.norm3.weight", %"crosstransformer.layers.1.norm3.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2048 |  # node_Cast_2048
            %"val_1565"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2049 |  # node_Reshape_2049
            %"view_55"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"getitem_34", %"val_1565") {allowzero=0}
    2050 |  # node_Transpose_2050
            %"t_12"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers.1.linear1.weight") {perm=[1, 0]}
    2051 |  # node_Gemm_2051
            %"addmm_11"<FLOAT,[3448,2048]> ⬅️ ::Gemm(%"view_55", %"t_12", %"crosstransformer.layers.1.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2052 |  # node_Cast_2052
            %"val_1566"<?,?> ⬅️ ::Cast(%"val_1396") {to=7}
    2053 |  # node_Reshape_2053
            %"view_56"<FLOAT,[1,3448,2048]> ⬅️ ::Reshape(%"addmm_11", %"val_1566") {allowzero=0}
    2054 |  # node__aten_gelu_approximate_none_2054
            %"gelu_26"<FLOAT,[1,3448,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_56")
    2055 |  # node_Identity_2055
            %"clone_13"<FLOAT,[1,3448,2048]> ⬅️ ::Identity(%"gelu_26")
    2056 |  # node_Cast_2056
            %"val_1567"<?,?> ⬅️ ::Cast(%"val_1398") {to=7}
    2057 |  # node_Reshape_2057
            %"view_57"<FLOAT,[3448,2048]> ⬅️ ::Reshape(%"clone_13", %"val_1567") {allowzero=0}
    2058 |  # node_Transpose_2058
            %"t_13"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers.1.linear2.weight") {perm=[1, 0]}
    2059 |  # node_Gemm_2059
            %"addmm_12"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_57", %"t_13", %"crosstransformer.layers.1.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2060 |  # node_Cast_2060
            %"val_1568"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    2061 |  # node_Reshape_2061
            %"view_58"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"addmm_12", %"val_1568") {allowzero=0}
    2062 |  # node_Identity_2062
            %"clone_14"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"view_58")
    2063 |  # node_Mul_2063
            %"mul_36"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.1.gamma_2.scale", %"clone_14")
    2064 |  # node_aten_add_2064
            %"add_37"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_36", %"mul_36") {alpha=1.0}
    2065 |  # node_Transpose_2065
            %"transpose_27"<FLOAT,[1,512,3448]> ⬅️ ::Transpose(%"add_37") {perm=[0, 2, 1]}
    2066 |  # node_Constant_2066
            %"val_1569"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2067 |  # node_Reshape_2067
            %"val_1570"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1569") {allowzero=0}
    2068 |  # node_Constant_2068
            %"val_1571"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2069 |  # node_Concat_2069
            %"val_1572"<?,?> ⬅️ ::Concat(%"val_1571", %"val_1570", %"val_1569") {axis=0}
    2070 |  # node_Reshape_2070
            %"val_1573"<?,?> ⬅️ ::Reshape(%"transpose_27", %"val_1572") {allowzero=0}
    2071 |  # node_Constant_2071
            %"val_1574"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2072 |  # node_CastLike_2072
            %"val_1575"<?,?> ⬅️ ::CastLike(%"val_1574", %"transpose_27")
    2073 |  # node_Expand_2073
            %"val_1576"<?,?> ⬅️ ::Expand(%"val_1575", %"val_1570")
    2074 |  # node_Constant_2074
            %"val_1577"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2075 |  # node_CastLike_2075
            %"val_1578"<?,?> ⬅️ ::CastLike(%"val_1577", %"transpose_27")
    2076 |  # node_Expand_2076
            %"val_1579"<?,?> ⬅️ ::Expand(%"val_1578", %"val_1570")
    2077 |  # node_InstanceNormalization_2077
            %"val_1580"<?,?> ⬅️ ::InstanceNormalization(%"val_1573", %"val_1576", %"val_1579") {epsilon=1e-05}
    2078 |  # node_Shape_2078
            %"val_1581"<?,?> ⬅️ ::Shape(%"transpose_27") {start=0}
    2079 |  # node_Reshape_2079
            %"val_1582"<?,?> ⬅️ ::Reshape(%"val_1580", %"val_1581") {allowzero=0}
    2080 |  # node_Constant_2080
            %"val_1583"<?,?> ⬅️ ::Constant() {value_int=1}
    2081 |  # node_Sub_2081
            %"val_1584"<?,?> ⬅️ ::Sub(%"val_50", %"val_1583")
    2082 |  # node_Range_2082
            %"val_1585"<?,?> ⬅️ ::Range(%"val_1583", %"val_1584", %"val_1583")
    2083 |  # node_Unsqueeze_2083
            %"val_1586"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.1.norm_out.weight", %"val_1585")
    2084 |  # node_Unsqueeze_2084
            %"val_1587"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.1.norm_out.bias", %"val_1585")
    2085 |  # node_CastLike_2085
            %"val_1588"<?,?> ⬅️ ::CastLike(%"val_1586", %"val_1582")
    2086 |  # node_Mul_2086
            %"val_1589"<?,?> ⬅️ ::Mul(%"val_1582", %"val_1588")
    2087 |  # node_CastLike_2087
            %"val_1590"<?,?> ⬅️ ::CastLike(%"val_1587", %"val_1589")
    2088 |  # node_Add_2088
            %"group_norm_34"<FLOAT,[1,512,3448]> ⬅️ ::Add(%"val_1589", %"val_1590")
    2089 |  # node_Transpose_2089
            %"transpose_28"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"group_norm_34") {perm=[0, 2, 1]}
    2090 |  # node_LayerNormalization_2090
            %"getitem_37"<FLOAT,[1,1723,512]>, %"native_layer_norm_9__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_9__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"transpose_19", %"crosstransformer.layers_t.1.norm1.weight", %"crosstransformer.layers_t.1.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2091 |  # node_LayerNormalization_2091
            %"getitem_40"<FLOAT,[1,3448,512]>, %"native_layer_norm_10__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_10__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"transpose_11", %"crosstransformer.layers_t.1.norm2.weight", %"crosstransformer.layers_t.1.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2092 |  # node_Transpose_2092
            %"transpose_29"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_37") {perm=[1, 0, 2]}
    2093 |  # node_Transpose_2093
            %"transpose_30"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_40") {perm=[1, 0, 2]}
    2094 |  # node_aten_split_with_sizes_2094
            %"split_with_sizes_2"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers_t.1.cross_attn.in_proj_weight", %"val_1509") {dim=0}
    2095 |  # node_aten_getitem_2095
            %"getitem_43"<FLOAT,[512,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_2", %"val_80")
    2096 |  # node_aten_getitem_2096
            %"getitem_44"<FLOAT,[1024,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_2", %"val_35")
    2097 |  # node_aten_split_with_sizes_2097
            %"split_with_sizes_3"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers_t.1.cross_attn.in_proj_bias", %"val_1509") {dim=0}
    2098 |  # node_aten_getitem_2098
            %"getitem_45"<FLOAT,[512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_3", %"val_80")
    2099 |  # node_aten_getitem_2099
            %"getitem_46"<FLOAT,[1024]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_3", %"val_35")
    2100 |  # node_Cast_2100
            %"val_1591"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2101 |  # node_Reshape_2101
            %"view_59"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_29", %"val_1591") {allowzero=0}
    2102 |  # node_Transpose_2102
            %"t_14"<FLOAT,[512,512]> ⬅️ ::Transpose(%"getitem_43") {perm=[1, 0]}
    2103 |  # node_Gemm_2103
            %"addmm_13"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_59", %"t_14", %"getitem_45") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2104 |  # node_Cast_2104
            %"val_1592"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    2105 |  # node_Reshape_2105
            %"view_60"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_13", %"val_1592") {allowzero=0}
    2106 |  # node_Cast_2106
            %"val_1593"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2107 |  # node_Reshape_2107
            %"view_61"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_30", %"val_1593") {allowzero=0}
    2108 |  # node_Transpose_2108
            %"t_15"<FLOAT,[512,1024]> ⬅️ ::Transpose(%"getitem_44") {perm=[1, 0]}
    2109 |  # node_Gemm_2109
            %"addmm_14"<FLOAT,[3448,1024]> ⬅️ ::Gemm(%"view_61", %"t_15", %"getitem_46") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2110 |  # node_Constant_2110
            %"val_1594"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([3448,    1, 1024]), name=None)}
    2111 |  # node_Cast_2111
            %"val_1595"<?,?> ⬅️ ::Cast(%"val_1594") {to=7}
    2112 |  # node_Reshape_2112
            %"view_62"<FLOAT,[3448,1,1024]> ⬅️ ::Reshape(%"addmm_14", %"val_1595") {allowzero=0}
    2113 |  # node_Constant_2113
            %"val_1596"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([3448,    1,    2,  512]), name=None)}
    2114 |  # node_Cast_2114
            %"val_1597"<?,?> ⬅️ ::Cast(%"val_1596") {to=7}
    2115 |  # node_Reshape_2115
            %"view_63"<FLOAT,[3448,1,2,512]> ⬅️ ::Reshape(%"view_62", %"val_1597") {allowzero=0}
    2116 |  # node_aten_unsqueeze_2116
            %"unsqueeze_28"<FLOAT,[1,3448,1,2,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_63") {dim=0}
    2117 |  # node_Transpose_2117
            %"transpose_31"<FLOAT,[2,3448,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_28") {perm=[3, 1, 2, 0, 4]}
    2118 |  # node_aten_squeeze_dim_2118
            %"squeeze_3"<FLOAT,[2,3448,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_31") {dim=-2}
    2119 |  # node_Identity_2119
            %"clone_15"<FLOAT,[2,3448,1,512]> ⬅️ ::Identity(%"squeeze_3")
    2120 |  # node_Gather_2120
            %"select_8"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_15", %"val_80") {axis=0}
    2121 |  # node_Gather_2121
            %"select_9"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_15", %"val_35") {axis=0}
    2122 |  # node_Cast_2122
            %"val_1598"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2123 |  # node_Reshape_2123
            %"view_64"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"view_60", %"val_1598") {allowzero=0}
    2124 |  # node_Transpose_2124
            %"transpose_32"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_64") {perm=[1, 0, 2]}
    2125 |  # node_Cast_2125
            %"val_1599"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2126 |  # node_Reshape_2126
            %"view_65"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_8", %"val_1599") {allowzero=0}
    2127 |  # node_Transpose_2127
            %"transpose_33"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_65") {perm=[1, 0, 2]}
    2128 |  # node_Cast_2128
            %"val_1600"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2129 |  # node_Reshape_2129
            %"view_66"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_9", %"val_1600") {allowzero=0}
    2130 |  # node_Transpose_2130
            %"transpose_34"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_66") {perm=[1, 0, 2]}
    2131 |  # node_Cast_2131
            %"val_1601"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2132 |  # node_Reshape_2132
            %"view_67"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_32", %"val_1601") {allowzero=0}
    2133 |  # node_Cast_2133
            %"val_1602"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2134 |  # node_Reshape_2134
            %"view_68"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_33", %"val_1602") {allowzero=0}
    2135 |  # node_Cast_2135
            %"val_1603"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2136 |  # node_Reshape_2136
            %"view_69"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_34", %"val_1603") {allowzero=0}
    2137 |  # node_Shape_2137
            %"val_1604"<?,?> ⬅️ ::Shape(%"view_67") {start=0}
    2138 |  # node_Constant_2138
            %"val_1605"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2139 |  # node_Gather_2139
            %"val_1606"<?,?> ⬅️ ::Gather(%"val_1604", %"val_1605") {axis=0}
    2140 |  # node_CastLike_2140
            %"val_1607"<?,?> ⬅️ ::CastLike(%"val_1606", %"view_67")
    2141 |  # node_Constant_2141
            %"val_1608"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2142 |  # node_CastLike_2142
            %"val_1609"<?,?> ⬅️ ::CastLike(%"val_1608", %"view_67")
    2143 |  # node_Sqrt_2143
            %"val_1610"<?,?> ⬅️ ::Sqrt(%"val_1607")
    2144 |  # node_Div_2144
            %"val_1611"<?,?> ⬅️ ::Div(%"val_1609", %"val_1610")
    2145 |  # node_CastLike_2145
            %"val_1612"<?,?> ⬅️ ::CastLike(%"val_1611", %"view_67")
    2146 |  # node_Shape_2146
            %"val_1613"<?,?> ⬅️ ::Shape(%"view_68") {start=0}
    2147 |  # node_Constant_2147
            %"val_1614"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2148 |  # node_Slice_2148
            %"val_1615"<?,?> ⬅️ ::Slice(%"val_1613", %"val_1357", %"val_1614", None, None)
    2149 |  # node_Slice_2149
            %"val_1616"<?,?> ⬅️ ::Slice(%"val_1613", %"val_1359", %"val_1357", None, None)
    2150 |  # node_Constant_2150
            %"val_1617"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2151 |  # node_Slice_2151
            %"val_1618"<?,?> ⬅️ ::Slice(%"val_1613", %"val_1617", %"val_1359", None, None)
    2152 |  # node_Constant_2152
            %"val_1619"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2153 |  # node_Concat_2153
            %"val_1620"<?,?> ⬅️ ::Concat(%"val_1619", %"val_1616", %"val_1615") {axis=0}
    2154 |  # node_Reshape_2154
            %"val_1621"<?,?> ⬅️ ::Reshape(%"view_68", %"val_1620") {allowzero=0}
    2155 |  # node_Transpose_2155
            %"val_1622"<?,?> ⬅️ ::Transpose(%"val_1621") {perm=[0, 2, 1]}
    2156 |  # node_Concat_2156
            %"val_1623"<?,?> ⬅️ ::Concat(%"val_1618", %"val_1615", %"val_1616") {axis=0}
    2157 |  # node_Reshape_2157
            %"val_1624"<?,?> ⬅️ ::Reshape(%"val_1622", %"val_1623") {allowzero=0}
    2158 |  # node_Sqrt_2158
            %"val_1625"<?,?> ⬅️ ::Sqrt(%"val_1612")
    2159 |  # node_Mul_2159
            %"val_1626"<?,?> ⬅️ ::Mul(%"view_67", %"val_1625")
    2160 |  # node_Sqrt_2160
            %"val_1627"<?,?> ⬅️ ::Sqrt(%"val_1612")
    2161 |  # node_CastLike_2161
            %"val_1628"<?,?> ⬅️ ::CastLike(%"val_1627", %"val_1624")
    2162 |  # node_Mul_2162
            %"val_1629"<?,?> ⬅️ ::Mul(%"val_1624", %"val_1628")
    2163 |  # node_MatMul_2163
            %"val_1630"<?,?> ⬅️ ::MatMul(%"val_1626", %"val_1629")
    2164 |  # node_Softmax_2164
            %"val_1631"<?,?> ⬅️ ::Softmax(%"val_1630") {axis=-1}
    2165 |  # node_Dropout_2165
            %"val_1632"<?,?>, %"val_1633"<?,?> ⬅️ ::Dropout(%"val_1631", %"val_953", None)
    2166 |  # node_MatMul_2166
            %"getitem_47"<FLOAT,[1,8,1723,64]> ⬅️ ::MatMul(%"val_1632", %"view_69")
    2167 |  # node_Shape_2167
            %"val_1634"<?,?> ⬅️ ::Shape(%"view_67") {start=0}
    2168 |  # node_Slice_2168
            %"val_1635"<?,?> ⬅️ ::Slice(%"val_1634", %"val_1379", %"val_1380", None, None)
    2169 |  # node_Slice_2169
            %"val_1636"<?,?> ⬅️ ::Slice(%"val_1634", %"val_1380", %"val_1382", None, None)
    2170 |  # node_Slice_2170
            %"val_1637"<?,?> ⬅️ ::Slice(%"val_1634", %"val_1359", %"val_1357", None, None)
    2171 |  # node_Cast_2171
            %"val_1638"<?,?> ⬅️ ::Cast(%"val_1636") {to=1}
    2172 |  # node_Div_2172
            %"val_1639"<?,?> ⬅️ ::Div(%"val_1638", %"val_1386")
    2173 |  # node_Ceil_2173
            %"val_1640"<?,?> ⬅️ ::Ceil(%"val_1639")
    2174 |  # node_Mul_2174
            %"val_1641"<?,?> ⬅️ ::Mul(%"val_1640", %"val_1386")
    2175 |  # node_Cast_2175
            %"val_1642"<?,?> ⬅️ ::Cast(%"val_1641") {to=7}
    2176 |  # node_Concat_2176
            %"val_1643"<?,?> ⬅️ ::Concat(%"val_1635", %"val_1637", %"val_1642") {axis=0}
    2177 |  # node_Expand_2177
            %"_scaled_dot_product_flash_attention_for_cpu_3__1"<FLOAT,[1,8,1723]> ⬅️ ::Expand(%"val_953", %"val_1643")
    2178 |  # node_Transpose_2178
            %"permute_16"<FLOAT,[1723,1,8,64]> ⬅️ ::Transpose(%"getitem_47") {perm=[2, 0, 1, 3]}
    2179 |  # node_Cast_2179
            %"val_1644"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2180 |  # node_Reshape_2180
            %"view_70"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"permute_16", %"val_1644") {allowzero=0}
    2181 |  # node_Transpose_2181
            %"t_16"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.1.cross_attn.out_proj.weight") {perm=[1, 0]}
    2182 |  # node_Gemm_2182
            %"addmm_15"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_70", %"t_16", %"crosstransformer.layers_t.1.cross_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2183 |  # node_Cast_2183
            %"val_1645"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    2184 |  # node_Reshape_2184
            %"view_71"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_15", %"val_1645") {allowzero=0}
    2185 |  # node_Transpose_2185
            %"transpose_35"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"view_71") {perm=[1, 0, 2]}
    2186 |  # node_Identity_2186
            %"clone_16"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"transpose_35")
    2187 |  # node_Mul_2187
            %"mul_37"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.1.gamma_1.scale", %"clone_16")
    2188 |  # node_aten_add_2188
            %"add_38"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_19", %"mul_37") {alpha=1.0}
    2189 |  # node_LayerNormalization_2189
            %"getitem_49"<FLOAT,[1,1723,512]>, %"native_layer_norm_11__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_11__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"add_38", %"crosstransformer.layers_t.1.norm3.weight", %"crosstransformer.layers_t.1.norm3.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2190 |  # node_Cast_2190
            %"val_1646"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2191 |  # node_Reshape_2191
            %"view_72"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"getitem_49", %"val_1646") {allowzero=0}
    2192 |  # node_Transpose_2192
            %"t_17"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers_t.1.linear1.weight") {perm=[1, 0]}
    2193 |  # node_Gemm_2193
            %"addmm_16"<FLOAT,[1723,2048]> ⬅️ ::Gemm(%"view_72", %"t_17", %"crosstransformer.layers_t.1.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2194 |  # node_Cast_2194
            %"val_1647"<?,?> ⬅️ ::Cast(%"val_1481") {to=7}
    2195 |  # node_Reshape_2195
            %"view_73"<FLOAT,[1,1723,2048]> ⬅️ ::Reshape(%"addmm_16", %"val_1647") {allowzero=0}
    2196 |  # node__aten_gelu_approximate_none_2196
            %"gelu_27"<FLOAT,[1,1723,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_73")
    2197 |  # node_Identity_2197
            %"clone_17"<FLOAT,[1,1723,2048]> ⬅️ ::Identity(%"gelu_27")
    2198 |  # node_Cast_2198
            %"val_1648"<?,?> ⬅️ ::Cast(%"val_1483") {to=7}
    2199 |  # node_Reshape_2199
            %"view_74"<FLOAT,[1723,2048]> ⬅️ ::Reshape(%"clone_17", %"val_1648") {allowzero=0}
    2200 |  # node_Transpose_2200
            %"t_18"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.1.linear2.weight") {perm=[1, 0]}
    2201 |  # node_Gemm_2201
            %"addmm_17"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_74", %"t_18", %"crosstransformer.layers_t.1.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2202 |  # node_Cast_2202
            %"val_1649"<?,?> ⬅️ ::Cast(%"val_1485") {to=7}
    2203 |  # node_Reshape_2203
            %"view_75"<FLOAT,[1,1723,512]> ⬅️ ::Reshape(%"addmm_17", %"val_1649") {allowzero=0}
    2204 |  # node_Identity_2204
            %"clone_18"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"view_75")
    2205 |  # node_Mul_2205
            %"mul_38"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.1.gamma_2.scale", %"clone_18")
    2206 |  # node_aten_add_2206
            %"add_39"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_38", %"mul_38") {alpha=1.0}
    2207 |  # node_Transpose_2207
            %"transpose_36"<FLOAT,[1,512,1723]> ⬅️ ::Transpose(%"add_39") {perm=[0, 2, 1]}
    2208 |  # node_Constant_2208
            %"val_1650"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2209 |  # node_Reshape_2209
            %"val_1651"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1650") {allowzero=0}
    2210 |  # node_Constant_2210
            %"val_1652"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2211 |  # node_Concat_2211
            %"val_1653"<?,?> ⬅️ ::Concat(%"val_1652", %"val_1651", %"val_1650") {axis=0}
    2212 |  # node_Reshape_2212
            %"val_1654"<?,?> ⬅️ ::Reshape(%"transpose_36", %"val_1653") {allowzero=0}
    2213 |  # node_Constant_2213
            %"val_1655"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2214 |  # node_CastLike_2214
            %"val_1656"<?,?> ⬅️ ::CastLike(%"val_1655", %"transpose_36")
    2215 |  # node_Expand_2215
            %"val_1657"<?,?> ⬅️ ::Expand(%"val_1656", %"val_1651")
    2216 |  # node_Constant_2216
            %"val_1658"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2217 |  # node_CastLike_2217
            %"val_1659"<?,?> ⬅️ ::CastLike(%"val_1658", %"transpose_36")
    2218 |  # node_Expand_2218
            %"val_1660"<?,?> ⬅️ ::Expand(%"val_1659", %"val_1651")
    2219 |  # node_InstanceNormalization_2219
            %"val_1661"<?,?> ⬅️ ::InstanceNormalization(%"val_1654", %"val_1657", %"val_1660") {epsilon=1e-05}
    2220 |  # node_Shape_2220
            %"val_1662"<?,?> ⬅️ ::Shape(%"transpose_36") {start=0}
    2221 |  # node_Reshape_2221
            %"val_1663"<?,?> ⬅️ ::Reshape(%"val_1661", %"val_1662") {allowzero=0}
    2222 |  # node_Constant_2222
            %"val_1664"<?,?> ⬅️ ::Constant() {value_int=1}
    2223 |  # node_Sub_2223
            %"val_1665"<?,?> ⬅️ ::Sub(%"val_50", %"val_1664")
    2224 |  # node_Range_2224
            %"val_1666"<?,?> ⬅️ ::Range(%"val_1664", %"val_1665", %"val_1664")
    2225 |  # node_Unsqueeze_2225
            %"val_1667"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.1.norm_out.weight", %"val_1666")
    2226 |  # node_Unsqueeze_2226
            %"val_1668"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.1.norm_out.bias", %"val_1666")
    2227 |  # node_CastLike_2227
            %"val_1669"<?,?> ⬅️ ::CastLike(%"val_1667", %"val_1663")
    2228 |  # node_Mul_2228
            %"val_1670"<?,?> ⬅️ ::Mul(%"val_1663", %"val_1669")
    2229 |  # node_CastLike_2229
            %"val_1671"<?,?> ⬅️ ::CastLike(%"val_1668", %"val_1670")
    2230 |  # node_Add_2230
            %"group_norm_35"<FLOAT,[1,512,1723]> ⬅️ ::Add(%"val_1670", %"val_1671")
    2231 |  # node_Transpose_2231
            %"transpose_37"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"group_norm_35") {perm=[0, 2, 1]}
    2232 |  # node_LayerNormalization_2232
            %"getitem_52"<FLOAT,[1,3448,512]>, %"native_layer_norm_12__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_12__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"transpose_28", %"crosstransformer.layers.2.norm1.weight", %"crosstransformer.layers.2.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2233 |  # node_Transpose_2233
            %"transpose_38"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_52") {perm=[1, 0, 2]}
    2234 |  # node_Cast_2234
            %"val_1672"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2235 |  # node_Reshape_2235
            %"view_76"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_38", %"val_1672") {allowzero=0}
    2236 |  # node_Transpose_2236
            %"t_19"<FLOAT,[512,1536]> ⬅️ ::Transpose(%"crosstransformer.layers.2.self_attn.in_proj_weight") {perm=[1, 0]}
    2237 |  # node_Gemm_2237
            %"addmm_18"<FLOAT,[3448,1536]> ⬅️ ::Gemm(%"view_76", %"t_19", %"crosstransformer.layers.2.self_attn.in_proj_bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2238 |  # node_Cast_2238
            %"val_1673"<?,?> ⬅️ ::Cast(%"val_1334") {to=7}
    2239 |  # node_Reshape_2239
            %"view_77"<FLOAT,[3448,1,1536]> ⬅️ ::Reshape(%"addmm_18", %"val_1673") {allowzero=0}
    2240 |  # node_Cast_2240
            %"val_1674"<?,?> ⬅️ ::Cast(%"val_1336") {to=7}
    2241 |  # node_Reshape_2241
            %"view_78"<FLOAT,[3448,1,3,512]> ⬅️ ::Reshape(%"view_77", %"val_1674") {allowzero=0}
    2242 |  # node_aten_unsqueeze_2242
            %"unsqueeze_29"<FLOAT,[1,3448,1,3,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_78") {dim=0}
    2243 |  # node_Transpose_2243
            %"transpose_39"<FLOAT,[3,3448,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_29") {perm=[3, 1, 2, 0, 4]}
    2244 |  # node_aten_squeeze_dim_2244
            %"squeeze_4"<FLOAT,[3,3448,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_39") {dim=-2}
    2245 |  # node_Identity_2245
            %"clone_19"<FLOAT,[3,3448,1,512]> ⬅️ ::Identity(%"squeeze_4")
    2246 |  # node_Gather_2246
            %"select_10"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_19", %"val_80") {axis=0}
    2247 |  # node_Gather_2247
            %"select_11"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_19", %"val_35") {axis=0}
    2248 |  # node_Gather_2248
            %"select_12"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_19", %"val_276") {axis=0}
    2249 |  # node_Cast_2249
            %"val_1675"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2250 |  # node_Reshape_2250
            %"view_79"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_10", %"val_1675") {allowzero=0}
    2251 |  # node_Transpose_2251
            %"transpose_40"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_79") {perm=[1, 0, 2]}
    2252 |  # node_Cast_2252
            %"val_1676"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2253 |  # node_Reshape_2253
            %"view_80"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_11", %"val_1676") {allowzero=0}
    2254 |  # node_Transpose_2254
            %"transpose_41"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_80") {perm=[1, 0, 2]}
    2255 |  # node_Cast_2255
            %"val_1677"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2256 |  # node_Reshape_2256
            %"view_81"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_12", %"val_1677") {allowzero=0}
    2257 |  # node_Transpose_2257
            %"transpose_42"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_81") {perm=[1, 0, 2]}
    2258 |  # node_Cast_2258
            %"val_1678"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2259 |  # node_Reshape_2259
            %"view_82"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_40", %"val_1678") {allowzero=0}
    2260 |  # node_Cast_2260
            %"val_1679"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2261 |  # node_Reshape_2261
            %"view_83"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_41", %"val_1679") {allowzero=0}
    2262 |  # node_Cast_2262
            %"val_1680"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2263 |  # node_Reshape_2263
            %"view_84"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_42", %"val_1680") {allowzero=0}
    2264 |  # node_Shape_2264
            %"val_1681"<?,?> ⬅️ ::Shape(%"view_82") {start=0}
    2265 |  # node_Constant_2265
            %"val_1682"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2266 |  # node_Gather_2266
            %"val_1683"<?,?> ⬅️ ::Gather(%"val_1681", %"val_1682") {axis=0}
    2267 |  # node_CastLike_2267
            %"val_1684"<?,?> ⬅️ ::CastLike(%"val_1683", %"view_82")
    2268 |  # node_Constant_2268
            %"val_1685"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2269 |  # node_CastLike_2269
            %"val_1686"<?,?> ⬅️ ::CastLike(%"val_1685", %"view_82")
    2270 |  # node_Sqrt_2270
            %"val_1687"<?,?> ⬅️ ::Sqrt(%"val_1684")
    2271 |  # node_Div_2271
            %"val_1688"<?,?> ⬅️ ::Div(%"val_1686", %"val_1687")
    2272 |  # node_CastLike_2272
            %"val_1689"<?,?> ⬅️ ::CastLike(%"val_1688", %"view_82")
    2273 |  # node_Shape_2273
            %"val_1690"<?,?> ⬅️ ::Shape(%"view_83") {start=0}
    2274 |  # node_Constant_2274
            %"val_1691"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2275 |  # node_Slice_2275
            %"val_1692"<?,?> ⬅️ ::Slice(%"val_1690", %"val_1357", %"val_1691", None, None)
    2276 |  # node_Slice_2276
            %"val_1693"<?,?> ⬅️ ::Slice(%"val_1690", %"val_1359", %"val_1357", None, None)
    2277 |  # node_Constant_2277
            %"val_1694"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2278 |  # node_Slice_2278
            %"val_1695"<?,?> ⬅️ ::Slice(%"val_1690", %"val_1694", %"val_1359", None, None)
    2279 |  # node_Constant_2279
            %"val_1696"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2280 |  # node_Concat_2280
            %"val_1697"<?,?> ⬅️ ::Concat(%"val_1696", %"val_1693", %"val_1692") {axis=0}
    2281 |  # node_Reshape_2281
            %"val_1698"<?,?> ⬅️ ::Reshape(%"view_83", %"val_1697") {allowzero=0}
    2282 |  # node_Transpose_2282
            %"val_1699"<?,?> ⬅️ ::Transpose(%"val_1698") {perm=[0, 2, 1]}
    2283 |  # node_Concat_2283
            %"val_1700"<?,?> ⬅️ ::Concat(%"val_1695", %"val_1692", %"val_1693") {axis=0}
    2284 |  # node_Reshape_2284
            %"val_1701"<?,?> ⬅️ ::Reshape(%"val_1699", %"val_1700") {allowzero=0}
    2285 |  # node_Sqrt_2285
            %"val_1702"<?,?> ⬅️ ::Sqrt(%"val_1689")
    2286 |  # node_Mul_2286
            %"val_1703"<?,?> ⬅️ ::Mul(%"view_82", %"val_1702")
    2287 |  # node_Sqrt_2287
            %"val_1704"<?,?> ⬅️ ::Sqrt(%"val_1689")
    2288 |  # node_CastLike_2288
            %"val_1705"<?,?> ⬅️ ::CastLike(%"val_1704", %"val_1701")
    2289 |  # node_Mul_2289
            %"val_1706"<?,?> ⬅️ ::Mul(%"val_1701", %"val_1705")
    2290 |  # node_MatMul_2290
            %"val_1707"<?,?> ⬅️ ::MatMul(%"val_1703", %"val_1706")
    2291 |  # node_Softmax_2291
            %"val_1708"<?,?> ⬅️ ::Softmax(%"val_1707") {axis=-1}
    2292 |  # node_Dropout_2292
            %"val_1709"<?,?>, %"val_1710"<?,?> ⬅️ ::Dropout(%"val_1708", %"val_953", None)
    2293 |  # node_MatMul_2293
            %"getitem_55"<FLOAT,[1,8,3448,64]> ⬅️ ::MatMul(%"val_1709", %"view_84")
    2294 |  # node_Shape_2294
            %"val_1711"<?,?> ⬅️ ::Shape(%"view_82") {start=0}
    2295 |  # node_Slice_2295
            %"val_1712"<?,?> ⬅️ ::Slice(%"val_1711", %"val_1379", %"val_1380", None, None)
    2296 |  # node_Slice_2296
            %"val_1713"<?,?> ⬅️ ::Slice(%"val_1711", %"val_1380", %"val_1382", None, None)
    2297 |  # node_Slice_2297
            %"val_1714"<?,?> ⬅️ ::Slice(%"val_1711", %"val_1359", %"val_1357", None, None)
    2298 |  # node_Cast_2298
            %"val_1715"<?,?> ⬅️ ::Cast(%"val_1713") {to=1}
    2299 |  # node_Div_2299
            %"val_1716"<?,?> ⬅️ ::Div(%"val_1715", %"val_1386")
    2300 |  # node_Ceil_2300
            %"val_1717"<?,?> ⬅️ ::Ceil(%"val_1716")
    2301 |  # node_Mul_2301
            %"val_1718"<?,?> ⬅️ ::Mul(%"val_1717", %"val_1386")
    2302 |  # node_Cast_2302
            %"val_1719"<?,?> ⬅️ ::Cast(%"val_1718") {to=7}
    2303 |  # node_Concat_2303
            %"val_1720"<?,?> ⬅️ ::Concat(%"val_1712", %"val_1714", %"val_1719") {axis=0}
    2304 |  # node_Expand_2304
            %"_scaled_dot_product_flash_attention_for_cpu_4__1"<FLOAT,[1,8,3448]> ⬅️ ::Expand(%"val_953", %"val_1720")
    2305 |  # node_Transpose_2305
            %"permute_17"<FLOAT,[3448,1,8,64]> ⬅️ ::Transpose(%"getitem_55") {perm=[2, 0, 1, 3]}
    2306 |  # node_Cast_2306
            %"val_1721"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2307 |  # node_Reshape_2307
            %"view_85"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"permute_17", %"val_1721") {allowzero=0}
    2308 |  # node_Transpose_2308
            %"t_20"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers.2.self_attn.out_proj.weight") {perm=[1, 0]}
    2309 |  # node_Gemm_2309
            %"addmm_19"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_85", %"t_20", %"crosstransformer.layers.2.self_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2310 |  # node_Cast_2310
            %"val_1722"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    2311 |  # node_Reshape_2311
            %"view_86"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_19", %"val_1722") {allowzero=0}
    2312 |  # node_Transpose_2312
            %"transpose_43"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"view_86") {perm=[1, 0, 2]}
    2313 |  # node_Identity_2313
            %"clone_20"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"transpose_43")
    2314 |  # node_Mul_2314
            %"mul_39"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.2.gamma_1.scale", %"clone_20")
    2315 |  # node_aten_add_2315
            %"add_40"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_28", %"mul_39") {alpha=1.0}
    2316 |  # node_LayerNormalization_2316
            %"getitem_57"<FLOAT,[1,3448,512]>, %"native_layer_norm_13__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_13__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"add_40", %"crosstransformer.layers.2.norm2.weight", %"crosstransformer.layers.2.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2317 |  # node_Cast_2317
            %"val_1723"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2318 |  # node_Reshape_2318
            %"view_87"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"getitem_57", %"val_1723") {allowzero=0}
    2319 |  # node_Transpose_2319
            %"t_21"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers.2.linear1.weight") {perm=[1, 0]}
    2320 |  # node_Gemm_2320
            %"addmm_20"<FLOAT,[3448,2048]> ⬅️ ::Gemm(%"view_87", %"t_21", %"crosstransformer.layers.2.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2321 |  # node_Cast_2321
            %"val_1724"<?,?> ⬅️ ::Cast(%"val_1396") {to=7}
    2322 |  # node_Reshape_2322
            %"view_88"<FLOAT,[1,3448,2048]> ⬅️ ::Reshape(%"addmm_20", %"val_1724") {allowzero=0}
    2323 |  # node__aten_gelu_approximate_none_2323
            %"gelu_28"<FLOAT,[1,3448,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_88")
    2324 |  # node_Identity_2324
            %"clone_21"<FLOAT,[1,3448,2048]> ⬅️ ::Identity(%"gelu_28")
    2325 |  # node_Cast_2325
            %"val_1725"<?,?> ⬅️ ::Cast(%"val_1398") {to=7}
    2326 |  # node_Reshape_2326
            %"view_89"<FLOAT,[3448,2048]> ⬅️ ::Reshape(%"clone_21", %"val_1725") {allowzero=0}
    2327 |  # node_Transpose_2327
            %"t_22"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers.2.linear2.weight") {perm=[1, 0]}
    2328 |  # node_Gemm_2328
            %"addmm_21"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_89", %"t_22", %"crosstransformer.layers.2.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2329 |  # node_Cast_2329
            %"val_1726"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    2330 |  # node_Reshape_2330
            %"view_90"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"addmm_21", %"val_1726") {allowzero=0}
    2331 |  # node_Identity_2331
            %"clone_22"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"view_90")
    2332 |  # node_Mul_2332
            %"mul_40"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.2.gamma_2.scale", %"clone_22")
    2333 |  # node_aten_add_2333
            %"add_41"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_40", %"mul_40") {alpha=1.0}
    2334 |  # node_Transpose_2334
            %"transpose_44"<FLOAT,[1,512,3448]> ⬅️ ::Transpose(%"add_41") {perm=[0, 2, 1]}
    2335 |  # node_Constant_2335
            %"val_1727"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2336 |  # node_Reshape_2336
            %"val_1728"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1727") {allowzero=0}
    2337 |  # node_Constant_2337
            %"val_1729"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2338 |  # node_Concat_2338
            %"val_1730"<?,?> ⬅️ ::Concat(%"val_1729", %"val_1728", %"val_1727") {axis=0}
    2339 |  # node_Reshape_2339
            %"val_1731"<?,?> ⬅️ ::Reshape(%"transpose_44", %"val_1730") {allowzero=0}
    2340 |  # node_Constant_2340
            %"val_1732"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2341 |  # node_CastLike_2341
            %"val_1733"<?,?> ⬅️ ::CastLike(%"val_1732", %"transpose_44")
    2342 |  # node_Expand_2342
            %"val_1734"<?,?> ⬅️ ::Expand(%"val_1733", %"val_1728")
    2343 |  # node_Constant_2343
            %"val_1735"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2344 |  # node_CastLike_2344
            %"val_1736"<?,?> ⬅️ ::CastLike(%"val_1735", %"transpose_44")
    2345 |  # node_Expand_2345
            %"val_1737"<?,?> ⬅️ ::Expand(%"val_1736", %"val_1728")
    2346 |  # node_InstanceNormalization_2346
            %"val_1738"<?,?> ⬅️ ::InstanceNormalization(%"val_1731", %"val_1734", %"val_1737") {epsilon=1e-05}
    2347 |  # node_Shape_2347
            %"val_1739"<?,?> ⬅️ ::Shape(%"transpose_44") {start=0}
    2348 |  # node_Reshape_2348
            %"val_1740"<?,?> ⬅️ ::Reshape(%"val_1738", %"val_1739") {allowzero=0}
    2349 |  # node_Constant_2349
            %"val_1741"<?,?> ⬅️ ::Constant() {value_int=1}
    2350 |  # node_Sub_2350
            %"val_1742"<?,?> ⬅️ ::Sub(%"val_50", %"val_1741")
    2351 |  # node_Range_2351
            %"val_1743"<?,?> ⬅️ ::Range(%"val_1741", %"val_1742", %"val_1741")
    2352 |  # node_Unsqueeze_2352
            %"val_1744"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.2.norm_out.weight", %"val_1743")
    2353 |  # node_Unsqueeze_2353
            %"val_1745"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.2.norm_out.bias", %"val_1743")
    2354 |  # node_CastLike_2354
            %"val_1746"<?,?> ⬅️ ::CastLike(%"val_1744", %"val_1740")
    2355 |  # node_Mul_2355
            %"val_1747"<?,?> ⬅️ ::Mul(%"val_1740", %"val_1746")
    2356 |  # node_CastLike_2356
            %"val_1748"<?,?> ⬅️ ::CastLike(%"val_1745", %"val_1747")
    2357 |  # node_Add_2357
            %"group_norm_36"<FLOAT,[1,512,3448]> ⬅️ ::Add(%"val_1747", %"val_1748")
    2358 |  # node_Transpose_2358
            %"transpose_45"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"group_norm_36") {perm=[0, 2, 1]}
    2359 |  # node_LayerNormalization_2359
            %"getitem_60"<FLOAT,[1,1723,512]>, %"native_layer_norm_14__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_14__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"transpose_37", %"crosstransformer.layers_t.2.norm1.weight", %"crosstransformer.layers_t.2.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2360 |  # node_Transpose_2360
            %"transpose_46"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_60") {perm=[1, 0, 2]}
    2361 |  # node_Cast_2361
            %"val_1749"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2362 |  # node_Reshape_2362
            %"view_91"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_46", %"val_1749") {allowzero=0}
    2363 |  # node_Transpose_2363
            %"t_23"<FLOAT,[512,1536]> ⬅️ ::Transpose(%"crosstransformer.layers_t.2.self_attn.in_proj_weight") {perm=[1, 0]}
    2364 |  # node_Gemm_2364
            %"addmm_22"<FLOAT,[1723,1536]> ⬅️ ::Gemm(%"view_91", %"t_23", %"crosstransformer.layers_t.2.self_attn.in_proj_bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2365 |  # node_Cast_2365
            %"val_1750"<?,?> ⬅️ ::Cast(%"val_1425") {to=7}
    2366 |  # node_Reshape_2366
            %"view_92"<FLOAT,[1723,1,1536]> ⬅️ ::Reshape(%"addmm_22", %"val_1750") {allowzero=0}
    2367 |  # node_Cast_2367
            %"val_1751"<?,?> ⬅️ ::Cast(%"val_1427") {to=7}
    2368 |  # node_Reshape_2368
            %"view_93"<FLOAT,[1723,1,3,512]> ⬅️ ::Reshape(%"view_92", %"val_1751") {allowzero=0}
    2369 |  # node_aten_unsqueeze_2369
            %"unsqueeze_30"<FLOAT,[1,1723,1,3,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_93") {dim=0}
    2370 |  # node_Transpose_2370
            %"transpose_47"<FLOAT,[3,1723,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_30") {perm=[3, 1, 2, 0, 4]}
    2371 |  # node_aten_squeeze_dim_2371
            %"squeeze_5"<FLOAT,[3,1723,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_47") {dim=-2}
    2372 |  # node_Identity_2372
            %"clone_23"<FLOAT,[3,1723,1,512]> ⬅️ ::Identity(%"squeeze_5")
    2373 |  # node_Gather_2373
            %"select_13"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_23", %"val_80") {axis=0}
    2374 |  # node_Gather_2374
            %"select_14"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_23", %"val_35") {axis=0}
    2375 |  # node_Gather_2375
            %"select_15"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_23", %"val_276") {axis=0}
    2376 |  # node_Cast_2376
            %"val_1752"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2377 |  # node_Reshape_2377
            %"view_94"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_13", %"val_1752") {allowzero=0}
    2378 |  # node_Transpose_2378
            %"transpose_48"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_94") {perm=[1, 0, 2]}
    2379 |  # node_Cast_2379
            %"val_1753"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2380 |  # node_Reshape_2380
            %"view_95"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_14", %"val_1753") {allowzero=0}
    2381 |  # node_Transpose_2381
            %"transpose_49"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_95") {perm=[1, 0, 2]}
    2382 |  # node_Cast_2382
            %"val_1754"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2383 |  # node_Reshape_2383
            %"view_96"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_15", %"val_1754") {allowzero=0}
    2384 |  # node_Transpose_2384
            %"transpose_50"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_96") {perm=[1, 0, 2]}
    2385 |  # node_Cast_2385
            %"val_1755"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2386 |  # node_Reshape_2386
            %"view_97"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_48", %"val_1755") {allowzero=0}
    2387 |  # node_Cast_2387
            %"val_1756"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2388 |  # node_Reshape_2388
            %"view_98"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_49", %"val_1756") {allowzero=0}
    2389 |  # node_Cast_2389
            %"val_1757"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2390 |  # node_Reshape_2390
            %"view_99"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_50", %"val_1757") {allowzero=0}
    2391 |  # node_Shape_2391
            %"val_1758"<?,?> ⬅️ ::Shape(%"view_97") {start=0}
    2392 |  # node_Constant_2392
            %"val_1759"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2393 |  # node_Gather_2393
            %"val_1760"<?,?> ⬅️ ::Gather(%"val_1758", %"val_1759") {axis=0}
    2394 |  # node_CastLike_2394
            %"val_1761"<?,?> ⬅️ ::CastLike(%"val_1760", %"view_97")
    2395 |  # node_Constant_2395
            %"val_1762"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2396 |  # node_CastLike_2396
            %"val_1763"<?,?> ⬅️ ::CastLike(%"val_1762", %"view_97")
    2397 |  # node_Sqrt_2397
            %"val_1764"<?,?> ⬅️ ::Sqrt(%"val_1761")
    2398 |  # node_Div_2398
            %"val_1765"<?,?> ⬅️ ::Div(%"val_1763", %"val_1764")
    2399 |  # node_CastLike_2399
            %"val_1766"<?,?> ⬅️ ::CastLike(%"val_1765", %"view_97")
    2400 |  # node_Shape_2400
            %"val_1767"<?,?> ⬅️ ::Shape(%"view_98") {start=0}
    2401 |  # node_Constant_2401
            %"val_1768"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2402 |  # node_Slice_2402
            %"val_1769"<?,?> ⬅️ ::Slice(%"val_1767", %"val_1357", %"val_1768", None, None)
    2403 |  # node_Slice_2403
            %"val_1770"<?,?> ⬅️ ::Slice(%"val_1767", %"val_1359", %"val_1357", None, None)
    2404 |  # node_Constant_2404
            %"val_1771"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2405 |  # node_Slice_2405
            %"val_1772"<?,?> ⬅️ ::Slice(%"val_1767", %"val_1771", %"val_1359", None, None)
    2406 |  # node_Constant_2406
            %"val_1773"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2407 |  # node_Concat_2407
            %"val_1774"<?,?> ⬅️ ::Concat(%"val_1773", %"val_1770", %"val_1769") {axis=0}
    2408 |  # node_Reshape_2408
            %"val_1775"<?,?> ⬅️ ::Reshape(%"view_98", %"val_1774") {allowzero=0}
    2409 |  # node_Transpose_2409
            %"val_1776"<?,?> ⬅️ ::Transpose(%"val_1775") {perm=[0, 2, 1]}
    2410 |  # node_Concat_2410
            %"val_1777"<?,?> ⬅️ ::Concat(%"val_1772", %"val_1769", %"val_1770") {axis=0}
    2411 |  # node_Reshape_2411
            %"val_1778"<?,?> ⬅️ ::Reshape(%"val_1776", %"val_1777") {allowzero=0}
    2412 |  # node_Sqrt_2412
            %"val_1779"<?,?> ⬅️ ::Sqrt(%"val_1766")
    2413 |  # node_Mul_2413
            %"val_1780"<?,?> ⬅️ ::Mul(%"view_97", %"val_1779")
    2414 |  # node_Sqrt_2414
            %"val_1781"<?,?> ⬅️ ::Sqrt(%"val_1766")
    2415 |  # node_CastLike_2415
            %"val_1782"<?,?> ⬅️ ::CastLike(%"val_1781", %"val_1778")
    2416 |  # node_Mul_2416
            %"val_1783"<?,?> ⬅️ ::Mul(%"val_1778", %"val_1782")
    2417 |  # node_MatMul_2417
            %"val_1784"<?,?> ⬅️ ::MatMul(%"val_1780", %"val_1783")
    2418 |  # node_Softmax_2418
            %"val_1785"<?,?> ⬅️ ::Softmax(%"val_1784") {axis=-1}
    2419 |  # node_Dropout_2419
            %"val_1786"<?,?>, %"val_1787"<?,?> ⬅️ ::Dropout(%"val_1785", %"val_953", None)
    2420 |  # node_MatMul_2420
            %"getitem_63"<FLOAT,[1,8,1723,64]> ⬅️ ::MatMul(%"val_1786", %"view_99")
    2421 |  # node_Shape_2421
            %"val_1788"<?,?> ⬅️ ::Shape(%"view_97") {start=0}
    2422 |  # node_Slice_2422
            %"val_1789"<?,?> ⬅️ ::Slice(%"val_1788", %"val_1379", %"val_1380", None, None)
    2423 |  # node_Slice_2423
            %"val_1790"<?,?> ⬅️ ::Slice(%"val_1788", %"val_1380", %"val_1382", None, None)
    2424 |  # node_Slice_2424
            %"val_1791"<?,?> ⬅️ ::Slice(%"val_1788", %"val_1359", %"val_1357", None, None)
    2425 |  # node_Cast_2425
            %"val_1792"<?,?> ⬅️ ::Cast(%"val_1790") {to=1}
    2426 |  # node_Div_2426
            %"val_1793"<?,?> ⬅️ ::Div(%"val_1792", %"val_1386")
    2427 |  # node_Ceil_2427
            %"val_1794"<?,?> ⬅️ ::Ceil(%"val_1793")
    2428 |  # node_Mul_2428
            %"val_1795"<?,?> ⬅️ ::Mul(%"val_1794", %"val_1386")
    2429 |  # node_Cast_2429
            %"val_1796"<?,?> ⬅️ ::Cast(%"val_1795") {to=7}
    2430 |  # node_Concat_2430
            %"val_1797"<?,?> ⬅️ ::Concat(%"val_1789", %"val_1791", %"val_1796") {axis=0}
    2431 |  # node_Expand_2431
            %"_scaled_dot_product_flash_attention_for_cpu_5__1"<FLOAT,[1,8,1723]> ⬅️ ::Expand(%"val_953", %"val_1797")
    2432 |  # node_Transpose_2432
            %"permute_18"<FLOAT,[1723,1,8,64]> ⬅️ ::Transpose(%"getitem_63") {perm=[2, 0, 1, 3]}
    2433 |  # node_Cast_2433
            %"val_1798"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2434 |  # node_Reshape_2434
            %"view_100"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"permute_18", %"val_1798") {allowzero=0}
    2435 |  # node_Transpose_2435
            %"t_24"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.2.self_attn.out_proj.weight") {perm=[1, 0]}
    2436 |  # node_Gemm_2436
            %"addmm_23"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_100", %"t_24", %"crosstransformer.layers_t.2.self_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2437 |  # node_Cast_2437
            %"val_1799"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    2438 |  # node_Reshape_2438
            %"view_101"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_23", %"val_1799") {allowzero=0}
    2439 |  # node_Transpose_2439
            %"transpose_51"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"view_101") {perm=[1, 0, 2]}
    2440 |  # node_Identity_2440
            %"clone_24"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"transpose_51")
    2441 |  # node_Mul_2441
            %"mul_41"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.2.gamma_1.scale", %"clone_24")
    2442 |  # node_aten_add_2442
            %"add_42"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_37", %"mul_41") {alpha=1.0}
    2443 |  # node_LayerNormalization_2443
            %"getitem_65"<FLOAT,[1,1723,512]>, %"native_layer_norm_15__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_15__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"add_42", %"crosstransformer.layers_t.2.norm2.weight", %"crosstransformer.layers_t.2.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2444 |  # node_Cast_2444
            %"val_1800"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2445 |  # node_Reshape_2445
            %"view_102"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"getitem_65", %"val_1800") {allowzero=0}
    2446 |  # node_Transpose_2446
            %"t_25"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers_t.2.linear1.weight") {perm=[1, 0]}
    2447 |  # node_Gemm_2447
            %"addmm_24"<FLOAT,[1723,2048]> ⬅️ ::Gemm(%"view_102", %"t_25", %"crosstransformer.layers_t.2.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2448 |  # node_Cast_2448
            %"val_1801"<?,?> ⬅️ ::Cast(%"val_1481") {to=7}
    2449 |  # node_Reshape_2449
            %"view_103"<FLOAT,[1,1723,2048]> ⬅️ ::Reshape(%"addmm_24", %"val_1801") {allowzero=0}
    2450 |  # node__aten_gelu_approximate_none_2450
            %"gelu_29"<FLOAT,[1,1723,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_103")
    2451 |  # node_Identity_2451
            %"clone_25"<FLOAT,[1,1723,2048]> ⬅️ ::Identity(%"gelu_29")
    2452 |  # node_Cast_2452
            %"val_1802"<?,?> ⬅️ ::Cast(%"val_1483") {to=7}
    2453 |  # node_Reshape_2453
            %"view_104"<FLOAT,[1723,2048]> ⬅️ ::Reshape(%"clone_25", %"val_1802") {allowzero=0}
    2454 |  # node_Transpose_2454
            %"t_26"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.2.linear2.weight") {perm=[1, 0]}
    2455 |  # node_Gemm_2455
            %"addmm_25"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_104", %"t_26", %"crosstransformer.layers_t.2.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2456 |  # node_Cast_2456
            %"val_1803"<?,?> ⬅️ ::Cast(%"val_1485") {to=7}
    2457 |  # node_Reshape_2457
            %"view_105"<FLOAT,[1,1723,512]> ⬅️ ::Reshape(%"addmm_25", %"val_1803") {allowzero=0}
    2458 |  # node_Identity_2458
            %"clone_26"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"view_105")
    2459 |  # node_Mul_2459
            %"mul_42"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.2.gamma_2.scale", %"clone_26")
    2460 |  # node_aten_add_2460
            %"add_43"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_42", %"mul_42") {alpha=1.0}
    2461 |  # node_Transpose_2461
            %"transpose_52"<FLOAT,[1,512,1723]> ⬅️ ::Transpose(%"add_43") {perm=[0, 2, 1]}
    2462 |  # node_Constant_2462
            %"val_1804"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2463 |  # node_Reshape_2463
            %"val_1805"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1804") {allowzero=0}
    2464 |  # node_Constant_2464
            %"val_1806"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2465 |  # node_Concat_2465
            %"val_1807"<?,?> ⬅️ ::Concat(%"val_1806", %"val_1805", %"val_1804") {axis=0}
    2466 |  # node_Reshape_2466
            %"val_1808"<?,?> ⬅️ ::Reshape(%"transpose_52", %"val_1807") {allowzero=0}
    2467 |  # node_Constant_2467
            %"val_1809"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2468 |  # node_CastLike_2468
            %"val_1810"<?,?> ⬅️ ::CastLike(%"val_1809", %"transpose_52")
    2469 |  # node_Expand_2469
            %"val_1811"<?,?> ⬅️ ::Expand(%"val_1810", %"val_1805")
    2470 |  # node_Constant_2470
            %"val_1812"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2471 |  # node_CastLike_2471
            %"val_1813"<?,?> ⬅️ ::CastLike(%"val_1812", %"transpose_52")
    2472 |  # node_Expand_2472
            %"val_1814"<?,?> ⬅️ ::Expand(%"val_1813", %"val_1805")
    2473 |  # node_InstanceNormalization_2473
            %"val_1815"<?,?> ⬅️ ::InstanceNormalization(%"val_1808", %"val_1811", %"val_1814") {epsilon=1e-05}
    2474 |  # node_Shape_2474
            %"val_1816"<?,?> ⬅️ ::Shape(%"transpose_52") {start=0}
    2475 |  # node_Reshape_2475
            %"val_1817"<?,?> ⬅️ ::Reshape(%"val_1815", %"val_1816") {allowzero=0}
    2476 |  # node_Constant_2476
            %"val_1818"<?,?> ⬅️ ::Constant() {value_int=1}
    2477 |  # node_Sub_2477
            %"val_1819"<?,?> ⬅️ ::Sub(%"val_50", %"val_1818")
    2478 |  # node_Range_2478
            %"val_1820"<?,?> ⬅️ ::Range(%"val_1818", %"val_1819", %"val_1818")
    2479 |  # node_Unsqueeze_2479
            %"val_1821"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.2.norm_out.weight", %"val_1820")
    2480 |  # node_Unsqueeze_2480
            %"val_1822"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.2.norm_out.bias", %"val_1820")
    2481 |  # node_CastLike_2481
            %"val_1823"<?,?> ⬅️ ::CastLike(%"val_1821", %"val_1817")
    2482 |  # node_Mul_2482
            %"val_1824"<?,?> ⬅️ ::Mul(%"val_1817", %"val_1823")
    2483 |  # node_CastLike_2483
            %"val_1825"<?,?> ⬅️ ::CastLike(%"val_1822", %"val_1824")
    2484 |  # node_Add_2484
            %"group_norm_37"<FLOAT,[1,512,1723]> ⬅️ ::Add(%"val_1824", %"val_1825")
    2485 |  # node_Transpose_2485
            %"transpose_53"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"group_norm_37") {perm=[0, 2, 1]}
    2486 |  # node_LayerNormalization_2486
            %"getitem_68"<FLOAT,[1,3448,512]>, %"native_layer_norm_16__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_16__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"transpose_45", %"crosstransformer.layers.3.norm1.weight", %"crosstransformer.layers.3.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2487 |  # node_LayerNormalization_2487
            %"getitem_71"<FLOAT,[1,1723,512]>, %"native_layer_norm_17__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_17__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"transpose_53", %"crosstransformer.layers.3.norm2.weight", %"crosstransformer.layers.3.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2488 |  # node_Transpose_2488
            %"transpose_54"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_68") {perm=[1, 0, 2]}
    2489 |  # node_Transpose_2489
            %"transpose_55"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_71") {perm=[1, 0, 2]}
    2490 |  # node_aten_split_with_sizes_2490
            %"split_with_sizes_4"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers.3.cross_attn.in_proj_weight", %"val_1509") {dim=0}
    2491 |  # node_aten_getitem_2491
            %"getitem_74"<FLOAT,[512,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_4", %"val_80")
    2492 |  # node_aten_getitem_2492
            %"getitem_75"<FLOAT,[1024,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_4", %"val_35")
    2493 |  # node_aten_split_with_sizes_2493
            %"split_with_sizes_5"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers.3.cross_attn.in_proj_bias", %"val_1509") {dim=0}
    2494 |  # node_aten_getitem_2494
            %"getitem_76"<FLOAT,[512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_5", %"val_80")
    2495 |  # node_aten_getitem_2495
            %"getitem_77"<FLOAT,[1024]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_5", %"val_35")
    2496 |  # node_Cast_2496
            %"val_1826"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2497 |  # node_Reshape_2497
            %"view_106"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_54", %"val_1826") {allowzero=0}
    2498 |  # node_Transpose_2498
            %"t_27"<FLOAT,[512,512]> ⬅️ ::Transpose(%"getitem_74") {perm=[1, 0]}
    2499 |  # node_Gemm_2499
            %"addmm_26"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_106", %"t_27", %"getitem_76") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2500 |  # node_Cast_2500
            %"val_1827"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    2501 |  # node_Reshape_2501
            %"view_107"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_26", %"val_1827") {allowzero=0}
    2502 |  # node_Cast_2502
            %"val_1828"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2503 |  # node_Reshape_2503
            %"view_108"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_55", %"val_1828") {allowzero=0}
    2504 |  # node_Transpose_2504
            %"t_28"<FLOAT,[512,1024]> ⬅️ ::Transpose(%"getitem_75") {perm=[1, 0]}
    2505 |  # node_Gemm_2505
            %"addmm_27"<FLOAT,[1723,1024]> ⬅️ ::Gemm(%"view_108", %"t_28", %"getitem_77") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2506 |  # node_Cast_2506
            %"val_1829"<?,?> ⬅️ ::Cast(%"val_1513") {to=7}
    2507 |  # node_Reshape_2507
            %"view_109"<FLOAT,[1723,1,1024]> ⬅️ ::Reshape(%"addmm_27", %"val_1829") {allowzero=0}
    2508 |  # node_Cast_2508
            %"val_1830"<?,?> ⬅️ ::Cast(%"val_1515") {to=7}
    2509 |  # node_Reshape_2509
            %"view_110"<FLOAT,[1723,1,2,512]> ⬅️ ::Reshape(%"view_109", %"val_1830") {allowzero=0}
    2510 |  # node_aten_unsqueeze_2510
            %"unsqueeze_31"<FLOAT,[1,1723,1,2,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_110") {dim=0}
    2511 |  # node_Transpose_2511
            %"transpose_56"<FLOAT,[2,1723,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_31") {perm=[3, 1, 2, 0, 4]}
    2512 |  # node_aten_squeeze_dim_2512
            %"squeeze_6"<FLOAT,[2,1723,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_56") {dim=-2}
    2513 |  # node_Identity_2513
            %"clone_27"<FLOAT,[2,1723,1,512]> ⬅️ ::Identity(%"squeeze_6")
    2514 |  # node_Gather_2514
            %"select_16"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_27", %"val_80") {axis=0}
    2515 |  # node_Gather_2515
            %"select_17"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_27", %"val_35") {axis=0}
    2516 |  # node_Cast_2516
            %"val_1831"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2517 |  # node_Reshape_2517
            %"view_111"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"view_107", %"val_1831") {allowzero=0}
    2518 |  # node_Transpose_2518
            %"transpose_57"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_111") {perm=[1, 0, 2]}
    2519 |  # node_Cast_2519
            %"val_1832"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2520 |  # node_Reshape_2520
            %"view_112"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_16", %"val_1832") {allowzero=0}
    2521 |  # node_Transpose_2521
            %"transpose_58"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_112") {perm=[1, 0, 2]}
    2522 |  # node_Cast_2522
            %"val_1833"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2523 |  # node_Reshape_2523
            %"view_113"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_17", %"val_1833") {allowzero=0}
    2524 |  # node_Transpose_2524
            %"transpose_59"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_113") {perm=[1, 0, 2]}
    2525 |  # node_Cast_2525
            %"val_1834"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2526 |  # node_Reshape_2526
            %"view_114"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_57", %"val_1834") {allowzero=0}
    2527 |  # node_Cast_2527
            %"val_1835"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2528 |  # node_Reshape_2528
            %"view_115"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_58", %"val_1835") {allowzero=0}
    2529 |  # node_Cast_2529
            %"val_1836"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2530 |  # node_Reshape_2530
            %"view_116"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_59", %"val_1836") {allowzero=0}
    2531 |  # node_Shape_2531
            %"val_1837"<?,?> ⬅️ ::Shape(%"view_114") {start=0}
    2532 |  # node_Constant_2532
            %"val_1838"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2533 |  # node_Gather_2533
            %"val_1839"<?,?> ⬅️ ::Gather(%"val_1837", %"val_1838") {axis=0}
    2534 |  # node_CastLike_2534
            %"val_1840"<?,?> ⬅️ ::CastLike(%"val_1839", %"view_114")
    2535 |  # node_Constant_2535
            %"val_1841"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2536 |  # node_CastLike_2536
            %"val_1842"<?,?> ⬅️ ::CastLike(%"val_1841", %"view_114")
    2537 |  # node_Sqrt_2537
            %"val_1843"<?,?> ⬅️ ::Sqrt(%"val_1840")
    2538 |  # node_Div_2538
            %"val_1844"<?,?> ⬅️ ::Div(%"val_1842", %"val_1843")
    2539 |  # node_CastLike_2539
            %"val_1845"<?,?> ⬅️ ::CastLike(%"val_1844", %"view_114")
    2540 |  # node_Shape_2540
            %"val_1846"<?,?> ⬅️ ::Shape(%"view_115") {start=0}
    2541 |  # node_Constant_2541
            %"val_1847"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2542 |  # node_Slice_2542
            %"val_1848"<?,?> ⬅️ ::Slice(%"val_1846", %"val_1357", %"val_1847", None, None)
    2543 |  # node_Slice_2543
            %"val_1849"<?,?> ⬅️ ::Slice(%"val_1846", %"val_1359", %"val_1357", None, None)
    2544 |  # node_Constant_2544
            %"val_1850"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2545 |  # node_Slice_2545
            %"val_1851"<?,?> ⬅️ ::Slice(%"val_1846", %"val_1850", %"val_1359", None, None)
    2546 |  # node_Constant_2546
            %"val_1852"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2547 |  # node_Concat_2547
            %"val_1853"<?,?> ⬅️ ::Concat(%"val_1852", %"val_1849", %"val_1848") {axis=0}
    2548 |  # node_Reshape_2548
            %"val_1854"<?,?> ⬅️ ::Reshape(%"view_115", %"val_1853") {allowzero=0}
    2549 |  # node_Transpose_2549
            %"val_1855"<?,?> ⬅️ ::Transpose(%"val_1854") {perm=[0, 2, 1]}
    2550 |  # node_Concat_2550
            %"val_1856"<?,?> ⬅️ ::Concat(%"val_1851", %"val_1848", %"val_1849") {axis=0}
    2551 |  # node_Reshape_2551
            %"val_1857"<?,?> ⬅️ ::Reshape(%"val_1855", %"val_1856") {allowzero=0}
    2552 |  # node_Sqrt_2552
            %"val_1858"<?,?> ⬅️ ::Sqrt(%"val_1845")
    2553 |  # node_Mul_2553
            %"val_1859"<?,?> ⬅️ ::Mul(%"view_114", %"val_1858")
    2554 |  # node_Sqrt_2554
            %"val_1860"<?,?> ⬅️ ::Sqrt(%"val_1845")
    2555 |  # node_CastLike_2555
            %"val_1861"<?,?> ⬅️ ::CastLike(%"val_1860", %"val_1857")
    2556 |  # node_Mul_2556
            %"val_1862"<?,?> ⬅️ ::Mul(%"val_1857", %"val_1861")
    2557 |  # node_MatMul_2557
            %"val_1863"<?,?> ⬅️ ::MatMul(%"val_1859", %"val_1862")
    2558 |  # node_Softmax_2558
            %"val_1864"<?,?> ⬅️ ::Softmax(%"val_1863") {axis=-1}
    2559 |  # node_Dropout_2559
            %"val_1865"<?,?>, %"val_1866"<?,?> ⬅️ ::Dropout(%"val_1864", %"val_953", None)
    2560 |  # node_MatMul_2560
            %"getitem_78"<FLOAT,[1,8,3448,64]> ⬅️ ::MatMul(%"val_1865", %"view_116")
    2561 |  # node_Shape_2561
            %"val_1867"<?,?> ⬅️ ::Shape(%"view_114") {start=0}
    2562 |  # node_Slice_2562
            %"val_1868"<?,?> ⬅️ ::Slice(%"val_1867", %"val_1379", %"val_1380", None, None)
    2563 |  # node_Slice_2563
            %"val_1869"<?,?> ⬅️ ::Slice(%"val_1867", %"val_1380", %"val_1382", None, None)
    2564 |  # node_Slice_2564
            %"val_1870"<?,?> ⬅️ ::Slice(%"val_1867", %"val_1359", %"val_1357", None, None)
    2565 |  # node_Cast_2565
            %"val_1871"<?,?> ⬅️ ::Cast(%"val_1869") {to=1}
    2566 |  # node_Div_2566
            %"val_1872"<?,?> ⬅️ ::Div(%"val_1871", %"val_1386")
    2567 |  # node_Ceil_2567
            %"val_1873"<?,?> ⬅️ ::Ceil(%"val_1872")
    2568 |  # node_Mul_2568
            %"val_1874"<?,?> ⬅️ ::Mul(%"val_1873", %"val_1386")
    2569 |  # node_Cast_2569
            %"val_1875"<?,?> ⬅️ ::Cast(%"val_1874") {to=7}
    2570 |  # node_Concat_2570
            %"val_1876"<?,?> ⬅️ ::Concat(%"val_1868", %"val_1870", %"val_1875") {axis=0}
    2571 |  # node_Expand_2571
            %"_scaled_dot_product_flash_attention_for_cpu_6__1"<FLOAT,[1,8,3448]> ⬅️ ::Expand(%"val_953", %"val_1876")
    2572 |  # node_Transpose_2572
            %"permute_19"<FLOAT,[3448,1,8,64]> ⬅️ ::Transpose(%"getitem_78") {perm=[2, 0, 1, 3]}
    2573 |  # node_Cast_2573
            %"val_1877"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2574 |  # node_Reshape_2574
            %"view_117"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"permute_19", %"val_1877") {allowzero=0}
    2575 |  # node_Transpose_2575
            %"t_29"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers.3.cross_attn.out_proj.weight") {perm=[1, 0]}
    2576 |  # node_Gemm_2576
            %"addmm_28"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_117", %"t_29", %"crosstransformer.layers.3.cross_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2577 |  # node_Cast_2577
            %"val_1878"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    2578 |  # node_Reshape_2578
            %"view_118"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_28", %"val_1878") {allowzero=0}
    2579 |  # node_Transpose_2579
            %"transpose_60"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"view_118") {perm=[1, 0, 2]}
    2580 |  # node_Identity_2580
            %"clone_28"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"transpose_60")
    2581 |  # node_Mul_2581
            %"mul_43"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.3.gamma_1.scale", %"clone_28")
    2582 |  # node_aten_add_2582
            %"add_44"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_45", %"mul_43") {alpha=1.0}
    2583 |  # node_LayerNormalization_2583
            %"getitem_80"<FLOAT,[1,3448,512]>, %"native_layer_norm_18__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_18__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"add_44", %"crosstransformer.layers.3.norm3.weight", %"crosstransformer.layers.3.norm3.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2584 |  # node_Cast_2584
            %"val_1879"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2585 |  # node_Reshape_2585
            %"view_119"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"getitem_80", %"val_1879") {allowzero=0}
    2586 |  # node_Transpose_2586
            %"t_30"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers.3.linear1.weight") {perm=[1, 0]}
    2587 |  # node_Gemm_2587
            %"addmm_29"<FLOAT,[3448,2048]> ⬅️ ::Gemm(%"view_119", %"t_30", %"crosstransformer.layers.3.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2588 |  # node_Cast_2588
            %"val_1880"<?,?> ⬅️ ::Cast(%"val_1396") {to=7}
    2589 |  # node_Reshape_2589
            %"view_120"<FLOAT,[1,3448,2048]> ⬅️ ::Reshape(%"addmm_29", %"val_1880") {allowzero=0}
    2590 |  # node__aten_gelu_approximate_none_2590
            %"gelu_30"<FLOAT,[1,3448,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_120")
    2591 |  # node_Identity_2591
            %"clone_29"<FLOAT,[1,3448,2048]> ⬅️ ::Identity(%"gelu_30")
    2592 |  # node_Cast_2592
            %"val_1881"<?,?> ⬅️ ::Cast(%"val_1398") {to=7}
    2593 |  # node_Reshape_2593
            %"view_121"<FLOAT,[3448,2048]> ⬅️ ::Reshape(%"clone_29", %"val_1881") {allowzero=0}
    2594 |  # node_Transpose_2594
            %"t_31"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers.3.linear2.weight") {perm=[1, 0]}
    2595 |  # node_Gemm_2595
            %"addmm_30"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_121", %"t_31", %"crosstransformer.layers.3.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2596 |  # node_Cast_2596
            %"val_1882"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    2597 |  # node_Reshape_2597
            %"view_122"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"addmm_30", %"val_1882") {allowzero=0}
    2598 |  # node_Identity_2598
            %"clone_30"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"view_122")
    2599 |  # node_Mul_2599
            %"mul_44"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.3.gamma_2.scale", %"clone_30")
    2600 |  # node_aten_add_2600
            %"add_45"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_44", %"mul_44") {alpha=1.0}
    2601 |  # node_Transpose_2601
            %"transpose_61"<FLOAT,[1,512,3448]> ⬅️ ::Transpose(%"add_45") {perm=[0, 2, 1]}
    2602 |  # node_Constant_2602
            %"val_1883"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2603 |  # node_Reshape_2603
            %"val_1884"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1883") {allowzero=0}
    2604 |  # node_Constant_2604
            %"val_1885"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2605 |  # node_Concat_2605
            %"val_1886"<?,?> ⬅️ ::Concat(%"val_1885", %"val_1884", %"val_1883") {axis=0}
    2606 |  # node_Reshape_2606
            %"val_1887"<?,?> ⬅️ ::Reshape(%"transpose_61", %"val_1886") {allowzero=0}
    2607 |  # node_Constant_2607
            %"val_1888"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2608 |  # node_CastLike_2608
            %"val_1889"<?,?> ⬅️ ::CastLike(%"val_1888", %"transpose_61")
    2609 |  # node_Expand_2609
            %"val_1890"<?,?> ⬅️ ::Expand(%"val_1889", %"val_1884")
    2610 |  # node_Constant_2610
            %"val_1891"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2611 |  # node_CastLike_2611
            %"val_1892"<?,?> ⬅️ ::CastLike(%"val_1891", %"transpose_61")
    2612 |  # node_Expand_2612
            %"val_1893"<?,?> ⬅️ ::Expand(%"val_1892", %"val_1884")
    2613 |  # node_InstanceNormalization_2613
            %"val_1894"<?,?> ⬅️ ::InstanceNormalization(%"val_1887", %"val_1890", %"val_1893") {epsilon=1e-05}
    2614 |  # node_Shape_2614
            %"val_1895"<?,?> ⬅️ ::Shape(%"transpose_61") {start=0}
    2615 |  # node_Reshape_2615
            %"val_1896"<?,?> ⬅️ ::Reshape(%"val_1894", %"val_1895") {allowzero=0}
    2616 |  # node_Constant_2616
            %"val_1897"<?,?> ⬅️ ::Constant() {value_int=1}
    2617 |  # node_Sub_2617
            %"val_1898"<?,?> ⬅️ ::Sub(%"val_50", %"val_1897")
    2618 |  # node_Range_2618
            %"val_1899"<?,?> ⬅️ ::Range(%"val_1897", %"val_1898", %"val_1897")
    2619 |  # node_Unsqueeze_2619
            %"val_1900"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.3.norm_out.weight", %"val_1899")
    2620 |  # node_Unsqueeze_2620
            %"val_1901"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.3.norm_out.bias", %"val_1899")
    2621 |  # node_CastLike_2621
            %"val_1902"<?,?> ⬅️ ::CastLike(%"val_1900", %"val_1896")
    2622 |  # node_Mul_2622
            %"val_1903"<?,?> ⬅️ ::Mul(%"val_1896", %"val_1902")
    2623 |  # node_CastLike_2623
            %"val_1904"<?,?> ⬅️ ::CastLike(%"val_1901", %"val_1903")
    2624 |  # node_Add_2624
            %"group_norm_38"<FLOAT,[1,512,3448]> ⬅️ ::Add(%"val_1903", %"val_1904")
    2625 |  # node_Transpose_2625
            %"transpose_62"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"group_norm_38") {perm=[0, 2, 1]}
    2626 |  # node_LayerNormalization_2626
            %"getitem_83"<FLOAT,[1,1723,512]>, %"native_layer_norm_19__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_19__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"transpose_53", %"crosstransformer.layers_t.3.norm1.weight", %"crosstransformer.layers_t.3.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2627 |  # node_LayerNormalization_2627
            %"getitem_86"<FLOAT,[1,3448,512]>, %"native_layer_norm_20__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_20__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"transpose_45", %"crosstransformer.layers_t.3.norm2.weight", %"crosstransformer.layers_t.3.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2628 |  # node_Transpose_2628
            %"transpose_63"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_83") {perm=[1, 0, 2]}
    2629 |  # node_Transpose_2629
            %"transpose_64"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_86") {perm=[1, 0, 2]}
    2630 |  # node_aten_split_with_sizes_2630
            %"split_with_sizes_6"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers_t.3.cross_attn.in_proj_weight", %"val_1509") {dim=0}
    2631 |  # node_aten_getitem_2631
            %"getitem_89"<FLOAT,[512,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_6", %"val_80")
    2632 |  # node_aten_getitem_2632
            %"getitem_90"<FLOAT,[1024,512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_6", %"val_35")
    2633 |  # node_aten_split_with_sizes_2633
            %"split_with_sizes_7"<?,?> ⬅️ pkg.onnxscript.torch_lib::aten_split_with_sizes(%"crosstransformer.layers_t.3.cross_attn.in_proj_bias", %"val_1509") {dim=0}
    2634 |  # node_aten_getitem_2634
            %"getitem_91"<FLOAT,[512]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_7", %"val_80")
    2635 |  # node_aten_getitem_2635
            %"getitem_92"<FLOAT,[1024]> ⬅️ pkg.onnxscript.torch_lib::aten_getitem(%"split_with_sizes_7", %"val_35")
    2636 |  # node_Cast_2636
            %"val_1905"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2637 |  # node_Reshape_2637
            %"view_123"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_63", %"val_1905") {allowzero=0}
    2638 |  # node_Transpose_2638
            %"t_32"<FLOAT,[512,512]> ⬅️ ::Transpose(%"getitem_89") {perm=[1, 0]}
    2639 |  # node_Gemm_2639
            %"addmm_31"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_123", %"t_32", %"getitem_91") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2640 |  # node_Cast_2640
            %"val_1906"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    2641 |  # node_Reshape_2641
            %"view_124"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_31", %"val_1906") {allowzero=0}
    2642 |  # node_Cast_2642
            %"val_1907"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2643 |  # node_Reshape_2643
            %"view_125"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_64", %"val_1907") {allowzero=0}
    2644 |  # node_Transpose_2644
            %"t_33"<FLOAT,[512,1024]> ⬅️ ::Transpose(%"getitem_90") {perm=[1, 0]}
    2645 |  # node_Gemm_2645
            %"addmm_32"<FLOAT,[3448,1024]> ⬅️ ::Gemm(%"view_125", %"t_33", %"getitem_92") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2646 |  # node_Cast_2646
            %"val_1908"<?,?> ⬅️ ::Cast(%"val_1594") {to=7}
    2647 |  # node_Reshape_2647
            %"view_126"<FLOAT,[3448,1,1024]> ⬅️ ::Reshape(%"addmm_32", %"val_1908") {allowzero=0}
    2648 |  # node_Cast_2648
            %"val_1909"<?,?> ⬅️ ::Cast(%"val_1596") {to=7}
    2649 |  # node_Reshape_2649
            %"view_127"<FLOAT,[3448,1,2,512]> ⬅️ ::Reshape(%"view_126", %"val_1909") {allowzero=0}
    2650 |  # node_aten_unsqueeze_2650
            %"unsqueeze_32"<FLOAT,[1,3448,1,2,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_127") {dim=0}
    2651 |  # node_Transpose_2651
            %"transpose_65"<FLOAT,[2,3448,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_32") {perm=[3, 1, 2, 0, 4]}
    2652 |  # node_aten_squeeze_dim_2652
            %"squeeze_7"<FLOAT,[2,3448,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_65") {dim=-2}
    2653 |  # node_Identity_2653
            %"clone_31"<FLOAT,[2,3448,1,512]> ⬅️ ::Identity(%"squeeze_7")
    2654 |  # node_Gather_2654
            %"select_18"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_31", %"val_80") {axis=0}
    2655 |  # node_Gather_2655
            %"select_19"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_31", %"val_35") {axis=0}
    2656 |  # node_Cast_2656
            %"val_1910"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2657 |  # node_Reshape_2657
            %"view_128"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"view_124", %"val_1910") {allowzero=0}
    2658 |  # node_Transpose_2658
            %"transpose_66"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_128") {perm=[1, 0, 2]}
    2659 |  # node_Cast_2659
            %"val_1911"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2660 |  # node_Reshape_2660
            %"view_129"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_18", %"val_1911") {allowzero=0}
    2661 |  # node_Transpose_2661
            %"transpose_67"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_129") {perm=[1, 0, 2]}
    2662 |  # node_Cast_2662
            %"val_1912"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2663 |  # node_Reshape_2663
            %"view_130"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_19", %"val_1912") {allowzero=0}
    2664 |  # node_Transpose_2664
            %"transpose_68"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_130") {perm=[1, 0, 2]}
    2665 |  # node_Cast_2665
            %"val_1913"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2666 |  # node_Reshape_2666
            %"view_131"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_66", %"val_1913") {allowzero=0}
    2667 |  # node_Cast_2667
            %"val_1914"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2668 |  # node_Reshape_2668
            %"view_132"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_67", %"val_1914") {allowzero=0}
    2669 |  # node_Cast_2669
            %"val_1915"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2670 |  # node_Reshape_2670
            %"view_133"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_68", %"val_1915") {allowzero=0}
    2671 |  # node_Shape_2671
            %"val_1916"<?,?> ⬅️ ::Shape(%"view_131") {start=0}
    2672 |  # node_Constant_2672
            %"val_1917"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2673 |  # node_Gather_2673
            %"val_1918"<?,?> ⬅️ ::Gather(%"val_1916", %"val_1917") {axis=0}
    2674 |  # node_CastLike_2674
            %"val_1919"<?,?> ⬅️ ::CastLike(%"val_1918", %"view_131")
    2675 |  # node_Constant_2675
            %"val_1920"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2676 |  # node_CastLike_2676
            %"val_1921"<?,?> ⬅️ ::CastLike(%"val_1920", %"view_131")
    2677 |  # node_Sqrt_2677
            %"val_1922"<?,?> ⬅️ ::Sqrt(%"val_1919")
    2678 |  # node_Div_2678
            %"val_1923"<?,?> ⬅️ ::Div(%"val_1921", %"val_1922")
    2679 |  # node_CastLike_2679
            %"val_1924"<?,?> ⬅️ ::CastLike(%"val_1923", %"view_131")
    2680 |  # node_Shape_2680
            %"val_1925"<?,?> ⬅️ ::Shape(%"view_132") {start=0}
    2681 |  # node_Constant_2681
            %"val_1926"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2682 |  # node_Slice_2682
            %"val_1927"<?,?> ⬅️ ::Slice(%"val_1925", %"val_1357", %"val_1926", None, None)
    2683 |  # node_Slice_2683
            %"val_1928"<?,?> ⬅️ ::Slice(%"val_1925", %"val_1359", %"val_1357", None, None)
    2684 |  # node_Constant_2684
            %"val_1929"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2685 |  # node_Slice_2685
            %"val_1930"<?,?> ⬅️ ::Slice(%"val_1925", %"val_1929", %"val_1359", None, None)
    2686 |  # node_Constant_2686
            %"val_1931"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2687 |  # node_Concat_2687
            %"val_1932"<?,?> ⬅️ ::Concat(%"val_1931", %"val_1928", %"val_1927") {axis=0}
    2688 |  # node_Reshape_2688
            %"val_1933"<?,?> ⬅️ ::Reshape(%"view_132", %"val_1932") {allowzero=0}
    2689 |  # node_Transpose_2689
            %"val_1934"<?,?> ⬅️ ::Transpose(%"val_1933") {perm=[0, 2, 1]}
    2690 |  # node_Concat_2690
            %"val_1935"<?,?> ⬅️ ::Concat(%"val_1930", %"val_1927", %"val_1928") {axis=0}
    2691 |  # node_Reshape_2691
            %"val_1936"<?,?> ⬅️ ::Reshape(%"val_1934", %"val_1935") {allowzero=0}
    2692 |  # node_Sqrt_2692
            %"val_1937"<?,?> ⬅️ ::Sqrt(%"val_1924")
    2693 |  # node_Mul_2693
            %"val_1938"<?,?> ⬅️ ::Mul(%"view_131", %"val_1937")
    2694 |  # node_Sqrt_2694
            %"val_1939"<?,?> ⬅️ ::Sqrt(%"val_1924")
    2695 |  # node_CastLike_2695
            %"val_1940"<?,?> ⬅️ ::CastLike(%"val_1939", %"val_1936")
    2696 |  # node_Mul_2696
            %"val_1941"<?,?> ⬅️ ::Mul(%"val_1936", %"val_1940")
    2697 |  # node_MatMul_2697
            %"val_1942"<?,?> ⬅️ ::MatMul(%"val_1938", %"val_1941")
    2698 |  # node_Softmax_2698
            %"val_1943"<?,?> ⬅️ ::Softmax(%"val_1942") {axis=-1}
    2699 |  # node_Dropout_2699
            %"val_1944"<?,?>, %"val_1945"<?,?> ⬅️ ::Dropout(%"val_1943", %"val_953", None)
    2700 |  # node_MatMul_2700
            %"getitem_93"<FLOAT,[1,8,1723,64]> ⬅️ ::MatMul(%"val_1944", %"view_133")
    2701 |  # node_Shape_2701
            %"val_1946"<?,?> ⬅️ ::Shape(%"view_131") {start=0}
    2702 |  # node_Slice_2702
            %"val_1947"<?,?> ⬅️ ::Slice(%"val_1946", %"val_1379", %"val_1380", None, None)
    2703 |  # node_Slice_2703
            %"val_1948"<?,?> ⬅️ ::Slice(%"val_1946", %"val_1380", %"val_1382", None, None)
    2704 |  # node_Slice_2704
            %"val_1949"<?,?> ⬅️ ::Slice(%"val_1946", %"val_1359", %"val_1357", None, None)
    2705 |  # node_Cast_2705
            %"val_1950"<?,?> ⬅️ ::Cast(%"val_1948") {to=1}
    2706 |  # node_Div_2706
            %"val_1951"<?,?> ⬅️ ::Div(%"val_1950", %"val_1386")
    2707 |  # node_Ceil_2707
            %"val_1952"<?,?> ⬅️ ::Ceil(%"val_1951")
    2708 |  # node_Mul_2708
            %"val_1953"<?,?> ⬅️ ::Mul(%"val_1952", %"val_1386")
    2709 |  # node_Cast_2709
            %"val_1954"<?,?> ⬅️ ::Cast(%"val_1953") {to=7}
    2710 |  # node_Concat_2710
            %"val_1955"<?,?> ⬅️ ::Concat(%"val_1947", %"val_1949", %"val_1954") {axis=0}
    2711 |  # node_Expand_2711
            %"_scaled_dot_product_flash_attention_for_cpu_7__1"<FLOAT,[1,8,1723]> ⬅️ ::Expand(%"val_953", %"val_1955")
    2712 |  # node_Transpose_2712
            %"permute_20"<FLOAT,[1723,1,8,64]> ⬅️ ::Transpose(%"getitem_93") {perm=[2, 0, 1, 3]}
    2713 |  # node_Cast_2713
            %"val_1956"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2714 |  # node_Reshape_2714
            %"view_134"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"permute_20", %"val_1956") {allowzero=0}
    2715 |  # node_Transpose_2715
            %"t_34"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.3.cross_attn.out_proj.weight") {perm=[1, 0]}
    2716 |  # node_Gemm_2716
            %"addmm_33"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_134", %"t_34", %"crosstransformer.layers_t.3.cross_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2717 |  # node_Cast_2717
            %"val_1957"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    2718 |  # node_Reshape_2718
            %"view_135"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_33", %"val_1957") {allowzero=0}
    2719 |  # node_Transpose_2719
            %"transpose_69"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"view_135") {perm=[1, 0, 2]}
    2720 |  # node_Identity_2720
            %"clone_32"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"transpose_69")
    2721 |  # node_Mul_2721
            %"mul_45"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.3.gamma_1.scale", %"clone_32")
    2722 |  # node_aten_add_2722
            %"add_46"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_53", %"mul_45") {alpha=1.0}
    2723 |  # node_LayerNormalization_2723
            %"getitem_95"<FLOAT,[1,1723,512]>, %"native_layer_norm_21__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_21__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"add_46", %"crosstransformer.layers_t.3.norm3.weight", %"crosstransformer.layers_t.3.norm3.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2724 |  # node_Cast_2724
            %"val_1958"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2725 |  # node_Reshape_2725
            %"view_136"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"getitem_95", %"val_1958") {allowzero=0}
    2726 |  # node_Transpose_2726
            %"t_35"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers_t.3.linear1.weight") {perm=[1, 0]}
    2727 |  # node_Gemm_2727
            %"addmm_34"<FLOAT,[1723,2048]> ⬅️ ::Gemm(%"view_136", %"t_35", %"crosstransformer.layers_t.3.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2728 |  # node_Cast_2728
            %"val_1959"<?,?> ⬅️ ::Cast(%"val_1481") {to=7}
    2729 |  # node_Reshape_2729
            %"view_137"<FLOAT,[1,1723,2048]> ⬅️ ::Reshape(%"addmm_34", %"val_1959") {allowzero=0}
    2730 |  # node__aten_gelu_approximate_none_2730
            %"gelu_31"<FLOAT,[1,1723,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_137")
    2731 |  # node_Identity_2731
            %"clone_33"<FLOAT,[1,1723,2048]> ⬅️ ::Identity(%"gelu_31")
    2732 |  # node_Cast_2732
            %"val_1960"<?,?> ⬅️ ::Cast(%"val_1483") {to=7}
    2733 |  # node_Reshape_2733
            %"view_138"<FLOAT,[1723,2048]> ⬅️ ::Reshape(%"clone_33", %"val_1960") {allowzero=0}
    2734 |  # node_Transpose_2734
            %"t_36"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.3.linear2.weight") {perm=[1, 0]}
    2735 |  # node_Gemm_2735
            %"addmm_35"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_138", %"t_36", %"crosstransformer.layers_t.3.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2736 |  # node_Cast_2736
            %"val_1961"<?,?> ⬅️ ::Cast(%"val_1485") {to=7}
    2737 |  # node_Reshape_2737
            %"view_139"<FLOAT,[1,1723,512]> ⬅️ ::Reshape(%"addmm_35", %"val_1961") {allowzero=0}
    2738 |  # node_Identity_2738
            %"clone_34"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"view_139")
    2739 |  # node_Mul_2739
            %"mul_46"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.3.gamma_2.scale", %"clone_34")
    2740 |  # node_aten_add_2740
            %"add_47"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_46", %"mul_46") {alpha=1.0}
    2741 |  # node_Transpose_2741
            %"transpose_70"<FLOAT,[1,512,1723]> ⬅️ ::Transpose(%"add_47") {perm=[0, 2, 1]}
    2742 |  # node_Constant_2742
            %"val_1962"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2743 |  # node_Reshape_2743
            %"val_1963"<?,?> ⬅️ ::Reshape(%"val_35", %"val_1962") {allowzero=0}
    2744 |  # node_Constant_2744
            %"val_1964"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2745 |  # node_Concat_2745
            %"val_1965"<?,?> ⬅️ ::Concat(%"val_1964", %"val_1963", %"val_1962") {axis=0}
    2746 |  # node_Reshape_2746
            %"val_1966"<?,?> ⬅️ ::Reshape(%"transpose_70", %"val_1965") {allowzero=0}
    2747 |  # node_Constant_2747
            %"val_1967"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2748 |  # node_CastLike_2748
            %"val_1968"<?,?> ⬅️ ::CastLike(%"val_1967", %"transpose_70")
    2749 |  # node_Expand_2749
            %"val_1969"<?,?> ⬅️ ::Expand(%"val_1968", %"val_1963")
    2750 |  # node_Constant_2750
            %"val_1970"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2751 |  # node_CastLike_2751
            %"val_1971"<?,?> ⬅️ ::CastLike(%"val_1970", %"transpose_70")
    2752 |  # node_Expand_2752
            %"val_1972"<?,?> ⬅️ ::Expand(%"val_1971", %"val_1963")
    2753 |  # node_InstanceNormalization_2753
            %"val_1973"<?,?> ⬅️ ::InstanceNormalization(%"val_1966", %"val_1969", %"val_1972") {epsilon=1e-05}
    2754 |  # node_Shape_2754
            %"val_1974"<?,?> ⬅️ ::Shape(%"transpose_70") {start=0}
    2755 |  # node_Reshape_2755
            %"val_1975"<?,?> ⬅️ ::Reshape(%"val_1973", %"val_1974") {allowzero=0}
    2756 |  # node_Constant_2756
            %"val_1976"<?,?> ⬅️ ::Constant() {value_int=1}
    2757 |  # node_Sub_2757
            %"val_1977"<?,?> ⬅️ ::Sub(%"val_50", %"val_1976")
    2758 |  # node_Range_2758
            %"val_1978"<?,?> ⬅️ ::Range(%"val_1976", %"val_1977", %"val_1976")
    2759 |  # node_Unsqueeze_2759
            %"val_1979"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.3.norm_out.weight", %"val_1978")
    2760 |  # node_Unsqueeze_2760
            %"val_1980"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.3.norm_out.bias", %"val_1978")
    2761 |  # node_CastLike_2761
            %"val_1981"<?,?> ⬅️ ::CastLike(%"val_1979", %"val_1975")
    2762 |  # node_Mul_2762
            %"val_1982"<?,?> ⬅️ ::Mul(%"val_1975", %"val_1981")
    2763 |  # node_CastLike_2763
            %"val_1983"<?,?> ⬅️ ::CastLike(%"val_1980", %"val_1982")
    2764 |  # node_Add_2764
            %"group_norm_39"<FLOAT,[1,512,1723]> ⬅️ ::Add(%"val_1982", %"val_1983")
    2765 |  # node_Transpose_2765
            %"transpose_71"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"group_norm_39") {perm=[0, 2, 1]}
    2766 |  # node_LayerNormalization_2766
            %"getitem_98"<FLOAT,[1,3448,512]>, %"native_layer_norm_22__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_22__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"transpose_62", %"crosstransformer.layers.4.norm1.weight", %"crosstransformer.layers.4.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2767 |  # node_Transpose_2767
            %"transpose_72"<FLOAT,[3448,1,512]> ⬅️ ::Transpose(%"getitem_98") {perm=[1, 0, 2]}
    2768 |  # node_Cast_2768
            %"val_1984"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2769 |  # node_Reshape_2769
            %"view_140"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"transpose_72", %"val_1984") {allowzero=0}
    2770 |  # node_Transpose_2770
            %"t_37"<FLOAT,[512,1536]> ⬅️ ::Transpose(%"crosstransformer.layers.4.self_attn.in_proj_weight") {perm=[1, 0]}
    2771 |  # node_Gemm_2771
            %"addmm_36"<FLOAT,[3448,1536]> ⬅️ ::Gemm(%"view_140", %"t_37", %"crosstransformer.layers.4.self_attn.in_proj_bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2772 |  # node_Cast_2772
            %"val_1985"<?,?> ⬅️ ::Cast(%"val_1334") {to=7}
    2773 |  # node_Reshape_2773
            %"view_141"<FLOAT,[3448,1,1536]> ⬅️ ::Reshape(%"addmm_36", %"val_1985") {allowzero=0}
    2774 |  # node_Cast_2774
            %"val_1986"<?,?> ⬅️ ::Cast(%"val_1336") {to=7}
    2775 |  # node_Reshape_2775
            %"view_142"<FLOAT,[3448,1,3,512]> ⬅️ ::Reshape(%"view_141", %"val_1986") {allowzero=0}
    2776 |  # node_aten_unsqueeze_2776
            %"unsqueeze_33"<FLOAT,[1,3448,1,3,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_142") {dim=0}
    2777 |  # node_Transpose_2777
            %"transpose_73"<FLOAT,[3,3448,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_33") {perm=[3, 1, 2, 0, 4]}
    2778 |  # node_aten_squeeze_dim_2778
            %"squeeze_8"<FLOAT,[3,3448,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_73") {dim=-2}
    2779 |  # node_Identity_2779
            %"clone_35"<FLOAT,[3,3448,1,512]> ⬅️ ::Identity(%"squeeze_8")
    2780 |  # node_Gather_2780
            %"select_20"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_35", %"val_80") {axis=0}
    2781 |  # node_Gather_2781
            %"select_21"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_35", %"val_35") {axis=0}
    2782 |  # node_Gather_2782
            %"select_22"<FLOAT,[3448,1,512]> ⬅️ ::Gather(%"clone_35", %"val_276") {axis=0}
    2783 |  # node_Cast_2783
            %"val_1987"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2784 |  # node_Reshape_2784
            %"view_143"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_20", %"val_1987") {allowzero=0}
    2785 |  # node_Transpose_2785
            %"transpose_74"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_143") {perm=[1, 0, 2]}
    2786 |  # node_Cast_2786
            %"val_1988"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2787 |  # node_Reshape_2787
            %"view_144"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_21", %"val_1988") {allowzero=0}
    2788 |  # node_Transpose_2788
            %"transpose_75"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_144") {perm=[1, 0, 2]}
    2789 |  # node_Cast_2789
            %"val_1989"<?,?> ⬅️ ::Cast(%"val_1338") {to=7}
    2790 |  # node_Reshape_2790
            %"view_145"<FLOAT,[3448,8,64]> ⬅️ ::Reshape(%"select_22", %"val_1989") {allowzero=0}
    2791 |  # node_Transpose_2791
            %"transpose_76"<FLOAT,[8,3448,64]> ⬅️ ::Transpose(%"view_145") {perm=[1, 0, 2]}
    2792 |  # node_Cast_2792
            %"val_1990"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2793 |  # node_Reshape_2793
            %"view_146"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_74", %"val_1990") {allowzero=0}
    2794 |  # node_Cast_2794
            %"val_1991"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2795 |  # node_Reshape_2795
            %"view_147"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_75", %"val_1991") {allowzero=0}
    2796 |  # node_Cast_2796
            %"val_1992"<?,?> ⬅️ ::Cast(%"val_1342") {to=7}
    2797 |  # node_Reshape_2797
            %"view_148"<FLOAT,[1,8,3448,64]> ⬅️ ::Reshape(%"transpose_76", %"val_1992") {allowzero=0}
    2798 |  # node_Shape_2798
            %"val_1993"<?,?> ⬅️ ::Shape(%"view_146") {start=0}
    2799 |  # node_Constant_2799
            %"val_1994"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2800 |  # node_Gather_2800
            %"val_1995"<?,?> ⬅️ ::Gather(%"val_1993", %"val_1994") {axis=0}
    2801 |  # node_CastLike_2801
            %"val_1996"<?,?> ⬅️ ::CastLike(%"val_1995", %"view_146")
    2802 |  # node_Constant_2802
            %"val_1997"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2803 |  # node_CastLike_2803
            %"val_1998"<?,?> ⬅️ ::CastLike(%"val_1997", %"view_146")
    2804 |  # node_Sqrt_2804
            %"val_1999"<?,?> ⬅️ ::Sqrt(%"val_1996")
    2805 |  # node_Div_2805
            %"val_2000"<?,?> ⬅️ ::Div(%"val_1998", %"val_1999")
    2806 |  # node_CastLike_2806
            %"val_2001"<?,?> ⬅️ ::CastLike(%"val_2000", %"view_146")
    2807 |  # node_Shape_2807
            %"val_2002"<?,?> ⬅️ ::Shape(%"view_147") {start=0}
    2808 |  # node_Constant_2808
            %"val_2003"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2809 |  # node_Slice_2809
            %"val_2004"<?,?> ⬅️ ::Slice(%"val_2002", %"val_1357", %"val_2003", None, None)
    2810 |  # node_Slice_2810
            %"val_2005"<?,?> ⬅️ ::Slice(%"val_2002", %"val_1359", %"val_1357", None, None)
    2811 |  # node_Constant_2811
            %"val_2006"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2812 |  # node_Slice_2812
            %"val_2007"<?,?> ⬅️ ::Slice(%"val_2002", %"val_2006", %"val_1359", None, None)
    2813 |  # node_Constant_2813
            %"val_2008"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2814 |  # node_Concat_2814
            %"val_2009"<?,?> ⬅️ ::Concat(%"val_2008", %"val_2005", %"val_2004") {axis=0}
    2815 |  # node_Reshape_2815
            %"val_2010"<?,?> ⬅️ ::Reshape(%"view_147", %"val_2009") {allowzero=0}
    2816 |  # node_Transpose_2816
            %"val_2011"<?,?> ⬅️ ::Transpose(%"val_2010") {perm=[0, 2, 1]}
    2817 |  # node_Concat_2817
            %"val_2012"<?,?> ⬅️ ::Concat(%"val_2007", %"val_2004", %"val_2005") {axis=0}
    2818 |  # node_Reshape_2818
            %"val_2013"<?,?> ⬅️ ::Reshape(%"val_2011", %"val_2012") {allowzero=0}
    2819 |  # node_Sqrt_2819
            %"val_2014"<?,?> ⬅️ ::Sqrt(%"val_2001")
    2820 |  # node_Mul_2820
            %"val_2015"<?,?> ⬅️ ::Mul(%"view_146", %"val_2014")
    2821 |  # node_Sqrt_2821
            %"val_2016"<?,?> ⬅️ ::Sqrt(%"val_2001")
    2822 |  # node_CastLike_2822
            %"val_2017"<?,?> ⬅️ ::CastLike(%"val_2016", %"val_2013")
    2823 |  # node_Mul_2823
            %"val_2018"<?,?> ⬅️ ::Mul(%"val_2013", %"val_2017")
    2824 |  # node_MatMul_2824
            %"val_2019"<?,?> ⬅️ ::MatMul(%"val_2015", %"val_2018")
    2825 |  # node_Softmax_2825
            %"val_2020"<?,?> ⬅️ ::Softmax(%"val_2019") {axis=-1}
    2826 |  # node_Dropout_2826
            %"val_2021"<?,?>, %"val_2022"<?,?> ⬅️ ::Dropout(%"val_2020", %"val_953", None)
    2827 |  # node_MatMul_2827
            %"getitem_101"<FLOAT,[1,8,3448,64]> ⬅️ ::MatMul(%"val_2021", %"view_148")
    2828 |  # node_Shape_2828
            %"val_2023"<?,?> ⬅️ ::Shape(%"view_146") {start=0}
    2829 |  # node_Slice_2829
            %"val_2024"<?,?> ⬅️ ::Slice(%"val_2023", %"val_1379", %"val_1380", None, None)
    2830 |  # node_Slice_2830
            %"val_2025"<?,?> ⬅️ ::Slice(%"val_2023", %"val_1380", %"val_1382", None, None)
    2831 |  # node_Slice_2831
            %"val_2026"<?,?> ⬅️ ::Slice(%"val_2023", %"val_1359", %"val_1357", None, None)
    2832 |  # node_Cast_2832
            %"val_2027"<?,?> ⬅️ ::Cast(%"val_2025") {to=1}
    2833 |  # node_Div_2833
            %"val_2028"<?,?> ⬅️ ::Div(%"val_2027", %"val_1386")
    2834 |  # node_Ceil_2834
            %"val_2029"<?,?> ⬅️ ::Ceil(%"val_2028")
    2835 |  # node_Mul_2835
            %"val_2030"<?,?> ⬅️ ::Mul(%"val_2029", %"val_1386")
    2836 |  # node_Cast_2836
            %"val_2031"<?,?> ⬅️ ::Cast(%"val_2030") {to=7}
    2837 |  # node_Concat_2837
            %"val_2032"<?,?> ⬅️ ::Concat(%"val_2024", %"val_2026", %"val_2031") {axis=0}
    2838 |  # node_Expand_2838
            %"_scaled_dot_product_flash_attention_for_cpu_8__1"<FLOAT,[1,8,3448]> ⬅️ ::Expand(%"val_953", %"val_2032")
    2839 |  # node_Transpose_2839
            %"permute_21"<FLOAT,[3448,1,8,64]> ⬅️ ::Transpose(%"getitem_101") {perm=[2, 0, 1, 3]}
    2840 |  # node_Cast_2840
            %"val_2033"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2841 |  # node_Reshape_2841
            %"view_149"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"permute_21", %"val_2033") {allowzero=0}
    2842 |  # node_Transpose_2842
            %"t_38"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers.4.self_attn.out_proj.weight") {perm=[1, 0]}
    2843 |  # node_Gemm_2843
            %"addmm_37"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_149", %"t_38", %"crosstransformer.layers.4.self_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2844 |  # node_Cast_2844
            %"val_2034"<?,?> ⬅️ ::Cast(%"val_1393") {to=7}
    2845 |  # node_Reshape_2845
            %"view_150"<FLOAT,[3448,1,512]> ⬅️ ::Reshape(%"addmm_37", %"val_2034") {allowzero=0}
    2846 |  # node_Transpose_2846
            %"transpose_77"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"view_150") {perm=[1, 0, 2]}
    2847 |  # node_Identity_2847
            %"clone_36"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"transpose_77")
    2848 |  # node_Mul_2848
            %"mul_47"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.4.gamma_1.scale", %"clone_36")
    2849 |  # node_aten_add_2849
            %"add_48"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_62", %"mul_47") {alpha=1.0}
    2850 |  # node_LayerNormalization_2850
            %"getitem_103"<FLOAT,[1,3448,512]>, %"native_layer_norm_23__1"<FLOAT,[1,3448,1]>, %"native_layer_norm_23__2"<FLOAT,[1,3448,1]> ⬅️ ::LayerNormalization(%"add_48", %"crosstransformer.layers.4.norm2.weight", %"crosstransformer.layers.4.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2851 |  # node_Cast_2851
            %"val_2035"<?,?> ⬅️ ::Cast(%"val_1332") {to=7}
    2852 |  # node_Reshape_2852
            %"view_151"<FLOAT,[3448,512]> ⬅️ ::Reshape(%"getitem_103", %"val_2035") {allowzero=0}
    2853 |  # node_Transpose_2853
            %"t_39"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers.4.linear1.weight") {perm=[1, 0]}
    2854 |  # node_Gemm_2854
            %"addmm_38"<FLOAT,[3448,2048]> ⬅️ ::Gemm(%"view_151", %"t_39", %"crosstransformer.layers.4.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2855 |  # node_Cast_2855
            %"val_2036"<?,?> ⬅️ ::Cast(%"val_1396") {to=7}
    2856 |  # node_Reshape_2856
            %"view_152"<FLOAT,[1,3448,2048]> ⬅️ ::Reshape(%"addmm_38", %"val_2036") {allowzero=0}
    2857 |  # node__aten_gelu_approximate_none_2857
            %"gelu_32"<FLOAT,[1,3448,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_152")
    2858 |  # node_Identity_2858
            %"clone_37"<FLOAT,[1,3448,2048]> ⬅️ ::Identity(%"gelu_32")
    2859 |  # node_Cast_2859
            %"val_2037"<?,?> ⬅️ ::Cast(%"val_1398") {to=7}
    2860 |  # node_Reshape_2860
            %"view_153"<FLOAT,[3448,2048]> ⬅️ ::Reshape(%"clone_37", %"val_2037") {allowzero=0}
    2861 |  # node_Transpose_2861
            %"t_40"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers.4.linear2.weight") {perm=[1, 0]}
    2862 |  # node_Gemm_2862
            %"addmm_39"<FLOAT,[3448,512]> ⬅️ ::Gemm(%"view_153", %"t_40", %"crosstransformer.layers.4.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2863 |  # node_Cast_2863
            %"val_2038"<?,?> ⬅️ ::Cast(%"val_1322") {to=7}
    2864 |  # node_Reshape_2864
            %"view_154"<FLOAT,[1,3448,512]> ⬅️ ::Reshape(%"addmm_39", %"val_2038") {allowzero=0}
    2865 |  # node_Identity_2865
            %"clone_38"<FLOAT,[1,3448,512]> ⬅️ ::Identity(%"view_154")
    2866 |  # node_Mul_2866
            %"mul_48"<FLOAT,[1,3448,512]> ⬅️ ::Mul(%"crosstransformer.layers.4.gamma_2.scale", %"clone_38")
    2867 |  # node_aten_add_2867
            %"add_49"<FLOAT,[1,3448,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_48", %"mul_48") {alpha=1.0}
    2868 |  # node_Transpose_2868
            %"transpose_78"<FLOAT,[1,512,3448]> ⬅️ ::Transpose(%"add_49") {perm=[0, 2, 1]}
    2869 |  # node_Constant_2869
            %"val_2039"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2870 |  # node_Reshape_2870
            %"val_2040"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2039") {allowzero=0}
    2871 |  # node_Constant_2871
            %"val_2041"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2872 |  # node_Concat_2872
            %"val_2042"<?,?> ⬅️ ::Concat(%"val_2041", %"val_2040", %"val_2039") {axis=0}
    2873 |  # node_Reshape_2873
            %"val_2043"<?,?> ⬅️ ::Reshape(%"transpose_78", %"val_2042") {allowzero=0}
    2874 |  # node_Constant_2874
            %"val_2044"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2875 |  # node_CastLike_2875
            %"val_2045"<?,?> ⬅️ ::CastLike(%"val_2044", %"transpose_78")
    2876 |  # node_Expand_2876
            %"val_2046"<?,?> ⬅️ ::Expand(%"val_2045", %"val_2040")
    2877 |  # node_Constant_2877
            %"val_2047"<?,?> ⬅️ ::Constant() {value_float=0.0}
    2878 |  # node_CastLike_2878
            %"val_2048"<?,?> ⬅️ ::CastLike(%"val_2047", %"transpose_78")
    2879 |  # node_Expand_2879
            %"val_2049"<?,?> ⬅️ ::Expand(%"val_2048", %"val_2040")
    2880 |  # node_InstanceNormalization_2880
            %"val_2050"<?,?> ⬅️ ::InstanceNormalization(%"val_2043", %"val_2046", %"val_2049") {epsilon=1e-05}
    2881 |  # node_Shape_2881
            %"val_2051"<?,?> ⬅️ ::Shape(%"transpose_78") {start=0}
    2882 |  # node_Reshape_2882
            %"val_2052"<?,?> ⬅️ ::Reshape(%"val_2050", %"val_2051") {allowzero=0}
    2883 |  # node_Constant_2883
            %"val_2053"<?,?> ⬅️ ::Constant() {value_int=1}
    2884 |  # node_Sub_2884
            %"val_2054"<?,?> ⬅️ ::Sub(%"val_50", %"val_2053")
    2885 |  # node_Range_2885
            %"val_2055"<?,?> ⬅️ ::Range(%"val_2053", %"val_2054", %"val_2053")
    2886 |  # node_Unsqueeze_2886
            %"val_2056"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.4.norm_out.weight", %"val_2055")
    2887 |  # node_Unsqueeze_2887
            %"val_2057"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers.4.norm_out.bias", %"val_2055")
    2888 |  # node_CastLike_2888
            %"val_2058"<?,?> ⬅️ ::CastLike(%"val_2056", %"val_2052")
    2889 |  # node_Mul_2889
            %"val_2059"<?,?> ⬅️ ::Mul(%"val_2052", %"val_2058")
    2890 |  # node_CastLike_2890
            %"val_2060"<?,?> ⬅️ ::CastLike(%"val_2057", %"val_2059")
    2891 |  # node_Add_2891
            %"group_norm_40"<FLOAT,[1,512,3448]> ⬅️ ::Add(%"val_2059", %"val_2060")
    2892 |  # node_Transpose_2892
            %"transpose_79"<FLOAT,[1,3448,512]> ⬅️ ::Transpose(%"group_norm_40") {perm=[0, 2, 1]}
    2893 |  # node_LayerNormalization_2893
            %"getitem_106"<FLOAT,[1,1723,512]>, %"native_layer_norm_24__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_24__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"transpose_71", %"crosstransformer.layers_t.4.norm1.weight", %"crosstransformer.layers_t.4.norm1.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2894 |  # node_Transpose_2894
            %"transpose_80"<FLOAT,[1723,1,512]> ⬅️ ::Transpose(%"getitem_106") {perm=[1, 0, 2]}
    2895 |  # node_Cast_2895
            %"val_2061"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2896 |  # node_Reshape_2896
            %"view_155"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"transpose_80", %"val_2061") {allowzero=0}
    2897 |  # node_Transpose_2897
            %"t_41"<FLOAT,[512,1536]> ⬅️ ::Transpose(%"crosstransformer.layers_t.4.self_attn.in_proj_weight") {perm=[1, 0]}
    2898 |  # node_Gemm_2898
            %"addmm_40"<FLOAT,[1723,1536]> ⬅️ ::Gemm(%"view_155", %"t_41", %"crosstransformer.layers_t.4.self_attn.in_proj_bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2899 |  # node_Cast_2899
            %"val_2062"<?,?> ⬅️ ::Cast(%"val_1425") {to=7}
    2900 |  # node_Reshape_2900
            %"view_156"<FLOAT,[1723,1,1536]> ⬅️ ::Reshape(%"addmm_40", %"val_2062") {allowzero=0}
    2901 |  # node_Cast_2901
            %"val_2063"<?,?> ⬅️ ::Cast(%"val_1427") {to=7}
    2902 |  # node_Reshape_2902
            %"view_157"<FLOAT,[1723,1,3,512]> ⬅️ ::Reshape(%"view_156", %"val_2063") {allowzero=0}
    2903 |  # node_aten_unsqueeze_2903
            %"unsqueeze_34"<FLOAT,[1,1723,1,3,512]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"view_157") {dim=0}
    2904 |  # node_Transpose_2904
            %"transpose_81"<FLOAT,[3,1723,1,1,512]> ⬅️ ::Transpose(%"unsqueeze_34") {perm=[3, 1, 2, 0, 4]}
    2905 |  # node_aten_squeeze_dim_2905
            %"squeeze_9"<FLOAT,[3,1723,1,512]> ⬅️ pkg.onnxscript.torch_lib::aten_squeeze_dim(%"transpose_81") {dim=-2}
    2906 |  # node_Identity_2906
            %"clone_39"<FLOAT,[3,1723,1,512]> ⬅️ ::Identity(%"squeeze_9")
    2907 |  # node_Gather_2907
            %"select_23"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_39", %"val_80") {axis=0}
    2908 |  # node_Gather_2908
            %"select_24"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_39", %"val_35") {axis=0}
    2909 |  # node_Gather_2909
            %"select_25"<FLOAT,[1723,1,512]> ⬅️ ::Gather(%"clone_39", %"val_276") {axis=0}
    2910 |  # node_Cast_2910
            %"val_2064"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2911 |  # node_Reshape_2911
            %"view_158"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_23", %"val_2064") {allowzero=0}
    2912 |  # node_Transpose_2912
            %"transpose_82"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_158") {perm=[1, 0, 2]}
    2913 |  # node_Cast_2913
            %"val_2065"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2914 |  # node_Reshape_2914
            %"view_159"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_24", %"val_2065") {allowzero=0}
    2915 |  # node_Transpose_2915
            %"transpose_83"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_159") {perm=[1, 0, 2]}
    2916 |  # node_Cast_2916
            %"val_2066"<?,?> ⬅️ ::Cast(%"val_1429") {to=7}
    2917 |  # node_Reshape_2917
            %"view_160"<FLOAT,[1723,8,64]> ⬅️ ::Reshape(%"select_25", %"val_2066") {allowzero=0}
    2918 |  # node_Transpose_2918
            %"transpose_84"<FLOAT,[8,1723,64]> ⬅️ ::Transpose(%"view_160") {perm=[1, 0, 2]}
    2919 |  # node_Cast_2919
            %"val_2067"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2920 |  # node_Reshape_2920
            %"view_161"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_82", %"val_2067") {allowzero=0}
    2921 |  # node_Cast_2921
            %"val_2068"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2922 |  # node_Reshape_2922
            %"view_162"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_83", %"val_2068") {allowzero=0}
    2923 |  # node_Cast_2923
            %"val_2069"<?,?> ⬅️ ::Cast(%"val_1433") {to=7}
    2924 |  # node_Reshape_2924
            %"view_163"<FLOAT,[1,8,1723,64]> ⬅️ ::Reshape(%"transpose_84", %"val_2069") {allowzero=0}
    2925 |  # node_Shape_2925
            %"val_2070"<?,?> ⬅️ ::Shape(%"view_161") {start=0}
    2926 |  # node_Constant_2926
            %"val_2071"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2927 |  # node_Gather_2927
            %"val_2072"<?,?> ⬅️ ::Gather(%"val_2070", %"val_2071") {axis=0}
    2928 |  # node_CastLike_2928
            %"val_2073"<?,?> ⬅️ ::CastLike(%"val_2072", %"view_161")
    2929 |  # node_Constant_2929
            %"val_2074"<?,?> ⬅️ ::Constant() {value_float=1.0}
    2930 |  # node_CastLike_2930
            %"val_2075"<?,?> ⬅️ ::CastLike(%"val_2074", %"view_161")
    2931 |  # node_Sqrt_2931
            %"val_2076"<?,?> ⬅️ ::Sqrt(%"val_2073")
    2932 |  # node_Div_2932
            %"val_2077"<?,?> ⬅️ ::Div(%"val_2075", %"val_2076")
    2933 |  # node_CastLike_2933
            %"val_2078"<?,?> ⬅️ ::CastLike(%"val_2077", %"view_161")
    2934 |  # node_Shape_2934
            %"val_2079"<?,?> ⬅️ ::Shape(%"view_162") {start=0}
    2935 |  # node_Constant_2935
            %"val_2080"<?,?> ⬅️ ::Constant() {value_ints=[9223372036854775807]}
    2936 |  # node_Slice_2936
            %"val_2081"<?,?> ⬅️ ::Slice(%"val_2079", %"val_1357", %"val_2080", None, None)
    2937 |  # node_Slice_2937
            %"val_2082"<?,?> ⬅️ ::Slice(%"val_2079", %"val_1359", %"val_1357", None, None)
    2938 |  # node_Constant_2938
            %"val_2083"<?,?> ⬅️ ::Constant() {value_ints=[-9223372036854775808]}
    2939 |  # node_Slice_2939
            %"val_2084"<?,?> ⬅️ ::Slice(%"val_2079", %"val_2083", %"val_1359", None, None)
    2940 |  # node_Constant_2940
            %"val_2085"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2941 |  # node_Concat_2941
            %"val_2086"<?,?> ⬅️ ::Concat(%"val_2085", %"val_2082", %"val_2081") {axis=0}
    2942 |  # node_Reshape_2942
            %"val_2087"<?,?> ⬅️ ::Reshape(%"view_162", %"val_2086") {allowzero=0}
    2943 |  # node_Transpose_2943
            %"val_2088"<?,?> ⬅️ ::Transpose(%"val_2087") {perm=[0, 2, 1]}
    2944 |  # node_Concat_2944
            %"val_2089"<?,?> ⬅️ ::Concat(%"val_2084", %"val_2081", %"val_2082") {axis=0}
    2945 |  # node_Reshape_2945
            %"val_2090"<?,?> ⬅️ ::Reshape(%"val_2088", %"val_2089") {allowzero=0}
    2946 |  # node_Sqrt_2946
            %"val_2091"<?,?> ⬅️ ::Sqrt(%"val_2078")
    2947 |  # node_Mul_2947
            %"val_2092"<?,?> ⬅️ ::Mul(%"view_161", %"val_2091")
    2948 |  # node_Sqrt_2948
            %"val_2093"<?,?> ⬅️ ::Sqrt(%"val_2078")
    2949 |  # node_CastLike_2949
            %"val_2094"<?,?> ⬅️ ::CastLike(%"val_2093", %"val_2090")
    2950 |  # node_Mul_2950
            %"val_2095"<?,?> ⬅️ ::Mul(%"val_2090", %"val_2094")
    2951 |  # node_MatMul_2951
            %"val_2096"<?,?> ⬅️ ::MatMul(%"val_2092", %"val_2095")
    2952 |  # node_Softmax_2952
            %"val_2097"<?,?> ⬅️ ::Softmax(%"val_2096") {axis=-1}
    2953 |  # node_Dropout_2953
            %"val_2098"<?,?>, %"val_2099"<?,?> ⬅️ ::Dropout(%"val_2097", %"val_953", None)
    2954 |  # node_MatMul_2954
            %"getitem_109"<FLOAT,[1,8,1723,64]> ⬅️ ::MatMul(%"val_2098", %"view_163")
    2955 |  # node_Shape_2955
            %"val_2100"<?,?> ⬅️ ::Shape(%"view_161") {start=0}
    2956 |  # node_Slice_2956
            %"val_2101"<?,?> ⬅️ ::Slice(%"val_2100", %"val_1379", %"val_1380", None, None)
    2957 |  # node_Slice_2957
            %"val_2102"<?,?> ⬅️ ::Slice(%"val_2100", %"val_1380", %"val_1382", None, None)
    2958 |  # node_Slice_2958
            %"val_2103"<?,?> ⬅️ ::Slice(%"val_2100", %"val_1359", %"val_1357", None, None)
    2959 |  # node_Cast_2959
            %"val_2104"<?,?> ⬅️ ::Cast(%"val_2102") {to=1}
    2960 |  # node_Div_2960
            %"val_2105"<?,?> ⬅️ ::Div(%"val_2104", %"val_1386")
    2961 |  # node_Ceil_2961
            %"val_2106"<?,?> ⬅️ ::Ceil(%"val_2105")
    2962 |  # node_Mul_2962
            %"val_2107"<?,?> ⬅️ ::Mul(%"val_2106", %"val_1386")
    2963 |  # node_Cast_2963
            %"val_2108"<?,?> ⬅️ ::Cast(%"val_2107") {to=7}
    2964 |  # node_Concat_2964
            %"val_2109"<?,?> ⬅️ ::Concat(%"val_2101", %"val_2103", %"val_2108") {axis=0}
    2965 |  # node_Expand_2965
            %"_scaled_dot_product_flash_attention_for_cpu_9__1"<FLOAT,[1,8,1723]> ⬅️ ::Expand(%"val_953", %"val_2109")
    2966 |  # node_Transpose_2966
            %"permute_22"<FLOAT,[1723,1,8,64]> ⬅️ ::Transpose(%"getitem_109") {perm=[2, 0, 1, 3]}
    2967 |  # node_Cast_2967
            %"val_2110"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2968 |  # node_Reshape_2968
            %"view_164"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"permute_22", %"val_2110") {allowzero=0}
    2969 |  # node_Transpose_2969
            %"t_42"<FLOAT,[512,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.4.self_attn.out_proj.weight") {perm=[1, 0]}
    2970 |  # node_Gemm_2970
            %"addmm_41"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_164", %"t_42", %"crosstransformer.layers_t.4.self_attn.out_proj.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2971 |  # node_Cast_2971
            %"val_2111"<?,?> ⬅️ ::Cast(%"val_1478") {to=7}
    2972 |  # node_Reshape_2972
            %"view_165"<FLOAT,[1723,1,512]> ⬅️ ::Reshape(%"addmm_41", %"val_2111") {allowzero=0}
    2973 |  # node_Transpose_2973
            %"transpose_85"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"view_165") {perm=[1, 0, 2]}
    2974 |  # node_Identity_2974
            %"clone_40"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"transpose_85")
    2975 |  # node_Mul_2975
            %"mul_49"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.4.gamma_1.scale", %"clone_40")
    2976 |  # node_aten_add_2976
            %"add_50"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"transpose_71", %"mul_49") {alpha=1.0}
    2977 |  # node_LayerNormalization_2977
            %"getitem_111"<FLOAT,[1,1723,512]>, %"native_layer_norm_25__1"<FLOAT,[1,1723,1]>, %"native_layer_norm_25__2"<FLOAT,[1,1723,1]> ⬅️ ::LayerNormalization(%"add_50", %"crosstransformer.layers_t.4.norm2.weight", %"crosstransformer.layers_t.4.norm2.bias") {axis=-1, epsilon=1e-05, stash_type=1}
    2978 |  # node_Cast_2978
            %"val_2112"<?,?> ⬅️ ::Cast(%"val_1423") {to=7}
    2979 |  # node_Reshape_2979
            %"view_166"<FLOAT,[1723,512]> ⬅️ ::Reshape(%"getitem_111", %"val_2112") {allowzero=0}
    2980 |  # node_Transpose_2980
            %"t_43"<FLOAT,[512,2048]> ⬅️ ::Transpose(%"crosstransformer.layers_t.4.linear1.weight") {perm=[1, 0]}
    2981 |  # node_Gemm_2981
            %"addmm_42"<FLOAT,[1723,2048]> ⬅️ ::Gemm(%"view_166", %"t_43", %"crosstransformer.layers_t.4.linear1.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2982 |  # node_Cast_2982
            %"val_2113"<?,?> ⬅️ ::Cast(%"val_1481") {to=7}
    2983 |  # node_Reshape_2983
            %"view_167"<FLOAT,[1,1723,2048]> ⬅️ ::Reshape(%"addmm_42", %"val_2113") {allowzero=0}
    2984 |  # node__aten_gelu_approximate_none_2984
            %"gelu_33"<FLOAT,[1,1723,2048]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"view_167")
    2985 |  # node_Identity_2985
            %"clone_41"<FLOAT,[1,1723,2048]> ⬅️ ::Identity(%"gelu_33")
    2986 |  # node_Cast_2986
            %"val_2114"<?,?> ⬅️ ::Cast(%"val_1483") {to=7}
    2987 |  # node_Reshape_2987
            %"view_168"<FLOAT,[1723,2048]> ⬅️ ::Reshape(%"clone_41", %"val_2114") {allowzero=0}
    2988 |  # node_Transpose_2988
            %"t_44"<FLOAT,[2048,512]> ⬅️ ::Transpose(%"crosstransformer.layers_t.4.linear2.weight") {perm=[1, 0]}
    2989 |  # node_Gemm_2989
            %"addmm_43"<FLOAT,[1723,512]> ⬅️ ::Gemm(%"view_168", %"t_44", %"crosstransformer.layers_t.4.linear2.bias") {alpha=1.0, beta=1.0, transA=0, transB=0}
    2990 |  # node_Cast_2990
            %"val_2115"<?,?> ⬅️ ::Cast(%"val_1485") {to=7}
    2991 |  # node_Reshape_2991
            %"view_169"<FLOAT,[1,1723,512]> ⬅️ ::Reshape(%"addmm_43", %"val_2115") {allowzero=0}
    2992 |  # node_Identity_2992
            %"clone_42"<FLOAT,[1,1723,512]> ⬅️ ::Identity(%"view_169")
    2993 |  # node_Mul_2993
            %"mul_50"<FLOAT,[1,1723,512]> ⬅️ ::Mul(%"crosstransformer.layers_t.4.gamma_2.scale", %"clone_42")
    2994 |  # node_aten_add_2994
            %"add_51"<FLOAT,[1,1723,512]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_50", %"mul_50") {alpha=1.0}
    2995 |  # node_Transpose_2995
            %"transpose_86"<FLOAT,[1,512,1723]> ⬅️ ::Transpose(%"add_51") {perm=[0, 2, 1]}
    2996 |  # node_Constant_2996
            %"val_2116"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    2997 |  # node_Reshape_2997
            %"val_2117"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2116") {allowzero=0}
    2998 |  # node_Constant_2998
            %"val_2118"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    2999 |  # node_Concat_2999
            %"val_2119"<?,?> ⬅️ ::Concat(%"val_2118", %"val_2117", %"val_2116") {axis=0}
    3000 |  # node_Reshape_3000
            %"val_2120"<?,?> ⬅️ ::Reshape(%"transpose_86", %"val_2119") {allowzero=0}
    3001 |  # node_Constant_3001
            %"val_2121"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3002 |  # node_CastLike_3002
            %"val_2122"<?,?> ⬅️ ::CastLike(%"val_2121", %"transpose_86")
    3003 |  # node_Expand_3003
            %"val_2123"<?,?> ⬅️ ::Expand(%"val_2122", %"val_2117")
    3004 |  # node_Constant_3004
            %"val_2124"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3005 |  # node_CastLike_3005
            %"val_2125"<?,?> ⬅️ ::CastLike(%"val_2124", %"transpose_86")
    3006 |  # node_Expand_3006
            %"val_2126"<?,?> ⬅️ ::Expand(%"val_2125", %"val_2117")
    3007 |  # node_InstanceNormalization_3007
            %"val_2127"<?,?> ⬅️ ::InstanceNormalization(%"val_2120", %"val_2123", %"val_2126") {epsilon=1e-05}
    3008 |  # node_Shape_3008
            %"val_2128"<?,?> ⬅️ ::Shape(%"transpose_86") {start=0}
    3009 |  # node_Reshape_3009
            %"val_2129"<?,?> ⬅️ ::Reshape(%"val_2127", %"val_2128") {allowzero=0}
    3010 |  # node_Constant_3010
            %"val_2130"<?,?> ⬅️ ::Constant() {value_int=1}
    3011 |  # node_Sub_3011
            %"val_2131"<?,?> ⬅️ ::Sub(%"val_50", %"val_2130")
    3012 |  # node_Range_3012
            %"val_2132"<?,?> ⬅️ ::Range(%"val_2130", %"val_2131", %"val_2130")
    3013 |  # node_Unsqueeze_3013
            %"val_2133"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.4.norm_out.weight", %"val_2132")
    3014 |  # node_Unsqueeze_3014
            %"val_2134"<?,?> ⬅️ ::Unsqueeze(%"crosstransformer.layers_t.4.norm_out.bias", %"val_2132")
    3015 |  # node_CastLike_3015
            %"val_2135"<?,?> ⬅️ ::CastLike(%"val_2133", %"val_2129")
    3016 |  # node_Mul_3016
            %"val_2136"<?,?> ⬅️ ::Mul(%"val_2129", %"val_2135")
    3017 |  # node_CastLike_3017
            %"val_2137"<?,?> ⬅️ ::CastLike(%"val_2134", %"val_2136")
    3018 |  # node_Add_3018
            %"group_norm_41"<FLOAT,[1,512,1723]> ⬅️ ::Add(%"val_2136", %"val_2137")
    3019 |  # node_Transpose_3019
            %"transpose_87"<FLOAT,[1,1723,512]> ⬅️ ::Transpose(%"group_norm_41") {perm=[0, 2, 1]}
    3020 |  # node_Constant_3020
            %"val_2138"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 431,   8, 512]), name=None)}
    3021 |  # node_Cast_3021
            %"val_2139"<?,?> ⬅️ ::Cast(%"val_2138") {to=7}
    3022 |  # node_Reshape_3022
            %"view_170"<FLOAT,[1,431,8,512]> ⬅️ ::Reshape(%"transpose_79", %"val_2139") {allowzero=0}
    3023 |  # node_Transpose_3023
            %"permute_23"<FLOAT,[1,512,8,431]> ⬅️ ::Transpose(%"view_170") {perm=[0, 3, 2, 1]}
    3024 |  # node_Transpose_3024
            %"permute_24"<FLOAT,[1,512,1723]> ⬅️ ::Transpose(%"transpose_87") {perm=[0, 2, 1]}
    3025 |  # node_Identity_3025
            %"clone_43"<FLOAT,[1,512,8,431]> ⬅️ ::Identity(%"permute_23")
    3026 |  # node_Constant_3026
            %"val_2140"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[3]>(array([   1,  512, 3448]), name=None)}
    3027 |  # node_Cast_3027
            %"val_2141"<?,?> ⬅️ ::Cast(%"val_2140") {to=7}
    3028 |  # node_Reshape_3028
            %"_unsafe_view_3"<FLOAT,[1,512,3448]> ⬅️ ::Reshape(%"clone_43", %"val_2141") {allowzero=0}
    3029 |  # node_Conv_3029
            %"convolution_50"<FLOAT,[1,384,3448]> ⬅️ ::Conv(%"_unsafe_view_3", %"channel_downsampler.weight", %"channel_downsampler.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3030 |  # node_Constant_3030
            %"val_2142"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([  1, 384,   8, 431]), name=None)}
    3031 |  # node_Cast_3031
            %"val_2143"<?,?> ⬅️ ::Cast(%"val_2142") {to=7}
    3032 |  # node_Reshape_3032
            %"view_171"<FLOAT,[1,384,8,431]> ⬅️ ::Reshape(%"convolution_50", %"val_2143") {allowzero=0}
    3033 |  # node_Conv_3033
            %"convolution_51"<FLOAT,[1,384,1723]> ⬅️ ::Conv(%"permute_24", %"channel_downsampler_t.weight", %"channel_downsampler_t.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3034 |  # node_aten_add_3034
            %"add_52"<FLOAT,[1,384,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_171", %"glu_23") {alpha=1.0}
    3035 |  # node_Conv_3035
            %"convolution_52"<FLOAT,[1,768,8,431]> ⬅️ ::Conv(%"add_52", %"decoder.0.rewrite.weight", %"decoder.0.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[1, 1, 1, 1], strides=[1, 1]}
    3036 |  # node_aten_glu_3036
            %"glu_24"<FLOAT,[1,384,8,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_52") {dim=1}
    3037 |  # node_Transpose_3037
            %"permute_25"<FLOAT,[1,8,384,431]> ⬅️ ::Transpose(%"glu_24") {perm=[0, 2, 1, 3]}
    3038 |  # node_Cast_3038
            %"val_2144"<?,?> ⬅️ ::Cast(%"val_833") {to=7}
    3039 |  # node_Reshape_3039
            %"view_172"<FLOAT,[8,384,431]> ⬅️ ::Reshape(%"permute_25", %"val_2144") {allowzero=0}
    3040 |  # node_Conv_3040
            %"convolution_53"<FLOAT,[8,48,431]> ⬅️ ::Conv(%"view_172", %"decoder.0.dconv.layers.0.0.weight", %"decoder.0.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3041 |  # node_Constant_3041
            %"val_2145"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3042 |  # node_Reshape_3042
            %"val_2146"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2145") {allowzero=0}
    3043 |  # node_Constant_3043
            %"val_2147"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3044 |  # node_Concat_3044
            %"val_2148"<?,?> ⬅️ ::Concat(%"val_2147", %"val_2146", %"val_2145") {axis=0}
    3045 |  # node_Reshape_3045
            %"val_2149"<?,?> ⬅️ ::Reshape(%"convolution_53", %"val_2148") {allowzero=0}
    3046 |  # node_Constant_3046
            %"val_2150"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3047 |  # node_CastLike_3047
            %"val_2151"<?,?> ⬅️ ::CastLike(%"val_2150", %"convolution_53")
    3048 |  # node_Expand_3048
            %"val_2152"<?,?> ⬅️ ::Expand(%"val_2151", %"val_2146")
    3049 |  # node_Constant_3049
            %"val_2153"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3050 |  # node_CastLike_3050
            %"val_2154"<?,?> ⬅️ ::CastLike(%"val_2153", %"convolution_53")
    3051 |  # node_Expand_3051
            %"val_2155"<?,?> ⬅️ ::Expand(%"val_2154", %"val_2146")
    3052 |  # node_InstanceNormalization_3052
            %"val_2156"<?,?> ⬅️ ::InstanceNormalization(%"val_2149", %"val_2152", %"val_2155") {epsilon=1e-05}
    3053 |  # node_Shape_3053
            %"val_2157"<?,?> ⬅️ ::Shape(%"convolution_53") {start=0}
    3054 |  # node_Reshape_3054
            %"val_2158"<?,?> ⬅️ ::Reshape(%"val_2156", %"val_2157") {allowzero=0}
    3055 |  # node_Constant_3055
            %"val_2159"<?,?> ⬅️ ::Constant() {value_int=1}
    3056 |  # node_Sub_3056
            %"val_2160"<?,?> ⬅️ ::Sub(%"val_50", %"val_2159")
    3057 |  # node_Range_3057
            %"val_2161"<?,?> ⬅️ ::Range(%"val_2159", %"val_2160", %"val_2159")
    3058 |  # node_Unsqueeze_3058
            %"val_2162"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.0.1.weight", %"val_2161")
    3059 |  # node_Unsqueeze_3059
            %"val_2163"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.0.1.bias", %"val_2161")
    3060 |  # node_CastLike_3060
            %"val_2164"<?,?> ⬅️ ::CastLike(%"val_2162", %"val_2158")
    3061 |  # node_Mul_3061
            %"val_2165"<?,?> ⬅️ ::Mul(%"val_2158", %"val_2164")
    3062 |  # node_CastLike_3062
            %"val_2166"<?,?> ⬅️ ::CastLike(%"val_2163", %"val_2165")
    3063 |  # node_Add_3063
            %"group_norm_42"<FLOAT,[8,48,431]> ⬅️ ::Add(%"val_2165", %"val_2166")
    3064 |  # node__aten_gelu_approximate_none_3064
            %"gelu_34"<FLOAT,[8,48,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_42")
    3065 |  # node_Conv_3065
            %"convolution_54"<FLOAT,[8,768,431]> ⬅️ ::Conv(%"gelu_34", %"decoder.0.dconv.layers.0.3.weight", %"decoder.0.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3066 |  # node_Constant_3066
            %"val_2167"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3067 |  # node_Reshape_3067
            %"val_2168"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2167") {allowzero=0}
    3068 |  # node_Constant_3068
            %"val_2169"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3069 |  # node_Concat_3069
            %"val_2170"<?,?> ⬅️ ::Concat(%"val_2169", %"val_2168", %"val_2167") {axis=0}
    3070 |  # node_Reshape_3070
            %"val_2171"<?,?> ⬅️ ::Reshape(%"convolution_54", %"val_2170") {allowzero=0}
    3071 |  # node_Constant_3071
            %"val_2172"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3072 |  # node_CastLike_3072
            %"val_2173"<?,?> ⬅️ ::CastLike(%"val_2172", %"convolution_54")
    3073 |  # node_Expand_3073
            %"val_2174"<?,?> ⬅️ ::Expand(%"val_2173", %"val_2168")
    3074 |  # node_Constant_3074
            %"val_2175"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3075 |  # node_CastLike_3075
            %"val_2176"<?,?> ⬅️ ::CastLike(%"val_2175", %"convolution_54")
    3076 |  # node_Expand_3076
            %"val_2177"<?,?> ⬅️ ::Expand(%"val_2176", %"val_2168")
    3077 |  # node_InstanceNormalization_3077
            %"val_2178"<?,?> ⬅️ ::InstanceNormalization(%"val_2171", %"val_2174", %"val_2177") {epsilon=1e-05}
    3078 |  # node_Shape_3078
            %"val_2179"<?,?> ⬅️ ::Shape(%"convolution_54") {start=0}
    3079 |  # node_Reshape_3079
            %"val_2180"<?,?> ⬅️ ::Reshape(%"val_2178", %"val_2179") {allowzero=0}
    3080 |  # node_Constant_3080
            %"val_2181"<?,?> ⬅️ ::Constant() {value_int=1}
    3081 |  # node_Sub_3081
            %"val_2182"<?,?> ⬅️ ::Sub(%"val_50", %"val_2181")
    3082 |  # node_Range_3082
            %"val_2183"<?,?> ⬅️ ::Range(%"val_2181", %"val_2182", %"val_2181")
    3083 |  # node_Unsqueeze_3083
            %"val_2184"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.0.4.weight", %"val_2183")
    3084 |  # node_Unsqueeze_3084
            %"val_2185"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.0.4.bias", %"val_2183")
    3085 |  # node_CastLike_3085
            %"val_2186"<?,?> ⬅️ ::CastLike(%"val_2184", %"val_2180")
    3086 |  # node_Mul_3086
            %"val_2187"<?,?> ⬅️ ::Mul(%"val_2180", %"val_2186")
    3087 |  # node_CastLike_3087
            %"val_2188"<?,?> ⬅️ ::CastLike(%"val_2185", %"val_2187")
    3088 |  # node_Add_3088
            %"group_norm_43"<FLOAT,[8,768,431]> ⬅️ ::Add(%"val_2187", %"val_2188")
    3089 |  # node_aten_glu_3089
            %"glu_25"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_43") {dim=1}
    3090 |  # node_Cast_3090
            %"val_2189"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3091 |  # node_Constant_3091
            %"val_2190"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3092 |  # node_Reshape_3092
            %"val_2191"<?,?> ⬅️ ::Reshape(%"val_2189", %"val_2190") {allowzero=0}
    3093 |  # node_Cast_3093
            %"val_2192"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3094 |  # node_Constant_3094
            %"val_2193"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3095 |  # node_Reshape_3095
            %"val_2194"<?,?> ⬅️ ::Reshape(%"val_2192", %"val_2193") {allowzero=0}
    3096 |  # node_Cast_3096
            %"val_2195"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3097 |  # node_Constant_3097
            %"val_2196"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3098 |  # node_Reshape_3098
            %"val_2197"<?,?> ⬅️ ::Reshape(%"val_2195", %"val_2196") {allowzero=0}
    3099 |  # node_Constant_3099
            %"val_2198"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3100 |  # node_Slice_3100
            %"slice_40"<FLOAT,[384]> ⬅️ ::Slice(%"decoder.0.dconv.layers.0.6.scale", %"val_2191", %"val_2194", %"val_2197", %"val_2198")
    3101 |  # node_aten_unsqueeze_3101
            %"unsqueeze_35"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_40") {dim=1}
    3102 |  # node_Mul_3102
            %"mul_51"<FLOAT,[8,384,431]> ⬅️ ::Mul(%"unsqueeze_35", %"glu_25")
    3103 |  # node_aten_add_3103
            %"add_53"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_172", %"mul_51") {alpha=1.0}
    3104 |  # node_Conv_3104
            %"convolution_55"<FLOAT,[8,48,431]> ⬅️ ::Conv(%"add_53", %"decoder.0.dconv.layers.1.0.weight", %"decoder.0.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    3105 |  # node_Constant_3105
            %"val_2199"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3106 |  # node_Reshape_3106
            %"val_2200"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2199") {allowzero=0}
    3107 |  # node_Constant_3107
            %"val_2201"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3108 |  # node_Concat_3108
            %"val_2202"<?,?> ⬅️ ::Concat(%"val_2201", %"val_2200", %"val_2199") {axis=0}
    3109 |  # node_Reshape_3109
            %"val_2203"<?,?> ⬅️ ::Reshape(%"convolution_55", %"val_2202") {allowzero=0}
    3110 |  # node_Constant_3110
            %"val_2204"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3111 |  # node_CastLike_3111
            %"val_2205"<?,?> ⬅️ ::CastLike(%"val_2204", %"convolution_55")
    3112 |  # node_Expand_3112
            %"val_2206"<?,?> ⬅️ ::Expand(%"val_2205", %"val_2200")
    3113 |  # node_Constant_3113
            %"val_2207"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3114 |  # node_CastLike_3114
            %"val_2208"<?,?> ⬅️ ::CastLike(%"val_2207", %"convolution_55")
    3115 |  # node_Expand_3115
            %"val_2209"<?,?> ⬅️ ::Expand(%"val_2208", %"val_2200")
    3116 |  # node_InstanceNormalization_3116
            %"val_2210"<?,?> ⬅️ ::InstanceNormalization(%"val_2203", %"val_2206", %"val_2209") {epsilon=1e-05}
    3117 |  # node_Shape_3117
            %"val_2211"<?,?> ⬅️ ::Shape(%"convolution_55") {start=0}
    3118 |  # node_Reshape_3118
            %"val_2212"<?,?> ⬅️ ::Reshape(%"val_2210", %"val_2211") {allowzero=0}
    3119 |  # node_Constant_3119
            %"val_2213"<?,?> ⬅️ ::Constant() {value_int=1}
    3120 |  # node_Sub_3120
            %"val_2214"<?,?> ⬅️ ::Sub(%"val_50", %"val_2213")
    3121 |  # node_Range_3121
            %"val_2215"<?,?> ⬅️ ::Range(%"val_2213", %"val_2214", %"val_2213")
    3122 |  # node_Unsqueeze_3122
            %"val_2216"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.1.1.weight", %"val_2215")
    3123 |  # node_Unsqueeze_3123
            %"val_2217"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.1.1.bias", %"val_2215")
    3124 |  # node_CastLike_3124
            %"val_2218"<?,?> ⬅️ ::CastLike(%"val_2216", %"val_2212")
    3125 |  # node_Mul_3125
            %"val_2219"<?,?> ⬅️ ::Mul(%"val_2212", %"val_2218")
    3126 |  # node_CastLike_3126
            %"val_2220"<?,?> ⬅️ ::CastLike(%"val_2217", %"val_2219")
    3127 |  # node_Add_3127
            %"group_norm_44"<FLOAT,[8,48,431]> ⬅️ ::Add(%"val_2219", %"val_2220")
    3128 |  # node__aten_gelu_approximate_none_3128
            %"gelu_35"<FLOAT,[8,48,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_44")
    3129 |  # node_Conv_3129
            %"convolution_56"<FLOAT,[8,768,431]> ⬅️ ::Conv(%"gelu_35", %"decoder.0.dconv.layers.1.3.weight", %"decoder.0.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3130 |  # node_Constant_3130
            %"val_2221"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3131 |  # node_Reshape_3131
            %"val_2222"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2221") {allowzero=0}
    3132 |  # node_Constant_3132
            %"val_2223"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3133 |  # node_Concat_3133
            %"val_2224"<?,?> ⬅️ ::Concat(%"val_2223", %"val_2222", %"val_2221") {axis=0}
    3134 |  # node_Reshape_3134
            %"val_2225"<?,?> ⬅️ ::Reshape(%"convolution_56", %"val_2224") {allowzero=0}
    3135 |  # node_Constant_3135
            %"val_2226"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3136 |  # node_CastLike_3136
            %"val_2227"<?,?> ⬅️ ::CastLike(%"val_2226", %"convolution_56")
    3137 |  # node_Expand_3137
            %"val_2228"<?,?> ⬅️ ::Expand(%"val_2227", %"val_2222")
    3138 |  # node_Constant_3138
            %"val_2229"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3139 |  # node_CastLike_3139
            %"val_2230"<?,?> ⬅️ ::CastLike(%"val_2229", %"convolution_56")
    3140 |  # node_Expand_3140
            %"val_2231"<?,?> ⬅️ ::Expand(%"val_2230", %"val_2222")
    3141 |  # node_InstanceNormalization_3141
            %"val_2232"<?,?> ⬅️ ::InstanceNormalization(%"val_2225", %"val_2228", %"val_2231") {epsilon=1e-05}
    3142 |  # node_Shape_3142
            %"val_2233"<?,?> ⬅️ ::Shape(%"convolution_56") {start=0}
    3143 |  # node_Reshape_3143
            %"val_2234"<?,?> ⬅️ ::Reshape(%"val_2232", %"val_2233") {allowzero=0}
    3144 |  # node_Constant_3144
            %"val_2235"<?,?> ⬅️ ::Constant() {value_int=1}
    3145 |  # node_Sub_3145
            %"val_2236"<?,?> ⬅️ ::Sub(%"val_50", %"val_2235")
    3146 |  # node_Range_3146
            %"val_2237"<?,?> ⬅️ ::Range(%"val_2235", %"val_2236", %"val_2235")
    3147 |  # node_Unsqueeze_3147
            %"val_2238"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.1.4.weight", %"val_2237")
    3148 |  # node_Unsqueeze_3148
            %"val_2239"<?,?> ⬅️ ::Unsqueeze(%"decoder.0.dconv.layers.1.4.bias", %"val_2237")
    3149 |  # node_CastLike_3149
            %"val_2240"<?,?> ⬅️ ::CastLike(%"val_2238", %"val_2234")
    3150 |  # node_Mul_3150
            %"val_2241"<?,?> ⬅️ ::Mul(%"val_2234", %"val_2240")
    3151 |  # node_CastLike_3151
            %"val_2242"<?,?> ⬅️ ::CastLike(%"val_2239", %"val_2241")
    3152 |  # node_Add_3152
            %"group_norm_45"<FLOAT,[8,768,431]> ⬅️ ::Add(%"val_2241", %"val_2242")
    3153 |  # node_aten_glu_3153
            %"glu_26"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_45") {dim=1}
    3154 |  # node_Cast_3154
            %"val_2243"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3155 |  # node_Constant_3155
            %"val_2244"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3156 |  # node_Reshape_3156
            %"val_2245"<?,?> ⬅️ ::Reshape(%"val_2243", %"val_2244") {allowzero=0}
    3157 |  # node_Cast_3157
            %"val_2246"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3158 |  # node_Constant_3158
            %"val_2247"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3159 |  # node_Reshape_3159
            %"val_2248"<?,?> ⬅️ ::Reshape(%"val_2246", %"val_2247") {allowzero=0}
    3160 |  # node_Cast_3160
            %"val_2249"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3161 |  # node_Constant_3161
            %"val_2250"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3162 |  # node_Reshape_3162
            %"val_2251"<?,?> ⬅️ ::Reshape(%"val_2249", %"val_2250") {allowzero=0}
    3163 |  # node_Constant_3163
            %"val_2252"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3164 |  # node_Slice_3164
            %"slice_41"<FLOAT,[384]> ⬅️ ::Slice(%"decoder.0.dconv.layers.1.6.scale", %"val_2245", %"val_2248", %"val_2251", %"val_2252")
    3165 |  # node_aten_unsqueeze_3165
            %"unsqueeze_36"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_41") {dim=1}
    3166 |  # node_Mul_3166
            %"mul_52"<FLOAT,[8,384,431]> ⬅️ ::Mul(%"unsqueeze_36", %"glu_26")
    3167 |  # node_aten_add_3167
            %"add_54"<FLOAT,[8,384,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_53", %"mul_52") {alpha=1.0}
    3168 |  # node_Cast_3168
            %"val_2253"<?,?> ⬅️ ::Cast(%"val_943") {to=7}
    3169 |  # node_Reshape_3169
            %"view_173"<FLOAT,[1,8,384,431]> ⬅️ ::Reshape(%"add_54", %"val_2253") {allowzero=0}
    3170 |  # node_Transpose_3170
            %"permute_26"<FLOAT,[1,384,8,431]> ⬅️ ::Transpose(%"view_173") {perm=[0, 2, 1, 3]}
    3171 |  # node_ConvTranspose_3171
            %"convolution_57"<FLOAT,[1,192,36,431]> ⬅️ ::ConvTranspose(%"permute_26", %"decoder.0.conv_tr.weight", %"decoder.0.conv_tr.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, output_padding=[0, 0], pads=[0, 0, 0, 0], strides=[4, 1]}
    3172 |  # node_Cast_3172
            %"val_2254"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3173 |  # node_Constant_3173
            %"val_2255"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3174 |  # node_Reshape_3174
            %"val_2256"<?,?> ⬅️ ::Reshape(%"val_2254", %"val_2255") {allowzero=0}
    3175 |  # node_Constant_3175
            %"val_2257"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(-2), name=None)}
    3176 |  # node_Cast_3176
            %"val_2258"<?,?> ⬅️ ::Cast(%"val_2257") {to=7}
    3177 |  # node_Constant_3177
            %"val_2259"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3178 |  # node_Reshape_3178
            %"val_2260"<?,?> ⬅️ ::Reshape(%"val_2258", %"val_2259") {allowzero=0}
    3179 |  # node_Cast_3179
            %"val_2261"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3180 |  # node_Constant_3180
            %"val_2262"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3181 |  # node_Reshape_3181
            %"val_2263"<?,?> ⬅️ ::Reshape(%"val_2261", %"val_2262") {allowzero=0}
    3182 |  # node_Constant_3182
            %"val_2264"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3183 |  # node_Slice_3183
            %"slice_42"<FLOAT,[1,192,32,431]> ⬅️ ::Slice(%"convolution_57", %"val_2256", %"val_2260", %"val_2263", %"val_2264")
    3184 |  # node_Cast_3184
            %"val_2265"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3185 |  # node_Constant_3185
            %"val_2266"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3186 |  # node_Reshape_3186
            %"val_2267"<?,?> ⬅️ ::Reshape(%"val_2265", %"val_2266") {allowzero=0}
    3187 |  # node_Cast_3187
            %"val_2268"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3188 |  # node_Constant_3188
            %"val_2269"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3189 |  # node_Reshape_3189
            %"val_2270"<?,?> ⬅️ ::Reshape(%"val_2268", %"val_2269") {allowzero=0}
    3190 |  # node_Cast_3190
            %"val_2271"<?,?> ⬅️ ::Cast(%"val_50") {to=7}
    3191 |  # node_Constant_3191
            %"val_2272"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3192 |  # node_Reshape_3192
            %"val_2273"<?,?> ⬅️ ::Reshape(%"val_2271", %"val_2272") {allowzero=0}
    3193 |  # node_Constant_3193
            %"val_2274"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3194 |  # node_Slice_3194
            %"slice_43"<FLOAT,[1,192,32,431]> ⬅️ ::Slice(%"slice_42", %"val_2267", %"val_2270", %"val_2273", %"val_2274")
    3195 |  # node__aten_gelu_approximate_none_3195
            %"gelu_36"<FLOAT,[1,192,32,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"slice_43")
    3196 |  # node_aten_add_3196
            %"add_55"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"convolution_51", %"glu_20") {alpha=1.0}
    3197 |  # node_Conv_3197
            %"convolution_58"<FLOAT,[1,768,1723]> ⬅️ ::Conv(%"add_55", %"tdecoder.0.rewrite.weight", %"tdecoder.0.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3198 |  # node_aten_glu_3198
            %"glu_27"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_58") {dim=1}
    3199 |  # node_Conv_3199
            %"convolution_59"<FLOAT,[1,48,1723]> ⬅️ ::Conv(%"glu_27", %"tdecoder.0.dconv.layers.0.0.weight", %"tdecoder.0.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3200 |  # node_Constant_3200
            %"val_2275"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3201 |  # node_Reshape_3201
            %"val_2276"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2275") {allowzero=0}
    3202 |  # node_Constant_3202
            %"val_2277"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3203 |  # node_Concat_3203
            %"val_2278"<?,?> ⬅️ ::Concat(%"val_2277", %"val_2276", %"val_2275") {axis=0}
    3204 |  # node_Reshape_3204
            %"val_2279"<?,?> ⬅️ ::Reshape(%"convolution_59", %"val_2278") {allowzero=0}
    3205 |  # node_Constant_3205
            %"val_2280"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3206 |  # node_CastLike_3206
            %"val_2281"<?,?> ⬅️ ::CastLike(%"val_2280", %"convolution_59")
    3207 |  # node_Expand_3207
            %"val_2282"<?,?> ⬅️ ::Expand(%"val_2281", %"val_2276")
    3208 |  # node_Constant_3208
            %"val_2283"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3209 |  # node_CastLike_3209
            %"val_2284"<?,?> ⬅️ ::CastLike(%"val_2283", %"convolution_59")
    3210 |  # node_Expand_3210
            %"val_2285"<?,?> ⬅️ ::Expand(%"val_2284", %"val_2276")
    3211 |  # node_InstanceNormalization_3211
            %"val_2286"<?,?> ⬅️ ::InstanceNormalization(%"val_2279", %"val_2282", %"val_2285") {epsilon=1e-05}
    3212 |  # node_Shape_3212
            %"val_2287"<?,?> ⬅️ ::Shape(%"convolution_59") {start=0}
    3213 |  # node_Reshape_3213
            %"val_2288"<?,?> ⬅️ ::Reshape(%"val_2286", %"val_2287") {allowzero=0}
    3214 |  # node_Constant_3214
            %"val_2289"<?,?> ⬅️ ::Constant() {value_int=1}
    3215 |  # node_Sub_3215
            %"val_2290"<?,?> ⬅️ ::Sub(%"val_50", %"val_2289")
    3216 |  # node_Range_3216
            %"val_2291"<?,?> ⬅️ ::Range(%"val_2289", %"val_2290", %"val_2289")
    3217 |  # node_Unsqueeze_3217
            %"val_2292"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.0.1.weight", %"val_2291")
    3218 |  # node_Unsqueeze_3218
            %"val_2293"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.0.1.bias", %"val_2291")
    3219 |  # node_CastLike_3219
            %"val_2294"<?,?> ⬅️ ::CastLike(%"val_2292", %"val_2288")
    3220 |  # node_Mul_3220
            %"val_2295"<?,?> ⬅️ ::Mul(%"val_2288", %"val_2294")
    3221 |  # node_CastLike_3221
            %"val_2296"<?,?> ⬅️ ::CastLike(%"val_2293", %"val_2295")
    3222 |  # node_Add_3222
            %"group_norm_46"<FLOAT,[1,48,1723]> ⬅️ ::Add(%"val_2295", %"val_2296")
    3223 |  # node__aten_gelu_approximate_none_3223
            %"gelu_37"<FLOAT,[1,48,1723]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_46")
    3224 |  # node_Conv_3224
            %"convolution_60"<FLOAT,[1,768,1723]> ⬅️ ::Conv(%"gelu_37", %"tdecoder.0.dconv.layers.0.3.weight", %"tdecoder.0.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3225 |  # node_Constant_3225
            %"val_2297"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3226 |  # node_Reshape_3226
            %"val_2298"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2297") {allowzero=0}
    3227 |  # node_Constant_3227
            %"val_2299"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3228 |  # node_Concat_3228
            %"val_2300"<?,?> ⬅️ ::Concat(%"val_2299", %"val_2298", %"val_2297") {axis=0}
    3229 |  # node_Reshape_3229
            %"val_2301"<?,?> ⬅️ ::Reshape(%"convolution_60", %"val_2300") {allowzero=0}
    3230 |  # node_Constant_3230
            %"val_2302"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3231 |  # node_CastLike_3231
            %"val_2303"<?,?> ⬅️ ::CastLike(%"val_2302", %"convolution_60")
    3232 |  # node_Expand_3232
            %"val_2304"<?,?> ⬅️ ::Expand(%"val_2303", %"val_2298")
    3233 |  # node_Constant_3233
            %"val_2305"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3234 |  # node_CastLike_3234
            %"val_2306"<?,?> ⬅️ ::CastLike(%"val_2305", %"convolution_60")
    3235 |  # node_Expand_3235
            %"val_2307"<?,?> ⬅️ ::Expand(%"val_2306", %"val_2298")
    3236 |  # node_InstanceNormalization_3236
            %"val_2308"<?,?> ⬅️ ::InstanceNormalization(%"val_2301", %"val_2304", %"val_2307") {epsilon=1e-05}
    3237 |  # node_Shape_3237
            %"val_2309"<?,?> ⬅️ ::Shape(%"convolution_60") {start=0}
    3238 |  # node_Reshape_3238
            %"val_2310"<?,?> ⬅️ ::Reshape(%"val_2308", %"val_2309") {allowzero=0}
    3239 |  # node_Constant_3239
            %"val_2311"<?,?> ⬅️ ::Constant() {value_int=1}
    3240 |  # node_Sub_3240
            %"val_2312"<?,?> ⬅️ ::Sub(%"val_50", %"val_2311")
    3241 |  # node_Range_3241
            %"val_2313"<?,?> ⬅️ ::Range(%"val_2311", %"val_2312", %"val_2311")
    3242 |  # node_Unsqueeze_3242
            %"val_2314"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.0.4.weight", %"val_2313")
    3243 |  # node_Unsqueeze_3243
            %"val_2315"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.0.4.bias", %"val_2313")
    3244 |  # node_CastLike_3244
            %"val_2316"<?,?> ⬅️ ::CastLike(%"val_2314", %"val_2310")
    3245 |  # node_Mul_3245
            %"val_2317"<?,?> ⬅️ ::Mul(%"val_2310", %"val_2316")
    3246 |  # node_CastLike_3246
            %"val_2318"<?,?> ⬅️ ::CastLike(%"val_2315", %"val_2317")
    3247 |  # node_Add_3247
            %"group_norm_47"<FLOAT,[1,768,1723]> ⬅️ ::Add(%"val_2317", %"val_2318")
    3248 |  # node_aten_glu_3248
            %"glu_28"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_47") {dim=1}
    3249 |  # node_Cast_3249
            %"val_2319"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3250 |  # node_Constant_3250
            %"val_2320"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3251 |  # node_Reshape_3251
            %"val_2321"<?,?> ⬅️ ::Reshape(%"val_2319", %"val_2320") {allowzero=0}
    3252 |  # node_Cast_3252
            %"val_2322"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3253 |  # node_Constant_3253
            %"val_2323"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3254 |  # node_Reshape_3254
            %"val_2324"<?,?> ⬅️ ::Reshape(%"val_2322", %"val_2323") {allowzero=0}
    3255 |  # node_Cast_3255
            %"val_2325"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3256 |  # node_Constant_3256
            %"val_2326"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3257 |  # node_Reshape_3257
            %"val_2327"<?,?> ⬅️ ::Reshape(%"val_2325", %"val_2326") {allowzero=0}
    3258 |  # node_Constant_3258
            %"val_2328"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3259 |  # node_Slice_3259
            %"slice_44"<FLOAT,[384]> ⬅️ ::Slice(%"tdecoder.0.dconv.layers.0.6.scale", %"val_2321", %"val_2324", %"val_2327", %"val_2328")
    3260 |  # node_aten_unsqueeze_3260
            %"unsqueeze_37"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_44") {dim=1}
    3261 |  # node_Mul_3261
            %"mul_53"<FLOAT,[1,384,1723]> ⬅️ ::Mul(%"unsqueeze_37", %"glu_28")
    3262 |  # node_aten_add_3262
            %"add_56"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"glu_27", %"mul_53") {alpha=1.0}
    3263 |  # node_Conv_3263
            %"convolution_61"<FLOAT,[1,48,1723]> ⬅️ ::Conv(%"add_56", %"tdecoder.0.dconv.layers.1.0.weight", %"tdecoder.0.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    3264 |  # node_Constant_3264
            %"val_2329"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3265 |  # node_Reshape_3265
            %"val_2330"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2329") {allowzero=0}
    3266 |  # node_Constant_3266
            %"val_2331"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3267 |  # node_Concat_3267
            %"val_2332"<?,?> ⬅️ ::Concat(%"val_2331", %"val_2330", %"val_2329") {axis=0}
    3268 |  # node_Reshape_3268
            %"val_2333"<?,?> ⬅️ ::Reshape(%"convolution_61", %"val_2332") {allowzero=0}
    3269 |  # node_Constant_3269
            %"val_2334"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3270 |  # node_CastLike_3270
            %"val_2335"<?,?> ⬅️ ::CastLike(%"val_2334", %"convolution_61")
    3271 |  # node_Expand_3271
            %"val_2336"<?,?> ⬅️ ::Expand(%"val_2335", %"val_2330")
    3272 |  # node_Constant_3272
            %"val_2337"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3273 |  # node_CastLike_3273
            %"val_2338"<?,?> ⬅️ ::CastLike(%"val_2337", %"convolution_61")
    3274 |  # node_Expand_3274
            %"val_2339"<?,?> ⬅️ ::Expand(%"val_2338", %"val_2330")
    3275 |  # node_InstanceNormalization_3275
            %"val_2340"<?,?> ⬅️ ::InstanceNormalization(%"val_2333", %"val_2336", %"val_2339") {epsilon=1e-05}
    3276 |  # node_Shape_3276
            %"val_2341"<?,?> ⬅️ ::Shape(%"convolution_61") {start=0}
    3277 |  # node_Reshape_3277
            %"val_2342"<?,?> ⬅️ ::Reshape(%"val_2340", %"val_2341") {allowzero=0}
    3278 |  # node_Constant_3278
            %"val_2343"<?,?> ⬅️ ::Constant() {value_int=1}
    3279 |  # node_Sub_3279
            %"val_2344"<?,?> ⬅️ ::Sub(%"val_50", %"val_2343")
    3280 |  # node_Range_3280
            %"val_2345"<?,?> ⬅️ ::Range(%"val_2343", %"val_2344", %"val_2343")
    3281 |  # node_Unsqueeze_3281
            %"val_2346"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.1.1.weight", %"val_2345")
    3282 |  # node_Unsqueeze_3282
            %"val_2347"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.1.1.bias", %"val_2345")
    3283 |  # node_CastLike_3283
            %"val_2348"<?,?> ⬅️ ::CastLike(%"val_2346", %"val_2342")
    3284 |  # node_Mul_3284
            %"val_2349"<?,?> ⬅️ ::Mul(%"val_2342", %"val_2348")
    3285 |  # node_CastLike_3285
            %"val_2350"<?,?> ⬅️ ::CastLike(%"val_2347", %"val_2349")
    3286 |  # node_Add_3286
            %"group_norm_48"<FLOAT,[1,48,1723]> ⬅️ ::Add(%"val_2349", %"val_2350")
    3287 |  # node__aten_gelu_approximate_none_3287
            %"gelu_38"<FLOAT,[1,48,1723]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_48")
    3288 |  # node_Conv_3288
            %"convolution_62"<FLOAT,[1,768,1723]> ⬅️ ::Conv(%"gelu_38", %"tdecoder.0.dconv.layers.1.3.weight", %"tdecoder.0.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3289 |  # node_Constant_3289
            %"val_2351"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3290 |  # node_Reshape_3290
            %"val_2352"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2351") {allowzero=0}
    3291 |  # node_Constant_3291
            %"val_2353"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3292 |  # node_Concat_3292
            %"val_2354"<?,?> ⬅️ ::Concat(%"val_2353", %"val_2352", %"val_2351") {axis=0}
    3293 |  # node_Reshape_3293
            %"val_2355"<?,?> ⬅️ ::Reshape(%"convolution_62", %"val_2354") {allowzero=0}
    3294 |  # node_Constant_3294
            %"val_2356"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3295 |  # node_CastLike_3295
            %"val_2357"<?,?> ⬅️ ::CastLike(%"val_2356", %"convolution_62")
    3296 |  # node_Expand_3296
            %"val_2358"<?,?> ⬅️ ::Expand(%"val_2357", %"val_2352")
    3297 |  # node_Constant_3297
            %"val_2359"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3298 |  # node_CastLike_3298
            %"val_2360"<?,?> ⬅️ ::CastLike(%"val_2359", %"convolution_62")
    3299 |  # node_Expand_3299
            %"val_2361"<?,?> ⬅️ ::Expand(%"val_2360", %"val_2352")
    3300 |  # node_InstanceNormalization_3300
            %"val_2362"<?,?> ⬅️ ::InstanceNormalization(%"val_2355", %"val_2358", %"val_2361") {epsilon=1e-05}
    3301 |  # node_Shape_3301
            %"val_2363"<?,?> ⬅️ ::Shape(%"convolution_62") {start=0}
    3302 |  # node_Reshape_3302
            %"val_2364"<?,?> ⬅️ ::Reshape(%"val_2362", %"val_2363") {allowzero=0}
    3303 |  # node_Constant_3303
            %"val_2365"<?,?> ⬅️ ::Constant() {value_int=1}
    3304 |  # node_Sub_3304
            %"val_2366"<?,?> ⬅️ ::Sub(%"val_50", %"val_2365")
    3305 |  # node_Range_3305
            %"val_2367"<?,?> ⬅️ ::Range(%"val_2365", %"val_2366", %"val_2365")
    3306 |  # node_Unsqueeze_3306
            %"val_2368"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.1.4.weight", %"val_2367")
    3307 |  # node_Unsqueeze_3307
            %"val_2369"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.0.dconv.layers.1.4.bias", %"val_2367")
    3308 |  # node_CastLike_3308
            %"val_2370"<?,?> ⬅️ ::CastLike(%"val_2368", %"val_2364")
    3309 |  # node_Mul_3309
            %"val_2371"<?,?> ⬅️ ::Mul(%"val_2364", %"val_2370")
    3310 |  # node_CastLike_3310
            %"val_2372"<?,?> ⬅️ ::CastLike(%"val_2369", %"val_2371")
    3311 |  # node_Add_3311
            %"group_norm_49"<FLOAT,[1,768,1723]> ⬅️ ::Add(%"val_2371", %"val_2372")
    3312 |  # node_aten_glu_3312
            %"glu_29"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_49") {dim=1}
    3313 |  # node_Cast_3313
            %"val_2373"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3314 |  # node_Constant_3314
            %"val_2374"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3315 |  # node_Reshape_3315
            %"val_2375"<?,?> ⬅️ ::Reshape(%"val_2373", %"val_2374") {allowzero=0}
    3316 |  # node_Cast_3316
            %"val_2376"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3317 |  # node_Constant_3317
            %"val_2377"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3318 |  # node_Reshape_3318
            %"val_2378"<?,?> ⬅️ ::Reshape(%"val_2376", %"val_2377") {allowzero=0}
    3319 |  # node_Cast_3319
            %"val_2379"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3320 |  # node_Constant_3320
            %"val_2380"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3321 |  # node_Reshape_3321
            %"val_2381"<?,?> ⬅️ ::Reshape(%"val_2379", %"val_2380") {allowzero=0}
    3322 |  # node_Constant_3322
            %"val_2382"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3323 |  # node_Slice_3323
            %"slice_45"<FLOAT,[384]> ⬅️ ::Slice(%"tdecoder.0.dconv.layers.1.6.scale", %"val_2375", %"val_2378", %"val_2381", %"val_2382")
    3324 |  # node_aten_unsqueeze_3324
            %"unsqueeze_38"<FLOAT,[384,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_45") {dim=1}
    3325 |  # node_Mul_3325
            %"mul_54"<FLOAT,[1,384,1723]> ⬅️ ::Mul(%"unsqueeze_38", %"glu_29")
    3326 |  # node_aten_add_3326
            %"add_57"<FLOAT,[1,384,1723]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_56", %"mul_54") {alpha=1.0}
    3327 |  # node_ConvTranspose_3327
            %"convolution_63"<FLOAT,[1,192,6896]> ⬅️ ::ConvTranspose(%"add_57", %"tdecoder.0.conv_tr.weight", %"tdecoder.0.conv_tr.bias") {auto_pad=NOTSET, dilations=[1], group=1, output_padding=[0], pads=[0, 0], strides=[4]}
    3328 |  # node_Cast_3328
            %"val_2383"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3329 |  # node_Constant_3329
            %"val_2384"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3330 |  # node_Reshape_3330
            %"val_2385"<?,?> ⬅️ ::Reshape(%"val_2383", %"val_2384") {allowzero=0}
    3331 |  # node_Constant_3331
            %"val_2386"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(6893), name=None)}
    3332 |  # node_Cast_3332
            %"val_2387"<?,?> ⬅️ ::Cast(%"val_2386") {to=7}
    3333 |  # node_Constant_3333
            %"val_2388"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3334 |  # node_Reshape_3334
            %"val_2389"<?,?> ⬅️ ::Reshape(%"val_2387", %"val_2388") {allowzero=0}
    3335 |  # node_Cast_3335
            %"val_2390"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3336 |  # node_Constant_3336
            %"val_2391"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3337 |  # node_Reshape_3337
            %"val_2392"<?,?> ⬅️ ::Reshape(%"val_2390", %"val_2391") {allowzero=0}
    3338 |  # node_Constant_3338
            %"val_2393"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3339 |  # node_Slice_3339
            %"slice_46"<FLOAT,[1,192,6891]> ⬅️ ::Slice(%"convolution_63", %"val_2385", %"val_2389", %"val_2392", %"val_2393")
    3340 |  # node__aten_gelu_approximate_none_3340
            %"gelu_39"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"slice_46")
    3341 |  # node_aten_add_3341
            %"add_58"<FLOAT,[1,192,32,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_36", %"glu_17") {alpha=1.0}
    3342 |  # node_Conv_3342
            %"convolution_64"<FLOAT,[1,384,32,431]> ⬅️ ::Conv(%"add_58", %"decoder.1.rewrite.weight", %"decoder.1.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[1, 1, 1, 1], strides=[1, 1]}
    3343 |  # node_aten_glu_3343
            %"glu_30"<FLOAT,[1,192,32,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_64") {dim=1}
    3344 |  # node_Transpose_3344
            %"permute_27"<FLOAT,[1,32,192,431]> ⬅️ ::Transpose(%"glu_30") {perm=[0, 2, 1, 3]}
    3345 |  # node_Cast_3345
            %"val_2394"<?,?> ⬅️ ::Cast(%"val_613") {to=7}
    3346 |  # node_Reshape_3346
            %"view_174"<FLOAT,[32,192,431]> ⬅️ ::Reshape(%"permute_27", %"val_2394") {allowzero=0}
    3347 |  # node_Conv_3347
            %"convolution_65"<FLOAT,[32,24,431]> ⬅️ ::Conv(%"view_174", %"decoder.1.dconv.layers.0.0.weight", %"decoder.1.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3348 |  # node_Constant_3348
            %"val_2395"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3349 |  # node_Reshape_3349
            %"val_2396"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2395") {allowzero=0}
    3350 |  # node_Constant_3350
            %"val_2397"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3351 |  # node_Concat_3351
            %"val_2398"<?,?> ⬅️ ::Concat(%"val_2397", %"val_2396", %"val_2395") {axis=0}
    3352 |  # node_Reshape_3352
            %"val_2399"<?,?> ⬅️ ::Reshape(%"convolution_65", %"val_2398") {allowzero=0}
    3353 |  # node_Constant_3353
            %"val_2400"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3354 |  # node_CastLike_3354
            %"val_2401"<?,?> ⬅️ ::CastLike(%"val_2400", %"convolution_65")
    3355 |  # node_Expand_3355
            %"val_2402"<?,?> ⬅️ ::Expand(%"val_2401", %"val_2396")
    3356 |  # node_Constant_3356
            %"val_2403"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3357 |  # node_CastLike_3357
            %"val_2404"<?,?> ⬅️ ::CastLike(%"val_2403", %"convolution_65")
    3358 |  # node_Expand_3358
            %"val_2405"<?,?> ⬅️ ::Expand(%"val_2404", %"val_2396")
    3359 |  # node_InstanceNormalization_3359
            %"val_2406"<?,?> ⬅️ ::InstanceNormalization(%"val_2399", %"val_2402", %"val_2405") {epsilon=1e-05}
    3360 |  # node_Shape_3360
            %"val_2407"<?,?> ⬅️ ::Shape(%"convolution_65") {start=0}
    3361 |  # node_Reshape_3361
            %"val_2408"<?,?> ⬅️ ::Reshape(%"val_2406", %"val_2407") {allowzero=0}
    3362 |  # node_Constant_3362
            %"val_2409"<?,?> ⬅️ ::Constant() {value_int=1}
    3363 |  # node_Sub_3363
            %"val_2410"<?,?> ⬅️ ::Sub(%"val_50", %"val_2409")
    3364 |  # node_Range_3364
            %"val_2411"<?,?> ⬅️ ::Range(%"val_2409", %"val_2410", %"val_2409")
    3365 |  # node_Unsqueeze_3365
            %"val_2412"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.0.1.weight", %"val_2411")
    3366 |  # node_Unsqueeze_3366
            %"val_2413"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.0.1.bias", %"val_2411")
    3367 |  # node_CastLike_3367
            %"val_2414"<?,?> ⬅️ ::CastLike(%"val_2412", %"val_2408")
    3368 |  # node_Mul_3368
            %"val_2415"<?,?> ⬅️ ::Mul(%"val_2408", %"val_2414")
    3369 |  # node_CastLike_3369
            %"val_2416"<?,?> ⬅️ ::CastLike(%"val_2413", %"val_2415")
    3370 |  # node_Add_3370
            %"group_norm_50"<FLOAT,[32,24,431]> ⬅️ ::Add(%"val_2415", %"val_2416")
    3371 |  # node__aten_gelu_approximate_none_3371
            %"gelu_40"<FLOAT,[32,24,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_50")
    3372 |  # node_Conv_3372
            %"convolution_66"<FLOAT,[32,384,431]> ⬅️ ::Conv(%"gelu_40", %"decoder.1.dconv.layers.0.3.weight", %"decoder.1.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3373 |  # node_Constant_3373
            %"val_2417"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3374 |  # node_Reshape_3374
            %"val_2418"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2417") {allowzero=0}
    3375 |  # node_Constant_3375
            %"val_2419"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3376 |  # node_Concat_3376
            %"val_2420"<?,?> ⬅️ ::Concat(%"val_2419", %"val_2418", %"val_2417") {axis=0}
    3377 |  # node_Reshape_3377
            %"val_2421"<?,?> ⬅️ ::Reshape(%"convolution_66", %"val_2420") {allowzero=0}
    3378 |  # node_Constant_3378
            %"val_2422"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3379 |  # node_CastLike_3379
            %"val_2423"<?,?> ⬅️ ::CastLike(%"val_2422", %"convolution_66")
    3380 |  # node_Expand_3380
            %"val_2424"<?,?> ⬅️ ::Expand(%"val_2423", %"val_2418")
    3381 |  # node_Constant_3381
            %"val_2425"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3382 |  # node_CastLike_3382
            %"val_2426"<?,?> ⬅️ ::CastLike(%"val_2425", %"convolution_66")
    3383 |  # node_Expand_3383
            %"val_2427"<?,?> ⬅️ ::Expand(%"val_2426", %"val_2418")
    3384 |  # node_InstanceNormalization_3384
            %"val_2428"<?,?> ⬅️ ::InstanceNormalization(%"val_2421", %"val_2424", %"val_2427") {epsilon=1e-05}
    3385 |  # node_Shape_3385
            %"val_2429"<?,?> ⬅️ ::Shape(%"convolution_66") {start=0}
    3386 |  # node_Reshape_3386
            %"val_2430"<?,?> ⬅️ ::Reshape(%"val_2428", %"val_2429") {allowzero=0}
    3387 |  # node_Constant_3387
            %"val_2431"<?,?> ⬅️ ::Constant() {value_int=1}
    3388 |  # node_Sub_3388
            %"val_2432"<?,?> ⬅️ ::Sub(%"val_50", %"val_2431")
    3389 |  # node_Range_3389
            %"val_2433"<?,?> ⬅️ ::Range(%"val_2431", %"val_2432", %"val_2431")
    3390 |  # node_Unsqueeze_3390
            %"val_2434"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.0.4.weight", %"val_2433")
    3391 |  # node_Unsqueeze_3391
            %"val_2435"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.0.4.bias", %"val_2433")
    3392 |  # node_CastLike_3392
            %"val_2436"<?,?> ⬅️ ::CastLike(%"val_2434", %"val_2430")
    3393 |  # node_Mul_3393
            %"val_2437"<?,?> ⬅️ ::Mul(%"val_2430", %"val_2436")
    3394 |  # node_CastLike_3394
            %"val_2438"<?,?> ⬅️ ::CastLike(%"val_2435", %"val_2437")
    3395 |  # node_Add_3395
            %"group_norm_51"<FLOAT,[32,384,431]> ⬅️ ::Add(%"val_2437", %"val_2438")
    3396 |  # node_aten_glu_3396
            %"glu_31"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_51") {dim=1}
    3397 |  # node_Cast_3397
            %"val_2439"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3398 |  # node_Constant_3398
            %"val_2440"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3399 |  # node_Reshape_3399
            %"val_2441"<?,?> ⬅️ ::Reshape(%"val_2439", %"val_2440") {allowzero=0}
    3400 |  # node_Cast_3400
            %"val_2442"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3401 |  # node_Constant_3401
            %"val_2443"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3402 |  # node_Reshape_3402
            %"val_2444"<?,?> ⬅️ ::Reshape(%"val_2442", %"val_2443") {allowzero=0}
    3403 |  # node_Cast_3403
            %"val_2445"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3404 |  # node_Constant_3404
            %"val_2446"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3405 |  # node_Reshape_3405
            %"val_2447"<?,?> ⬅️ ::Reshape(%"val_2445", %"val_2446") {allowzero=0}
    3406 |  # node_Constant_3406
            %"val_2448"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3407 |  # node_Slice_3407
            %"slice_47"<FLOAT,[192]> ⬅️ ::Slice(%"decoder.1.dconv.layers.0.6.scale", %"val_2441", %"val_2444", %"val_2447", %"val_2448")
    3408 |  # node_aten_unsqueeze_3408
            %"unsqueeze_39"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_47") {dim=1}
    3409 |  # node_Mul_3409
            %"mul_55"<FLOAT,[32,192,431]> ⬅️ ::Mul(%"unsqueeze_39", %"glu_31")
    3410 |  # node_aten_add_3410
            %"add_59"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_174", %"mul_55") {alpha=1.0}
    3411 |  # node_Conv_3411
            %"convolution_67"<FLOAT,[32,24,431]> ⬅️ ::Conv(%"add_59", %"decoder.1.dconv.layers.1.0.weight", %"decoder.1.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    3412 |  # node_Constant_3412
            %"val_2449"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3413 |  # node_Reshape_3413
            %"val_2450"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2449") {allowzero=0}
    3414 |  # node_Constant_3414
            %"val_2451"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3415 |  # node_Concat_3415
            %"val_2452"<?,?> ⬅️ ::Concat(%"val_2451", %"val_2450", %"val_2449") {axis=0}
    3416 |  # node_Reshape_3416
            %"val_2453"<?,?> ⬅️ ::Reshape(%"convolution_67", %"val_2452") {allowzero=0}
    3417 |  # node_Constant_3417
            %"val_2454"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3418 |  # node_CastLike_3418
            %"val_2455"<?,?> ⬅️ ::CastLike(%"val_2454", %"convolution_67")
    3419 |  # node_Expand_3419
            %"val_2456"<?,?> ⬅️ ::Expand(%"val_2455", %"val_2450")
    3420 |  # node_Constant_3420
            %"val_2457"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3421 |  # node_CastLike_3421
            %"val_2458"<?,?> ⬅️ ::CastLike(%"val_2457", %"convolution_67")
    3422 |  # node_Expand_3422
            %"val_2459"<?,?> ⬅️ ::Expand(%"val_2458", %"val_2450")
    3423 |  # node_InstanceNormalization_3423
            %"val_2460"<?,?> ⬅️ ::InstanceNormalization(%"val_2453", %"val_2456", %"val_2459") {epsilon=1e-05}
    3424 |  # node_Shape_3424
            %"val_2461"<?,?> ⬅️ ::Shape(%"convolution_67") {start=0}
    3425 |  # node_Reshape_3425
            %"val_2462"<?,?> ⬅️ ::Reshape(%"val_2460", %"val_2461") {allowzero=0}
    3426 |  # node_Constant_3426
            %"val_2463"<?,?> ⬅️ ::Constant() {value_int=1}
    3427 |  # node_Sub_3427
            %"val_2464"<?,?> ⬅️ ::Sub(%"val_50", %"val_2463")
    3428 |  # node_Range_3428
            %"val_2465"<?,?> ⬅️ ::Range(%"val_2463", %"val_2464", %"val_2463")
    3429 |  # node_Unsqueeze_3429
            %"val_2466"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.1.1.weight", %"val_2465")
    3430 |  # node_Unsqueeze_3430
            %"val_2467"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.1.1.bias", %"val_2465")
    3431 |  # node_CastLike_3431
            %"val_2468"<?,?> ⬅️ ::CastLike(%"val_2466", %"val_2462")
    3432 |  # node_Mul_3432
            %"val_2469"<?,?> ⬅️ ::Mul(%"val_2462", %"val_2468")
    3433 |  # node_CastLike_3433
            %"val_2470"<?,?> ⬅️ ::CastLike(%"val_2467", %"val_2469")
    3434 |  # node_Add_3434
            %"group_norm_52"<FLOAT,[32,24,431]> ⬅️ ::Add(%"val_2469", %"val_2470")
    3435 |  # node__aten_gelu_approximate_none_3435
            %"gelu_41"<FLOAT,[32,24,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_52")
    3436 |  # node_Conv_3436
            %"convolution_68"<FLOAT,[32,384,431]> ⬅️ ::Conv(%"gelu_41", %"decoder.1.dconv.layers.1.3.weight", %"decoder.1.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3437 |  # node_Constant_3437
            %"val_2471"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3438 |  # node_Reshape_3438
            %"val_2472"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2471") {allowzero=0}
    3439 |  # node_Constant_3439
            %"val_2473"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3440 |  # node_Concat_3440
            %"val_2474"<?,?> ⬅️ ::Concat(%"val_2473", %"val_2472", %"val_2471") {axis=0}
    3441 |  # node_Reshape_3441
            %"val_2475"<?,?> ⬅️ ::Reshape(%"convolution_68", %"val_2474") {allowzero=0}
    3442 |  # node_Constant_3442
            %"val_2476"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3443 |  # node_CastLike_3443
            %"val_2477"<?,?> ⬅️ ::CastLike(%"val_2476", %"convolution_68")
    3444 |  # node_Expand_3444
            %"val_2478"<?,?> ⬅️ ::Expand(%"val_2477", %"val_2472")
    3445 |  # node_Constant_3445
            %"val_2479"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3446 |  # node_CastLike_3446
            %"val_2480"<?,?> ⬅️ ::CastLike(%"val_2479", %"convolution_68")
    3447 |  # node_Expand_3447
            %"val_2481"<?,?> ⬅️ ::Expand(%"val_2480", %"val_2472")
    3448 |  # node_InstanceNormalization_3448
            %"val_2482"<?,?> ⬅️ ::InstanceNormalization(%"val_2475", %"val_2478", %"val_2481") {epsilon=1e-05}
    3449 |  # node_Shape_3449
            %"val_2483"<?,?> ⬅️ ::Shape(%"convolution_68") {start=0}
    3450 |  # node_Reshape_3450
            %"val_2484"<?,?> ⬅️ ::Reshape(%"val_2482", %"val_2483") {allowzero=0}
    3451 |  # node_Constant_3451
            %"val_2485"<?,?> ⬅️ ::Constant() {value_int=1}
    3452 |  # node_Sub_3452
            %"val_2486"<?,?> ⬅️ ::Sub(%"val_50", %"val_2485")
    3453 |  # node_Range_3453
            %"val_2487"<?,?> ⬅️ ::Range(%"val_2485", %"val_2486", %"val_2485")
    3454 |  # node_Unsqueeze_3454
            %"val_2488"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.1.4.weight", %"val_2487")
    3455 |  # node_Unsqueeze_3455
            %"val_2489"<?,?> ⬅️ ::Unsqueeze(%"decoder.1.dconv.layers.1.4.bias", %"val_2487")
    3456 |  # node_CastLike_3456
            %"val_2490"<?,?> ⬅️ ::CastLike(%"val_2488", %"val_2484")
    3457 |  # node_Mul_3457
            %"val_2491"<?,?> ⬅️ ::Mul(%"val_2484", %"val_2490")
    3458 |  # node_CastLike_3458
            %"val_2492"<?,?> ⬅️ ::CastLike(%"val_2489", %"val_2491")
    3459 |  # node_Add_3459
            %"group_norm_53"<FLOAT,[32,384,431]> ⬅️ ::Add(%"val_2491", %"val_2492")
    3460 |  # node_aten_glu_3460
            %"glu_32"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_53") {dim=1}
    3461 |  # node_Cast_3461
            %"val_2493"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3462 |  # node_Constant_3462
            %"val_2494"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3463 |  # node_Reshape_3463
            %"val_2495"<?,?> ⬅️ ::Reshape(%"val_2493", %"val_2494") {allowzero=0}
    3464 |  # node_Cast_3464
            %"val_2496"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3465 |  # node_Constant_3465
            %"val_2497"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3466 |  # node_Reshape_3466
            %"val_2498"<?,?> ⬅️ ::Reshape(%"val_2496", %"val_2497") {allowzero=0}
    3467 |  # node_Cast_3467
            %"val_2499"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3468 |  # node_Constant_3468
            %"val_2500"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3469 |  # node_Reshape_3469
            %"val_2501"<?,?> ⬅️ ::Reshape(%"val_2499", %"val_2500") {allowzero=0}
    3470 |  # node_Constant_3470
            %"val_2502"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3471 |  # node_Slice_3471
            %"slice_48"<FLOAT,[192]> ⬅️ ::Slice(%"decoder.1.dconv.layers.1.6.scale", %"val_2495", %"val_2498", %"val_2501", %"val_2502")
    3472 |  # node_aten_unsqueeze_3472
            %"unsqueeze_40"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_48") {dim=1}
    3473 |  # node_Mul_3473
            %"mul_56"<FLOAT,[32,192,431]> ⬅️ ::Mul(%"unsqueeze_40", %"glu_32")
    3474 |  # node_aten_add_3474
            %"add_60"<FLOAT,[32,192,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_59", %"mul_56") {alpha=1.0}
    3475 |  # node_Cast_3475
            %"val_2503"<?,?> ⬅️ ::Cast(%"val_723") {to=7}
    3476 |  # node_Reshape_3476
            %"view_175"<FLOAT,[1,32,192,431]> ⬅️ ::Reshape(%"add_60", %"val_2503") {allowzero=0}
    3477 |  # node_Transpose_3477
            %"permute_28"<FLOAT,[1,192,32,431]> ⬅️ ::Transpose(%"view_175") {perm=[0, 2, 1, 3]}
    3478 |  # node_ConvTranspose_3478
            %"convolution_69"<FLOAT,[1,96,132,431]> ⬅️ ::ConvTranspose(%"permute_28", %"decoder.1.conv_tr.weight", %"decoder.1.conv_tr.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, output_padding=[0, 0], pads=[0, 0, 0, 0], strides=[4, 1]}
    3479 |  # node_Cast_3479
            %"val_2504"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3480 |  # node_Constant_3480
            %"val_2505"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3481 |  # node_Reshape_3481
            %"val_2506"<?,?> ⬅️ ::Reshape(%"val_2504", %"val_2505") {allowzero=0}
    3482 |  # node_Cast_3482
            %"val_2507"<?,?> ⬅️ ::Cast(%"val_2257") {to=7}
    3483 |  # node_Constant_3483
            %"val_2508"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3484 |  # node_Reshape_3484
            %"val_2509"<?,?> ⬅️ ::Reshape(%"val_2507", %"val_2508") {allowzero=0}
    3485 |  # node_Cast_3485
            %"val_2510"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3486 |  # node_Constant_3486
            %"val_2511"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3487 |  # node_Reshape_3487
            %"val_2512"<?,?> ⬅️ ::Reshape(%"val_2510", %"val_2511") {allowzero=0}
    3488 |  # node_Constant_3488
            %"val_2513"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3489 |  # node_Slice_3489
            %"slice_49"<FLOAT,[1,96,128,431]> ⬅️ ::Slice(%"convolution_69", %"val_2506", %"val_2509", %"val_2512", %"val_2513")
    3490 |  # node_Cast_3490
            %"val_2514"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3491 |  # node_Constant_3491
            %"val_2515"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3492 |  # node_Reshape_3492
            %"val_2516"<?,?> ⬅️ ::Reshape(%"val_2514", %"val_2515") {allowzero=0}
    3493 |  # node_Cast_3493
            %"val_2517"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3494 |  # node_Constant_3494
            %"val_2518"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3495 |  # node_Reshape_3495
            %"val_2519"<?,?> ⬅️ ::Reshape(%"val_2517", %"val_2518") {allowzero=0}
    3496 |  # node_Cast_3496
            %"val_2520"<?,?> ⬅️ ::Cast(%"val_50") {to=7}
    3497 |  # node_Constant_3497
            %"val_2521"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3498 |  # node_Reshape_3498
            %"val_2522"<?,?> ⬅️ ::Reshape(%"val_2520", %"val_2521") {allowzero=0}
    3499 |  # node_Constant_3499
            %"val_2523"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3500 |  # node_Slice_3500
            %"slice_50"<FLOAT,[1,96,128,431]> ⬅️ ::Slice(%"slice_49", %"val_2516", %"val_2519", %"val_2522", %"val_2523")
    3501 |  # node__aten_gelu_approximate_none_3501
            %"gelu_42"<FLOAT,[1,96,128,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"slice_50")
    3502 |  # node_aten_add_3502
            %"add_61"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_39", %"glu_14") {alpha=1.0}
    3503 |  # node_Conv_3503
            %"convolution_70"<FLOAT,[1,384,6891]> ⬅️ ::Conv(%"add_61", %"tdecoder.1.rewrite.weight", %"tdecoder.1.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3504 |  # node_aten_glu_3504
            %"glu_33"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_70") {dim=1}
    3505 |  # node_Conv_3505
            %"convolution_71"<FLOAT,[1,24,6891]> ⬅️ ::Conv(%"glu_33", %"tdecoder.1.dconv.layers.0.0.weight", %"tdecoder.1.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3506 |  # node_Constant_3506
            %"val_2524"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3507 |  # node_Reshape_3507
            %"val_2525"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2524") {allowzero=0}
    3508 |  # node_Constant_3508
            %"val_2526"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3509 |  # node_Concat_3509
            %"val_2527"<?,?> ⬅️ ::Concat(%"val_2526", %"val_2525", %"val_2524") {axis=0}
    3510 |  # node_Reshape_3510
            %"val_2528"<?,?> ⬅️ ::Reshape(%"convolution_71", %"val_2527") {allowzero=0}
    3511 |  # node_Constant_3511
            %"val_2529"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3512 |  # node_CastLike_3512
            %"val_2530"<?,?> ⬅️ ::CastLike(%"val_2529", %"convolution_71")
    3513 |  # node_Expand_3513
            %"val_2531"<?,?> ⬅️ ::Expand(%"val_2530", %"val_2525")
    3514 |  # node_Constant_3514
            %"val_2532"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3515 |  # node_CastLike_3515
            %"val_2533"<?,?> ⬅️ ::CastLike(%"val_2532", %"convolution_71")
    3516 |  # node_Expand_3516
            %"val_2534"<?,?> ⬅️ ::Expand(%"val_2533", %"val_2525")
    3517 |  # node_InstanceNormalization_3517
            %"val_2535"<?,?> ⬅️ ::InstanceNormalization(%"val_2528", %"val_2531", %"val_2534") {epsilon=1e-05}
    3518 |  # node_Shape_3518
            %"val_2536"<?,?> ⬅️ ::Shape(%"convolution_71") {start=0}
    3519 |  # node_Reshape_3519
            %"val_2537"<?,?> ⬅️ ::Reshape(%"val_2535", %"val_2536") {allowzero=0}
    3520 |  # node_Constant_3520
            %"val_2538"<?,?> ⬅️ ::Constant() {value_int=1}
    3521 |  # node_Sub_3521
            %"val_2539"<?,?> ⬅️ ::Sub(%"val_50", %"val_2538")
    3522 |  # node_Range_3522
            %"val_2540"<?,?> ⬅️ ::Range(%"val_2538", %"val_2539", %"val_2538")
    3523 |  # node_Unsqueeze_3523
            %"val_2541"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.0.1.weight", %"val_2540")
    3524 |  # node_Unsqueeze_3524
            %"val_2542"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.0.1.bias", %"val_2540")
    3525 |  # node_CastLike_3525
            %"val_2543"<?,?> ⬅️ ::CastLike(%"val_2541", %"val_2537")
    3526 |  # node_Mul_3526
            %"val_2544"<?,?> ⬅️ ::Mul(%"val_2537", %"val_2543")
    3527 |  # node_CastLike_3527
            %"val_2545"<?,?> ⬅️ ::CastLike(%"val_2542", %"val_2544")
    3528 |  # node_Add_3528
            %"group_norm_54"<FLOAT,[1,24,6891]> ⬅️ ::Add(%"val_2544", %"val_2545")
    3529 |  # node__aten_gelu_approximate_none_3529
            %"gelu_43"<FLOAT,[1,24,6891]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_54")
    3530 |  # node_Conv_3530
            %"convolution_72"<FLOAT,[1,384,6891]> ⬅️ ::Conv(%"gelu_43", %"tdecoder.1.dconv.layers.0.3.weight", %"tdecoder.1.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3531 |  # node_Constant_3531
            %"val_2546"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3532 |  # node_Reshape_3532
            %"val_2547"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2546") {allowzero=0}
    3533 |  # node_Constant_3533
            %"val_2548"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3534 |  # node_Concat_3534
            %"val_2549"<?,?> ⬅️ ::Concat(%"val_2548", %"val_2547", %"val_2546") {axis=0}
    3535 |  # node_Reshape_3535
            %"val_2550"<?,?> ⬅️ ::Reshape(%"convolution_72", %"val_2549") {allowzero=0}
    3536 |  # node_Constant_3536
            %"val_2551"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3537 |  # node_CastLike_3537
            %"val_2552"<?,?> ⬅️ ::CastLike(%"val_2551", %"convolution_72")
    3538 |  # node_Expand_3538
            %"val_2553"<?,?> ⬅️ ::Expand(%"val_2552", %"val_2547")
    3539 |  # node_Constant_3539
            %"val_2554"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3540 |  # node_CastLike_3540
            %"val_2555"<?,?> ⬅️ ::CastLike(%"val_2554", %"convolution_72")
    3541 |  # node_Expand_3541
            %"val_2556"<?,?> ⬅️ ::Expand(%"val_2555", %"val_2547")
    3542 |  # node_InstanceNormalization_3542
            %"val_2557"<?,?> ⬅️ ::InstanceNormalization(%"val_2550", %"val_2553", %"val_2556") {epsilon=1e-05}
    3543 |  # node_Shape_3543
            %"val_2558"<?,?> ⬅️ ::Shape(%"convolution_72") {start=0}
    3544 |  # node_Reshape_3544
            %"val_2559"<?,?> ⬅️ ::Reshape(%"val_2557", %"val_2558") {allowzero=0}
    3545 |  # node_Constant_3545
            %"val_2560"<?,?> ⬅️ ::Constant() {value_int=1}
    3546 |  # node_Sub_3546
            %"val_2561"<?,?> ⬅️ ::Sub(%"val_50", %"val_2560")
    3547 |  # node_Range_3547
            %"val_2562"<?,?> ⬅️ ::Range(%"val_2560", %"val_2561", %"val_2560")
    3548 |  # node_Unsqueeze_3548
            %"val_2563"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.0.4.weight", %"val_2562")
    3549 |  # node_Unsqueeze_3549
            %"val_2564"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.0.4.bias", %"val_2562")
    3550 |  # node_CastLike_3550
            %"val_2565"<?,?> ⬅️ ::CastLike(%"val_2563", %"val_2559")
    3551 |  # node_Mul_3551
            %"val_2566"<?,?> ⬅️ ::Mul(%"val_2559", %"val_2565")
    3552 |  # node_CastLike_3552
            %"val_2567"<?,?> ⬅️ ::CastLike(%"val_2564", %"val_2566")
    3553 |  # node_Add_3553
            %"group_norm_55"<FLOAT,[1,384,6891]> ⬅️ ::Add(%"val_2566", %"val_2567")
    3554 |  # node_aten_glu_3554
            %"glu_34"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_55") {dim=1}
    3555 |  # node_Cast_3555
            %"val_2568"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3556 |  # node_Constant_3556
            %"val_2569"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3557 |  # node_Reshape_3557
            %"val_2570"<?,?> ⬅️ ::Reshape(%"val_2568", %"val_2569") {allowzero=0}
    3558 |  # node_Cast_3558
            %"val_2571"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3559 |  # node_Constant_3559
            %"val_2572"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3560 |  # node_Reshape_3560
            %"val_2573"<?,?> ⬅️ ::Reshape(%"val_2571", %"val_2572") {allowzero=0}
    3561 |  # node_Cast_3561
            %"val_2574"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3562 |  # node_Constant_3562
            %"val_2575"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3563 |  # node_Reshape_3563
            %"val_2576"<?,?> ⬅️ ::Reshape(%"val_2574", %"val_2575") {allowzero=0}
    3564 |  # node_Constant_3564
            %"val_2577"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3565 |  # node_Slice_3565
            %"slice_51"<FLOAT,[192]> ⬅️ ::Slice(%"tdecoder.1.dconv.layers.0.6.scale", %"val_2570", %"val_2573", %"val_2576", %"val_2577")
    3566 |  # node_aten_unsqueeze_3566
            %"unsqueeze_41"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_51") {dim=1}
    3567 |  # node_Mul_3567
            %"mul_57"<FLOAT,[1,192,6891]> ⬅️ ::Mul(%"unsqueeze_41", %"glu_34")
    3568 |  # node_aten_add_3568
            %"add_62"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"glu_33", %"mul_57") {alpha=1.0}
    3569 |  # node_Conv_3569
            %"convolution_73"<FLOAT,[1,24,6891]> ⬅️ ::Conv(%"add_62", %"tdecoder.1.dconv.layers.1.0.weight", %"tdecoder.1.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    3570 |  # node_Constant_3570
            %"val_2578"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3571 |  # node_Reshape_3571
            %"val_2579"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2578") {allowzero=0}
    3572 |  # node_Constant_3572
            %"val_2580"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3573 |  # node_Concat_3573
            %"val_2581"<?,?> ⬅️ ::Concat(%"val_2580", %"val_2579", %"val_2578") {axis=0}
    3574 |  # node_Reshape_3574
            %"val_2582"<?,?> ⬅️ ::Reshape(%"convolution_73", %"val_2581") {allowzero=0}
    3575 |  # node_Constant_3575
            %"val_2583"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3576 |  # node_CastLike_3576
            %"val_2584"<?,?> ⬅️ ::CastLike(%"val_2583", %"convolution_73")
    3577 |  # node_Expand_3577
            %"val_2585"<?,?> ⬅️ ::Expand(%"val_2584", %"val_2579")
    3578 |  # node_Constant_3578
            %"val_2586"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3579 |  # node_CastLike_3579
            %"val_2587"<?,?> ⬅️ ::CastLike(%"val_2586", %"convolution_73")
    3580 |  # node_Expand_3580
            %"val_2588"<?,?> ⬅️ ::Expand(%"val_2587", %"val_2579")
    3581 |  # node_InstanceNormalization_3581
            %"val_2589"<?,?> ⬅️ ::InstanceNormalization(%"val_2582", %"val_2585", %"val_2588") {epsilon=1e-05}
    3582 |  # node_Shape_3582
            %"val_2590"<?,?> ⬅️ ::Shape(%"convolution_73") {start=0}
    3583 |  # node_Reshape_3583
            %"val_2591"<?,?> ⬅️ ::Reshape(%"val_2589", %"val_2590") {allowzero=0}
    3584 |  # node_Constant_3584
            %"val_2592"<?,?> ⬅️ ::Constant() {value_int=1}
    3585 |  # node_Sub_3585
            %"val_2593"<?,?> ⬅️ ::Sub(%"val_50", %"val_2592")
    3586 |  # node_Range_3586
            %"val_2594"<?,?> ⬅️ ::Range(%"val_2592", %"val_2593", %"val_2592")
    3587 |  # node_Unsqueeze_3587
            %"val_2595"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.1.1.weight", %"val_2594")
    3588 |  # node_Unsqueeze_3588
            %"val_2596"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.1.1.bias", %"val_2594")
    3589 |  # node_CastLike_3589
            %"val_2597"<?,?> ⬅️ ::CastLike(%"val_2595", %"val_2591")
    3590 |  # node_Mul_3590
            %"val_2598"<?,?> ⬅️ ::Mul(%"val_2591", %"val_2597")
    3591 |  # node_CastLike_3591
            %"val_2599"<?,?> ⬅️ ::CastLike(%"val_2596", %"val_2598")
    3592 |  # node_Add_3592
            %"group_norm_56"<FLOAT,[1,24,6891]> ⬅️ ::Add(%"val_2598", %"val_2599")
    3593 |  # node__aten_gelu_approximate_none_3593
            %"gelu_44"<FLOAT,[1,24,6891]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_56")
    3594 |  # node_Conv_3594
            %"convolution_74"<FLOAT,[1,384,6891]> ⬅️ ::Conv(%"gelu_44", %"tdecoder.1.dconv.layers.1.3.weight", %"tdecoder.1.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3595 |  # node_Constant_3595
            %"val_2600"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3596 |  # node_Reshape_3596
            %"val_2601"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2600") {allowzero=0}
    3597 |  # node_Constant_3597
            %"val_2602"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3598 |  # node_Concat_3598
            %"val_2603"<?,?> ⬅️ ::Concat(%"val_2602", %"val_2601", %"val_2600") {axis=0}
    3599 |  # node_Reshape_3599
            %"val_2604"<?,?> ⬅️ ::Reshape(%"convolution_74", %"val_2603") {allowzero=0}
    3600 |  # node_Constant_3600
            %"val_2605"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3601 |  # node_CastLike_3601
            %"val_2606"<?,?> ⬅️ ::CastLike(%"val_2605", %"convolution_74")
    3602 |  # node_Expand_3602
            %"val_2607"<?,?> ⬅️ ::Expand(%"val_2606", %"val_2601")
    3603 |  # node_Constant_3603
            %"val_2608"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3604 |  # node_CastLike_3604
            %"val_2609"<?,?> ⬅️ ::CastLike(%"val_2608", %"convolution_74")
    3605 |  # node_Expand_3605
            %"val_2610"<?,?> ⬅️ ::Expand(%"val_2609", %"val_2601")
    3606 |  # node_InstanceNormalization_3606
            %"val_2611"<?,?> ⬅️ ::InstanceNormalization(%"val_2604", %"val_2607", %"val_2610") {epsilon=1e-05}
    3607 |  # node_Shape_3607
            %"val_2612"<?,?> ⬅️ ::Shape(%"convolution_74") {start=0}
    3608 |  # node_Reshape_3608
            %"val_2613"<?,?> ⬅️ ::Reshape(%"val_2611", %"val_2612") {allowzero=0}
    3609 |  # node_Constant_3609
            %"val_2614"<?,?> ⬅️ ::Constant() {value_int=1}
    3610 |  # node_Sub_3610
            %"val_2615"<?,?> ⬅️ ::Sub(%"val_50", %"val_2614")
    3611 |  # node_Range_3611
            %"val_2616"<?,?> ⬅️ ::Range(%"val_2614", %"val_2615", %"val_2614")
    3612 |  # node_Unsqueeze_3612
            %"val_2617"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.1.4.weight", %"val_2616")
    3613 |  # node_Unsqueeze_3613
            %"val_2618"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.1.dconv.layers.1.4.bias", %"val_2616")
    3614 |  # node_CastLike_3614
            %"val_2619"<?,?> ⬅️ ::CastLike(%"val_2617", %"val_2613")
    3615 |  # node_Mul_3615
            %"val_2620"<?,?> ⬅️ ::Mul(%"val_2613", %"val_2619")
    3616 |  # node_CastLike_3616
            %"val_2621"<?,?> ⬅️ ::CastLike(%"val_2618", %"val_2620")
    3617 |  # node_Add_3617
            %"group_norm_57"<FLOAT,[1,384,6891]> ⬅️ ::Add(%"val_2620", %"val_2621")
    3618 |  # node_aten_glu_3618
            %"glu_35"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_57") {dim=1}
    3619 |  # node_Cast_3619
            %"val_2622"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3620 |  # node_Constant_3620
            %"val_2623"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3621 |  # node_Reshape_3621
            %"val_2624"<?,?> ⬅️ ::Reshape(%"val_2622", %"val_2623") {allowzero=0}
    3622 |  # node_Cast_3622
            %"val_2625"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3623 |  # node_Constant_3623
            %"val_2626"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3624 |  # node_Reshape_3624
            %"val_2627"<?,?> ⬅️ ::Reshape(%"val_2625", %"val_2626") {allowzero=0}
    3625 |  # node_Cast_3625
            %"val_2628"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3626 |  # node_Constant_3626
            %"val_2629"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3627 |  # node_Reshape_3627
            %"val_2630"<?,?> ⬅️ ::Reshape(%"val_2628", %"val_2629") {allowzero=0}
    3628 |  # node_Constant_3628
            %"val_2631"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3629 |  # node_Slice_3629
            %"slice_52"<FLOAT,[192]> ⬅️ ::Slice(%"tdecoder.1.dconv.layers.1.6.scale", %"val_2624", %"val_2627", %"val_2630", %"val_2631")
    3630 |  # node_aten_unsqueeze_3630
            %"unsqueeze_42"<FLOAT,[192,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_52") {dim=1}
    3631 |  # node_Mul_3631
            %"mul_58"<FLOAT,[1,192,6891]> ⬅️ ::Mul(%"unsqueeze_42", %"glu_35")
    3632 |  # node_aten_add_3632
            %"add_63"<FLOAT,[1,192,6891]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_62", %"mul_58") {alpha=1.0}
    3633 |  # node_ConvTranspose_3633
            %"convolution_75"<FLOAT,[1,96,27568]> ⬅️ ::ConvTranspose(%"add_63", %"tdecoder.1.conv_tr.weight", %"tdecoder.1.conv_tr.bias") {auto_pad=NOTSET, dilations=[1], group=1, output_padding=[0], pads=[0, 0], strides=[4]}
    3634 |  # node_Cast_3634
            %"val_2632"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3635 |  # node_Constant_3635
            %"val_2633"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3636 |  # node_Reshape_3636
            %"val_2634"<?,?> ⬅️ ::Reshape(%"val_2632", %"val_2633") {allowzero=0}
    3637 |  # node_Constant_3637
            %"val_2635"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(27565), name=None)}
    3638 |  # node_Cast_3638
            %"val_2636"<?,?> ⬅️ ::Cast(%"val_2635") {to=7}
    3639 |  # node_Constant_3639
            %"val_2637"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3640 |  # node_Reshape_3640
            %"val_2638"<?,?> ⬅️ ::Reshape(%"val_2636", %"val_2637") {allowzero=0}
    3641 |  # node_Cast_3641
            %"val_2639"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3642 |  # node_Constant_3642
            %"val_2640"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3643 |  # node_Reshape_3643
            %"val_2641"<?,?> ⬅️ ::Reshape(%"val_2639", %"val_2640") {allowzero=0}
    3644 |  # node_Constant_3644
            %"val_2642"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3645 |  # node_Slice_3645
            %"slice_53"<FLOAT,[1,96,27563]> ⬅️ ::Slice(%"convolution_75", %"val_2634", %"val_2638", %"val_2641", %"val_2642")
    3646 |  # node__aten_gelu_approximate_none_3646
            %"gelu_45"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"slice_53")
    3647 |  # node_aten_add_3647
            %"add_64"<FLOAT,[1,96,128,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_42", %"glu_11") {alpha=1.0}
    3648 |  # node_Conv_3648
            %"convolution_76"<FLOAT,[1,192,128,431]> ⬅️ ::Conv(%"add_64", %"decoder.2.rewrite.weight", %"decoder.2.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[1, 1, 1, 1], strides=[1, 1]}
    3649 |  # node_aten_glu_3649
            %"glu_36"<FLOAT,[1,96,128,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_76") {dim=1}
    3650 |  # node_Transpose_3650
            %"permute_29"<FLOAT,[1,128,96,431]> ⬅️ ::Transpose(%"glu_36") {perm=[0, 2, 1, 3]}
    3651 |  # node_Cast_3651
            %"val_2643"<?,?> ⬅️ ::Cast(%"val_392") {to=7}
    3652 |  # node_Reshape_3652
            %"view_176"<FLOAT,[128,96,431]> ⬅️ ::Reshape(%"permute_29", %"val_2643") {allowzero=0}
    3653 |  # node_Conv_3653
            %"convolution_77"<FLOAT,[128,12,431]> ⬅️ ::Conv(%"view_176", %"decoder.2.dconv.layers.0.0.weight", %"decoder.2.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3654 |  # node_Constant_3654
            %"val_2644"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3655 |  # node_Reshape_3655
            %"val_2645"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2644") {allowzero=0}
    3656 |  # node_Constant_3656
            %"val_2646"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3657 |  # node_Concat_3657
            %"val_2647"<?,?> ⬅️ ::Concat(%"val_2646", %"val_2645", %"val_2644") {axis=0}
    3658 |  # node_Reshape_3658
            %"val_2648"<?,?> ⬅️ ::Reshape(%"convolution_77", %"val_2647") {allowzero=0}
    3659 |  # node_Constant_3659
            %"val_2649"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3660 |  # node_CastLike_3660
            %"val_2650"<?,?> ⬅️ ::CastLike(%"val_2649", %"convolution_77")
    3661 |  # node_Expand_3661
            %"val_2651"<?,?> ⬅️ ::Expand(%"val_2650", %"val_2645")
    3662 |  # node_Constant_3662
            %"val_2652"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3663 |  # node_CastLike_3663
            %"val_2653"<?,?> ⬅️ ::CastLike(%"val_2652", %"convolution_77")
    3664 |  # node_Expand_3664
            %"val_2654"<?,?> ⬅️ ::Expand(%"val_2653", %"val_2645")
    3665 |  # node_InstanceNormalization_3665
            %"val_2655"<?,?> ⬅️ ::InstanceNormalization(%"val_2648", %"val_2651", %"val_2654") {epsilon=1e-05}
    3666 |  # node_Shape_3666
            %"val_2656"<?,?> ⬅️ ::Shape(%"convolution_77") {start=0}
    3667 |  # node_Reshape_3667
            %"val_2657"<?,?> ⬅️ ::Reshape(%"val_2655", %"val_2656") {allowzero=0}
    3668 |  # node_Constant_3668
            %"val_2658"<?,?> ⬅️ ::Constant() {value_int=1}
    3669 |  # node_Sub_3669
            %"val_2659"<?,?> ⬅️ ::Sub(%"val_50", %"val_2658")
    3670 |  # node_Range_3670
            %"val_2660"<?,?> ⬅️ ::Range(%"val_2658", %"val_2659", %"val_2658")
    3671 |  # node_Unsqueeze_3671
            %"val_2661"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.0.1.weight", %"val_2660")
    3672 |  # node_Unsqueeze_3672
            %"val_2662"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.0.1.bias", %"val_2660")
    3673 |  # node_CastLike_3673
            %"val_2663"<?,?> ⬅️ ::CastLike(%"val_2661", %"val_2657")
    3674 |  # node_Mul_3674
            %"val_2664"<?,?> ⬅️ ::Mul(%"val_2657", %"val_2663")
    3675 |  # node_CastLike_3675
            %"val_2665"<?,?> ⬅️ ::CastLike(%"val_2662", %"val_2664")
    3676 |  # node_Add_3676
            %"group_norm_58"<FLOAT,[128,12,431]> ⬅️ ::Add(%"val_2664", %"val_2665")
    3677 |  # node__aten_gelu_approximate_none_3677
            %"gelu_46"<FLOAT,[128,12,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_58")
    3678 |  # node_Conv_3678
            %"convolution_78"<FLOAT,[128,192,431]> ⬅️ ::Conv(%"gelu_46", %"decoder.2.dconv.layers.0.3.weight", %"decoder.2.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3679 |  # node_Constant_3679
            %"val_2666"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3680 |  # node_Reshape_3680
            %"val_2667"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2666") {allowzero=0}
    3681 |  # node_Constant_3681
            %"val_2668"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3682 |  # node_Concat_3682
            %"val_2669"<?,?> ⬅️ ::Concat(%"val_2668", %"val_2667", %"val_2666") {axis=0}
    3683 |  # node_Reshape_3683
            %"val_2670"<?,?> ⬅️ ::Reshape(%"convolution_78", %"val_2669") {allowzero=0}
    3684 |  # node_Constant_3684
            %"val_2671"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3685 |  # node_CastLike_3685
            %"val_2672"<?,?> ⬅️ ::CastLike(%"val_2671", %"convolution_78")
    3686 |  # node_Expand_3686
            %"val_2673"<?,?> ⬅️ ::Expand(%"val_2672", %"val_2667")
    3687 |  # node_Constant_3687
            %"val_2674"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3688 |  # node_CastLike_3688
            %"val_2675"<?,?> ⬅️ ::CastLike(%"val_2674", %"convolution_78")
    3689 |  # node_Expand_3689
            %"val_2676"<?,?> ⬅️ ::Expand(%"val_2675", %"val_2667")
    3690 |  # node_InstanceNormalization_3690
            %"val_2677"<?,?> ⬅️ ::InstanceNormalization(%"val_2670", %"val_2673", %"val_2676") {epsilon=1e-05}
    3691 |  # node_Shape_3691
            %"val_2678"<?,?> ⬅️ ::Shape(%"convolution_78") {start=0}
    3692 |  # node_Reshape_3692
            %"val_2679"<?,?> ⬅️ ::Reshape(%"val_2677", %"val_2678") {allowzero=0}
    3693 |  # node_Constant_3693
            %"val_2680"<?,?> ⬅️ ::Constant() {value_int=1}
    3694 |  # node_Sub_3694
            %"val_2681"<?,?> ⬅️ ::Sub(%"val_50", %"val_2680")
    3695 |  # node_Range_3695
            %"val_2682"<?,?> ⬅️ ::Range(%"val_2680", %"val_2681", %"val_2680")
    3696 |  # node_Unsqueeze_3696
            %"val_2683"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.0.4.weight", %"val_2682")
    3697 |  # node_Unsqueeze_3697
            %"val_2684"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.0.4.bias", %"val_2682")
    3698 |  # node_CastLike_3698
            %"val_2685"<?,?> ⬅️ ::CastLike(%"val_2683", %"val_2679")
    3699 |  # node_Mul_3699
            %"val_2686"<?,?> ⬅️ ::Mul(%"val_2679", %"val_2685")
    3700 |  # node_CastLike_3700
            %"val_2687"<?,?> ⬅️ ::CastLike(%"val_2684", %"val_2686")
    3701 |  # node_Add_3701
            %"group_norm_59"<FLOAT,[128,192,431]> ⬅️ ::Add(%"val_2686", %"val_2687")
    3702 |  # node_aten_glu_3702
            %"glu_37"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_59") {dim=1}
    3703 |  # node_Cast_3703
            %"val_2688"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3704 |  # node_Constant_3704
            %"val_2689"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3705 |  # node_Reshape_3705
            %"val_2690"<?,?> ⬅️ ::Reshape(%"val_2688", %"val_2689") {allowzero=0}
    3706 |  # node_Cast_3706
            %"val_2691"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3707 |  # node_Constant_3707
            %"val_2692"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3708 |  # node_Reshape_3708
            %"val_2693"<?,?> ⬅️ ::Reshape(%"val_2691", %"val_2692") {allowzero=0}
    3709 |  # node_Cast_3709
            %"val_2694"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3710 |  # node_Constant_3710
            %"val_2695"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3711 |  # node_Reshape_3711
            %"val_2696"<?,?> ⬅️ ::Reshape(%"val_2694", %"val_2695") {allowzero=0}
    3712 |  # node_Constant_3712
            %"val_2697"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3713 |  # node_Slice_3713
            %"slice_54"<FLOAT,[96]> ⬅️ ::Slice(%"decoder.2.dconv.layers.0.6.scale", %"val_2690", %"val_2693", %"val_2696", %"val_2697")
    3714 |  # node_aten_unsqueeze_3714
            %"unsqueeze_43"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_54") {dim=1}
    3715 |  # node_Mul_3715
            %"mul_59"<FLOAT,[128,96,431]> ⬅️ ::Mul(%"unsqueeze_43", %"glu_37")
    3716 |  # node_aten_add_3716
            %"add_65"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_176", %"mul_59") {alpha=1.0}
    3717 |  # node_Conv_3717
            %"convolution_79"<FLOAT,[128,12,431]> ⬅️ ::Conv(%"add_65", %"decoder.2.dconv.layers.1.0.weight", %"decoder.2.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    3718 |  # node_Constant_3718
            %"val_2698"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3719 |  # node_Reshape_3719
            %"val_2699"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2698") {allowzero=0}
    3720 |  # node_Constant_3720
            %"val_2700"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3721 |  # node_Concat_3721
            %"val_2701"<?,?> ⬅️ ::Concat(%"val_2700", %"val_2699", %"val_2698") {axis=0}
    3722 |  # node_Reshape_3722
            %"val_2702"<?,?> ⬅️ ::Reshape(%"convolution_79", %"val_2701") {allowzero=0}
    3723 |  # node_Constant_3723
            %"val_2703"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3724 |  # node_CastLike_3724
            %"val_2704"<?,?> ⬅️ ::CastLike(%"val_2703", %"convolution_79")
    3725 |  # node_Expand_3725
            %"val_2705"<?,?> ⬅️ ::Expand(%"val_2704", %"val_2699")
    3726 |  # node_Constant_3726
            %"val_2706"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3727 |  # node_CastLike_3727
            %"val_2707"<?,?> ⬅️ ::CastLike(%"val_2706", %"convolution_79")
    3728 |  # node_Expand_3728
            %"val_2708"<?,?> ⬅️ ::Expand(%"val_2707", %"val_2699")
    3729 |  # node_InstanceNormalization_3729
            %"val_2709"<?,?> ⬅️ ::InstanceNormalization(%"val_2702", %"val_2705", %"val_2708") {epsilon=1e-05}
    3730 |  # node_Shape_3730
            %"val_2710"<?,?> ⬅️ ::Shape(%"convolution_79") {start=0}
    3731 |  # node_Reshape_3731
            %"val_2711"<?,?> ⬅️ ::Reshape(%"val_2709", %"val_2710") {allowzero=0}
    3732 |  # node_Constant_3732
            %"val_2712"<?,?> ⬅️ ::Constant() {value_int=1}
    3733 |  # node_Sub_3733
            %"val_2713"<?,?> ⬅️ ::Sub(%"val_50", %"val_2712")
    3734 |  # node_Range_3734
            %"val_2714"<?,?> ⬅️ ::Range(%"val_2712", %"val_2713", %"val_2712")
    3735 |  # node_Unsqueeze_3735
            %"val_2715"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.1.1.weight", %"val_2714")
    3736 |  # node_Unsqueeze_3736
            %"val_2716"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.1.1.bias", %"val_2714")
    3737 |  # node_CastLike_3737
            %"val_2717"<?,?> ⬅️ ::CastLike(%"val_2715", %"val_2711")
    3738 |  # node_Mul_3738
            %"val_2718"<?,?> ⬅️ ::Mul(%"val_2711", %"val_2717")
    3739 |  # node_CastLike_3739
            %"val_2719"<?,?> ⬅️ ::CastLike(%"val_2716", %"val_2718")
    3740 |  # node_Add_3740
            %"group_norm_60"<FLOAT,[128,12,431]> ⬅️ ::Add(%"val_2718", %"val_2719")
    3741 |  # node__aten_gelu_approximate_none_3741
            %"gelu_47"<FLOAT,[128,12,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_60")
    3742 |  # node_Conv_3742
            %"convolution_80"<FLOAT,[128,192,431]> ⬅️ ::Conv(%"gelu_47", %"decoder.2.dconv.layers.1.3.weight", %"decoder.2.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3743 |  # node_Constant_3743
            %"val_2720"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3744 |  # node_Reshape_3744
            %"val_2721"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2720") {allowzero=0}
    3745 |  # node_Constant_3745
            %"val_2722"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3746 |  # node_Concat_3746
            %"val_2723"<?,?> ⬅️ ::Concat(%"val_2722", %"val_2721", %"val_2720") {axis=0}
    3747 |  # node_Reshape_3747
            %"val_2724"<?,?> ⬅️ ::Reshape(%"convolution_80", %"val_2723") {allowzero=0}
    3748 |  # node_Constant_3748
            %"val_2725"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3749 |  # node_CastLike_3749
            %"val_2726"<?,?> ⬅️ ::CastLike(%"val_2725", %"convolution_80")
    3750 |  # node_Expand_3750
            %"val_2727"<?,?> ⬅️ ::Expand(%"val_2726", %"val_2721")
    3751 |  # node_Constant_3751
            %"val_2728"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3752 |  # node_CastLike_3752
            %"val_2729"<?,?> ⬅️ ::CastLike(%"val_2728", %"convolution_80")
    3753 |  # node_Expand_3753
            %"val_2730"<?,?> ⬅️ ::Expand(%"val_2729", %"val_2721")
    3754 |  # node_InstanceNormalization_3754
            %"val_2731"<?,?> ⬅️ ::InstanceNormalization(%"val_2724", %"val_2727", %"val_2730") {epsilon=1e-05}
    3755 |  # node_Shape_3755
            %"val_2732"<?,?> ⬅️ ::Shape(%"convolution_80") {start=0}
    3756 |  # node_Reshape_3756
            %"val_2733"<?,?> ⬅️ ::Reshape(%"val_2731", %"val_2732") {allowzero=0}
    3757 |  # node_Constant_3757
            %"val_2734"<?,?> ⬅️ ::Constant() {value_int=1}
    3758 |  # node_Sub_3758
            %"val_2735"<?,?> ⬅️ ::Sub(%"val_50", %"val_2734")
    3759 |  # node_Range_3759
            %"val_2736"<?,?> ⬅️ ::Range(%"val_2734", %"val_2735", %"val_2734")
    3760 |  # node_Unsqueeze_3760
            %"val_2737"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.1.4.weight", %"val_2736")
    3761 |  # node_Unsqueeze_3761
            %"val_2738"<?,?> ⬅️ ::Unsqueeze(%"decoder.2.dconv.layers.1.4.bias", %"val_2736")
    3762 |  # node_CastLike_3762
            %"val_2739"<?,?> ⬅️ ::CastLike(%"val_2737", %"val_2733")
    3763 |  # node_Mul_3763
            %"val_2740"<?,?> ⬅️ ::Mul(%"val_2733", %"val_2739")
    3764 |  # node_CastLike_3764
            %"val_2741"<?,?> ⬅️ ::CastLike(%"val_2738", %"val_2740")
    3765 |  # node_Add_3765
            %"group_norm_61"<FLOAT,[128,192,431]> ⬅️ ::Add(%"val_2740", %"val_2741")
    3766 |  # node_aten_glu_3766
            %"glu_38"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_61") {dim=1}
    3767 |  # node_Cast_3767
            %"val_2742"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3768 |  # node_Constant_3768
            %"val_2743"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3769 |  # node_Reshape_3769
            %"val_2744"<?,?> ⬅️ ::Reshape(%"val_2742", %"val_2743") {allowzero=0}
    3770 |  # node_Cast_3770
            %"val_2745"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3771 |  # node_Constant_3771
            %"val_2746"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3772 |  # node_Reshape_3772
            %"val_2747"<?,?> ⬅️ ::Reshape(%"val_2745", %"val_2746") {allowzero=0}
    3773 |  # node_Cast_3773
            %"val_2748"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3774 |  # node_Constant_3774
            %"val_2749"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3775 |  # node_Reshape_3775
            %"val_2750"<?,?> ⬅️ ::Reshape(%"val_2748", %"val_2749") {allowzero=0}
    3776 |  # node_Constant_3776
            %"val_2751"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3777 |  # node_Slice_3777
            %"slice_55"<FLOAT,[96]> ⬅️ ::Slice(%"decoder.2.dconv.layers.1.6.scale", %"val_2744", %"val_2747", %"val_2750", %"val_2751")
    3778 |  # node_aten_unsqueeze_3778
            %"unsqueeze_44"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_55") {dim=1}
    3779 |  # node_Mul_3779
            %"mul_60"<FLOAT,[128,96,431]> ⬅️ ::Mul(%"unsqueeze_44", %"glu_38")
    3780 |  # node_aten_add_3780
            %"add_66"<FLOAT,[128,96,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_65", %"mul_60") {alpha=1.0}
    3781 |  # node_Cast_3781
            %"val_2752"<?,?> ⬅️ ::Cast(%"val_502") {to=7}
    3782 |  # node_Reshape_3782
            %"view_177"<FLOAT,[1,128,96,431]> ⬅️ ::Reshape(%"add_66", %"val_2752") {allowzero=0}
    3783 |  # node_Transpose_3783
            %"permute_30"<FLOAT,[1,96,128,431]> ⬅️ ::Transpose(%"view_177") {perm=[0, 2, 1, 3]}
    3784 |  # node_ConvTranspose_3784
            %"convolution_81"<FLOAT,[1,48,516,431]> ⬅️ ::ConvTranspose(%"permute_30", %"decoder.2.conv_tr.weight", %"decoder.2.conv_tr.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, output_padding=[0, 0], pads=[0, 0, 0, 0], strides=[4, 1]}
    3785 |  # node_Cast_3785
            %"val_2753"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3786 |  # node_Constant_3786
            %"val_2754"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3787 |  # node_Reshape_3787
            %"val_2755"<?,?> ⬅️ ::Reshape(%"val_2753", %"val_2754") {allowzero=0}
    3788 |  # node_Cast_3788
            %"val_2756"<?,?> ⬅️ ::Cast(%"val_2257") {to=7}
    3789 |  # node_Constant_3789
            %"val_2757"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3790 |  # node_Reshape_3790
            %"val_2758"<?,?> ⬅️ ::Reshape(%"val_2756", %"val_2757") {allowzero=0}
    3791 |  # node_Cast_3791
            %"val_2759"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3792 |  # node_Constant_3792
            %"val_2760"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3793 |  # node_Reshape_3793
            %"val_2761"<?,?> ⬅️ ::Reshape(%"val_2759", %"val_2760") {allowzero=0}
    3794 |  # node_Constant_3794
            %"val_2762"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3795 |  # node_Slice_3795
            %"slice_56"<FLOAT,[1,48,512,431]> ⬅️ ::Slice(%"convolution_81", %"val_2755", %"val_2758", %"val_2761", %"val_2762")
    3796 |  # node_Cast_3796
            %"val_2763"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3797 |  # node_Constant_3797
            %"val_2764"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3798 |  # node_Reshape_3798
            %"val_2765"<?,?> ⬅️ ::Reshape(%"val_2763", %"val_2764") {allowzero=0}
    3799 |  # node_Cast_3799
            %"val_2766"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3800 |  # node_Constant_3800
            %"val_2767"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3801 |  # node_Reshape_3801
            %"val_2768"<?,?> ⬅️ ::Reshape(%"val_2766", %"val_2767") {allowzero=0}
    3802 |  # node_Cast_3802
            %"val_2769"<?,?> ⬅️ ::Cast(%"val_50") {to=7}
    3803 |  # node_Constant_3803
            %"val_2770"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3804 |  # node_Reshape_3804
            %"val_2771"<?,?> ⬅️ ::Reshape(%"val_2769", %"val_2770") {allowzero=0}
    3805 |  # node_Constant_3805
            %"val_2772"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3806 |  # node_Slice_3806
            %"slice_57"<FLOAT,[1,48,512,431]> ⬅️ ::Slice(%"slice_56", %"val_2765", %"val_2768", %"val_2771", %"val_2772")
    3807 |  # node__aten_gelu_approximate_none_3807
            %"gelu_48"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"slice_57")
    3808 |  # node_aten_add_3808
            %"add_67"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_45", %"glu_8") {alpha=1.0}
    3809 |  # node_Conv_3809
            %"convolution_82"<FLOAT,[1,192,27563]> ⬅️ ::Conv(%"add_67", %"tdecoder.2.rewrite.weight", %"tdecoder.2.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3810 |  # node_aten_glu_3810
            %"glu_39"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_82") {dim=1}
    3811 |  # node_Conv_3811
            %"convolution_83"<FLOAT,[1,12,27563]> ⬅️ ::Conv(%"glu_39", %"tdecoder.2.dconv.layers.0.0.weight", %"tdecoder.2.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3812 |  # node_Constant_3812
            %"val_2773"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3813 |  # node_Reshape_3813
            %"val_2774"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2773") {allowzero=0}
    3814 |  # node_Constant_3814
            %"val_2775"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3815 |  # node_Concat_3815
            %"val_2776"<?,?> ⬅️ ::Concat(%"val_2775", %"val_2774", %"val_2773") {axis=0}
    3816 |  # node_Reshape_3816
            %"val_2777"<?,?> ⬅️ ::Reshape(%"convolution_83", %"val_2776") {allowzero=0}
    3817 |  # node_Constant_3817
            %"val_2778"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3818 |  # node_CastLike_3818
            %"val_2779"<?,?> ⬅️ ::CastLike(%"val_2778", %"convolution_83")
    3819 |  # node_Expand_3819
            %"val_2780"<?,?> ⬅️ ::Expand(%"val_2779", %"val_2774")
    3820 |  # node_Constant_3820
            %"val_2781"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3821 |  # node_CastLike_3821
            %"val_2782"<?,?> ⬅️ ::CastLike(%"val_2781", %"convolution_83")
    3822 |  # node_Expand_3822
            %"val_2783"<?,?> ⬅️ ::Expand(%"val_2782", %"val_2774")
    3823 |  # node_InstanceNormalization_3823
            %"val_2784"<?,?> ⬅️ ::InstanceNormalization(%"val_2777", %"val_2780", %"val_2783") {epsilon=1e-05}
    3824 |  # node_Shape_3824
            %"val_2785"<?,?> ⬅️ ::Shape(%"convolution_83") {start=0}
    3825 |  # node_Reshape_3825
            %"val_2786"<?,?> ⬅️ ::Reshape(%"val_2784", %"val_2785") {allowzero=0}
    3826 |  # node_Constant_3826
            %"val_2787"<?,?> ⬅️ ::Constant() {value_int=1}
    3827 |  # node_Sub_3827
            %"val_2788"<?,?> ⬅️ ::Sub(%"val_50", %"val_2787")
    3828 |  # node_Range_3828
            %"val_2789"<?,?> ⬅️ ::Range(%"val_2787", %"val_2788", %"val_2787")
    3829 |  # node_Unsqueeze_3829
            %"val_2790"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.0.1.weight", %"val_2789")
    3830 |  # node_Unsqueeze_3830
            %"val_2791"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.0.1.bias", %"val_2789")
    3831 |  # node_CastLike_3831
            %"val_2792"<?,?> ⬅️ ::CastLike(%"val_2790", %"val_2786")
    3832 |  # node_Mul_3832
            %"val_2793"<?,?> ⬅️ ::Mul(%"val_2786", %"val_2792")
    3833 |  # node_CastLike_3833
            %"val_2794"<?,?> ⬅️ ::CastLike(%"val_2791", %"val_2793")
    3834 |  # node_Add_3834
            %"group_norm_62"<FLOAT,[1,12,27563]> ⬅️ ::Add(%"val_2793", %"val_2794")
    3835 |  # node__aten_gelu_approximate_none_3835
            %"gelu_49"<FLOAT,[1,12,27563]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_62")
    3836 |  # node_Conv_3836
            %"convolution_84"<FLOAT,[1,192,27563]> ⬅️ ::Conv(%"gelu_49", %"tdecoder.2.dconv.layers.0.3.weight", %"tdecoder.2.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3837 |  # node_Constant_3837
            %"val_2795"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3838 |  # node_Reshape_3838
            %"val_2796"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2795") {allowzero=0}
    3839 |  # node_Constant_3839
            %"val_2797"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3840 |  # node_Concat_3840
            %"val_2798"<?,?> ⬅️ ::Concat(%"val_2797", %"val_2796", %"val_2795") {axis=0}
    3841 |  # node_Reshape_3841
            %"val_2799"<?,?> ⬅️ ::Reshape(%"convolution_84", %"val_2798") {allowzero=0}
    3842 |  # node_Constant_3842
            %"val_2800"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3843 |  # node_CastLike_3843
            %"val_2801"<?,?> ⬅️ ::CastLike(%"val_2800", %"convolution_84")
    3844 |  # node_Expand_3844
            %"val_2802"<?,?> ⬅️ ::Expand(%"val_2801", %"val_2796")
    3845 |  # node_Constant_3845
            %"val_2803"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3846 |  # node_CastLike_3846
            %"val_2804"<?,?> ⬅️ ::CastLike(%"val_2803", %"convolution_84")
    3847 |  # node_Expand_3847
            %"val_2805"<?,?> ⬅️ ::Expand(%"val_2804", %"val_2796")
    3848 |  # node_InstanceNormalization_3848
            %"val_2806"<?,?> ⬅️ ::InstanceNormalization(%"val_2799", %"val_2802", %"val_2805") {epsilon=1e-05}
    3849 |  # node_Shape_3849
            %"val_2807"<?,?> ⬅️ ::Shape(%"convolution_84") {start=0}
    3850 |  # node_Reshape_3850
            %"val_2808"<?,?> ⬅️ ::Reshape(%"val_2806", %"val_2807") {allowzero=0}
    3851 |  # node_Constant_3851
            %"val_2809"<?,?> ⬅️ ::Constant() {value_int=1}
    3852 |  # node_Sub_3852
            %"val_2810"<?,?> ⬅️ ::Sub(%"val_50", %"val_2809")
    3853 |  # node_Range_3853
            %"val_2811"<?,?> ⬅️ ::Range(%"val_2809", %"val_2810", %"val_2809")
    3854 |  # node_Unsqueeze_3854
            %"val_2812"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.0.4.weight", %"val_2811")
    3855 |  # node_Unsqueeze_3855
            %"val_2813"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.0.4.bias", %"val_2811")
    3856 |  # node_CastLike_3856
            %"val_2814"<?,?> ⬅️ ::CastLike(%"val_2812", %"val_2808")
    3857 |  # node_Mul_3857
            %"val_2815"<?,?> ⬅️ ::Mul(%"val_2808", %"val_2814")
    3858 |  # node_CastLike_3858
            %"val_2816"<?,?> ⬅️ ::CastLike(%"val_2813", %"val_2815")
    3859 |  # node_Add_3859
            %"group_norm_63"<FLOAT,[1,192,27563]> ⬅️ ::Add(%"val_2815", %"val_2816")
    3860 |  # node_aten_glu_3860
            %"glu_40"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_63") {dim=1}
    3861 |  # node_Cast_3861
            %"val_2817"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3862 |  # node_Constant_3862
            %"val_2818"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3863 |  # node_Reshape_3863
            %"val_2819"<?,?> ⬅️ ::Reshape(%"val_2817", %"val_2818") {allowzero=0}
    3864 |  # node_Cast_3864
            %"val_2820"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3865 |  # node_Constant_3865
            %"val_2821"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3866 |  # node_Reshape_3866
            %"val_2822"<?,?> ⬅️ ::Reshape(%"val_2820", %"val_2821") {allowzero=0}
    3867 |  # node_Cast_3867
            %"val_2823"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3868 |  # node_Constant_3868
            %"val_2824"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3869 |  # node_Reshape_3869
            %"val_2825"<?,?> ⬅️ ::Reshape(%"val_2823", %"val_2824") {allowzero=0}
    3870 |  # node_Constant_3870
            %"val_2826"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3871 |  # node_Slice_3871
            %"slice_58"<FLOAT,[96]> ⬅️ ::Slice(%"tdecoder.2.dconv.layers.0.6.scale", %"val_2819", %"val_2822", %"val_2825", %"val_2826")
    3872 |  # node_aten_unsqueeze_3872
            %"unsqueeze_45"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_58") {dim=1}
    3873 |  # node_Mul_3873
            %"mul_61"<FLOAT,[1,96,27563]> ⬅️ ::Mul(%"unsqueeze_45", %"glu_40")
    3874 |  # node_aten_add_3874
            %"add_68"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"glu_39", %"mul_61") {alpha=1.0}
    3875 |  # node_Conv_3875
            %"convolution_85"<FLOAT,[1,12,27563]> ⬅️ ::Conv(%"add_68", %"tdecoder.2.dconv.layers.1.0.weight", %"tdecoder.2.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    3876 |  # node_Constant_3876
            %"val_2827"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3877 |  # node_Reshape_3877
            %"val_2828"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2827") {allowzero=0}
    3878 |  # node_Constant_3878
            %"val_2829"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3879 |  # node_Concat_3879
            %"val_2830"<?,?> ⬅️ ::Concat(%"val_2829", %"val_2828", %"val_2827") {axis=0}
    3880 |  # node_Reshape_3880
            %"val_2831"<?,?> ⬅️ ::Reshape(%"convolution_85", %"val_2830") {allowzero=0}
    3881 |  # node_Constant_3881
            %"val_2832"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3882 |  # node_CastLike_3882
            %"val_2833"<?,?> ⬅️ ::CastLike(%"val_2832", %"convolution_85")
    3883 |  # node_Expand_3883
            %"val_2834"<?,?> ⬅️ ::Expand(%"val_2833", %"val_2828")
    3884 |  # node_Constant_3884
            %"val_2835"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3885 |  # node_CastLike_3885
            %"val_2836"<?,?> ⬅️ ::CastLike(%"val_2835", %"convolution_85")
    3886 |  # node_Expand_3886
            %"val_2837"<?,?> ⬅️ ::Expand(%"val_2836", %"val_2828")
    3887 |  # node_InstanceNormalization_3887
            %"val_2838"<?,?> ⬅️ ::InstanceNormalization(%"val_2831", %"val_2834", %"val_2837") {epsilon=1e-05}
    3888 |  # node_Shape_3888
            %"val_2839"<?,?> ⬅️ ::Shape(%"convolution_85") {start=0}
    3889 |  # node_Reshape_3889
            %"val_2840"<?,?> ⬅️ ::Reshape(%"val_2838", %"val_2839") {allowzero=0}
    3890 |  # node_Constant_3890
            %"val_2841"<?,?> ⬅️ ::Constant() {value_int=1}
    3891 |  # node_Sub_3891
            %"val_2842"<?,?> ⬅️ ::Sub(%"val_50", %"val_2841")
    3892 |  # node_Range_3892
            %"val_2843"<?,?> ⬅️ ::Range(%"val_2841", %"val_2842", %"val_2841")
    3893 |  # node_Unsqueeze_3893
            %"val_2844"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.1.1.weight", %"val_2843")
    3894 |  # node_Unsqueeze_3894
            %"val_2845"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.1.1.bias", %"val_2843")
    3895 |  # node_CastLike_3895
            %"val_2846"<?,?> ⬅️ ::CastLike(%"val_2844", %"val_2840")
    3896 |  # node_Mul_3896
            %"val_2847"<?,?> ⬅️ ::Mul(%"val_2840", %"val_2846")
    3897 |  # node_CastLike_3897
            %"val_2848"<?,?> ⬅️ ::CastLike(%"val_2845", %"val_2847")
    3898 |  # node_Add_3898
            %"group_norm_64"<FLOAT,[1,12,27563]> ⬅️ ::Add(%"val_2847", %"val_2848")
    3899 |  # node__aten_gelu_approximate_none_3899
            %"gelu_50"<FLOAT,[1,12,27563]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_64")
    3900 |  # node_Conv_3900
            %"convolution_86"<FLOAT,[1,192,27563]> ⬅️ ::Conv(%"gelu_50", %"tdecoder.2.dconv.layers.1.3.weight", %"tdecoder.2.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3901 |  # node_Constant_3901
            %"val_2849"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3902 |  # node_Reshape_3902
            %"val_2850"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2849") {allowzero=0}
    3903 |  # node_Constant_3903
            %"val_2851"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3904 |  # node_Concat_3904
            %"val_2852"<?,?> ⬅️ ::Concat(%"val_2851", %"val_2850", %"val_2849") {axis=0}
    3905 |  # node_Reshape_3905
            %"val_2853"<?,?> ⬅️ ::Reshape(%"convolution_86", %"val_2852") {allowzero=0}
    3906 |  # node_Constant_3906
            %"val_2854"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3907 |  # node_CastLike_3907
            %"val_2855"<?,?> ⬅️ ::CastLike(%"val_2854", %"convolution_86")
    3908 |  # node_Expand_3908
            %"val_2856"<?,?> ⬅️ ::Expand(%"val_2855", %"val_2850")
    3909 |  # node_Constant_3909
            %"val_2857"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3910 |  # node_CastLike_3910
            %"val_2858"<?,?> ⬅️ ::CastLike(%"val_2857", %"convolution_86")
    3911 |  # node_Expand_3911
            %"val_2859"<?,?> ⬅️ ::Expand(%"val_2858", %"val_2850")
    3912 |  # node_InstanceNormalization_3912
            %"val_2860"<?,?> ⬅️ ::InstanceNormalization(%"val_2853", %"val_2856", %"val_2859") {epsilon=1e-05}
    3913 |  # node_Shape_3913
            %"val_2861"<?,?> ⬅️ ::Shape(%"convolution_86") {start=0}
    3914 |  # node_Reshape_3914
            %"val_2862"<?,?> ⬅️ ::Reshape(%"val_2860", %"val_2861") {allowzero=0}
    3915 |  # node_Constant_3915
            %"val_2863"<?,?> ⬅️ ::Constant() {value_int=1}
    3916 |  # node_Sub_3916
            %"val_2864"<?,?> ⬅️ ::Sub(%"val_50", %"val_2863")
    3917 |  # node_Range_3917
            %"val_2865"<?,?> ⬅️ ::Range(%"val_2863", %"val_2864", %"val_2863")
    3918 |  # node_Unsqueeze_3918
            %"val_2866"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.1.4.weight", %"val_2865")
    3919 |  # node_Unsqueeze_3919
            %"val_2867"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.2.dconv.layers.1.4.bias", %"val_2865")
    3920 |  # node_CastLike_3920
            %"val_2868"<?,?> ⬅️ ::CastLike(%"val_2866", %"val_2862")
    3921 |  # node_Mul_3921
            %"val_2869"<?,?> ⬅️ ::Mul(%"val_2862", %"val_2868")
    3922 |  # node_CastLike_3922
            %"val_2870"<?,?> ⬅️ ::CastLike(%"val_2867", %"val_2869")
    3923 |  # node_Add_3923
            %"group_norm_65"<FLOAT,[1,192,27563]> ⬅️ ::Add(%"val_2869", %"val_2870")
    3924 |  # node_aten_glu_3924
            %"glu_41"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_65") {dim=1}
    3925 |  # node_Cast_3925
            %"val_2871"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3926 |  # node_Constant_3926
            %"val_2872"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3927 |  # node_Reshape_3927
            %"val_2873"<?,?> ⬅️ ::Reshape(%"val_2871", %"val_2872") {allowzero=0}
    3928 |  # node_Cast_3928
            %"val_2874"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    3929 |  # node_Constant_3929
            %"val_2875"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3930 |  # node_Reshape_3930
            %"val_2876"<?,?> ⬅️ ::Reshape(%"val_2874", %"val_2875") {allowzero=0}
    3931 |  # node_Cast_3931
            %"val_2877"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    3932 |  # node_Constant_3932
            %"val_2878"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3933 |  # node_Reshape_3933
            %"val_2879"<?,?> ⬅️ ::Reshape(%"val_2877", %"val_2878") {allowzero=0}
    3934 |  # node_Constant_3934
            %"val_2880"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3935 |  # node_Slice_3935
            %"slice_59"<FLOAT,[96]> ⬅️ ::Slice(%"tdecoder.2.dconv.layers.1.6.scale", %"val_2873", %"val_2876", %"val_2879", %"val_2880")
    3936 |  # node_aten_unsqueeze_3936
            %"unsqueeze_46"<FLOAT,[96,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_59") {dim=1}
    3937 |  # node_Mul_3937
            %"mul_62"<FLOAT,[1,96,27563]> ⬅️ ::Mul(%"unsqueeze_46", %"glu_41")
    3938 |  # node_aten_add_3938
            %"add_69"<FLOAT,[1,96,27563]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_68", %"mul_62") {alpha=1.0}
    3939 |  # node_ConvTranspose_3939
            %"convolution_87"<FLOAT,[1,48,110256]> ⬅️ ::ConvTranspose(%"add_69", %"tdecoder.2.conv_tr.weight", %"tdecoder.2.conv_tr.bias") {auto_pad=NOTSET, dilations=[1], group=1, output_padding=[0], pads=[0, 0], strides=[4]}
    3940 |  # node_Cast_3940
            %"val_2881"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3941 |  # node_Constant_3941
            %"val_2882"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3942 |  # node_Reshape_3942
            %"val_2883"<?,?> ⬅️ ::Reshape(%"val_2881", %"val_2882") {allowzero=0}
    3943 |  # node_Constant_3943
            %"val_2884"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(110252), name=None)}
    3944 |  # node_Cast_3944
            %"val_2885"<?,?> ⬅️ ::Cast(%"val_2884") {to=7}
    3945 |  # node_Constant_3945
            %"val_2886"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3946 |  # node_Reshape_3946
            %"val_2887"<?,?> ⬅️ ::Reshape(%"val_2885", %"val_2886") {allowzero=0}
    3947 |  # node_Cast_3947
            %"val_2888"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    3948 |  # node_Constant_3948
            %"val_2889"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3949 |  # node_Reshape_3949
            %"val_2890"<?,?> ⬅️ ::Reshape(%"val_2888", %"val_2889") {allowzero=0}
    3950 |  # node_Constant_3950
            %"val_2891"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    3951 |  # node_Slice_3951
            %"slice_60"<FLOAT,[1,48,110250]> ⬅️ ::Slice(%"convolution_87", %"val_2883", %"val_2887", %"val_2890", %"val_2891")
    3952 |  # node__aten_gelu_approximate_none_3952
            %"gelu_51"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"slice_60")
    3953 |  # node_aten_add_3953
            %"add_70"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_48", %"add_16") {alpha=1.0}
    3954 |  # node_Conv_3954
            %"convolution_88"<FLOAT,[1,96,512,431]> ⬅️ ::Conv(%"add_70", %"decoder.3.rewrite.weight", %"decoder.3.rewrite.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, pads=[1, 1, 1, 1], strides=[1, 1]}
    3955 |  # node_aten_glu_3955
            %"glu_42"<FLOAT,[1,48,512,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_88") {dim=1}
    3956 |  # node_Transpose_3956
            %"permute_31"<FLOAT,[1,512,48,431]> ⬅️ ::Transpose(%"glu_42") {perm=[0, 2, 1, 3]}
    3957 |  # node_Cast_3957
            %"val_2892"<?,?> ⬅️ ::Cast(%"val_146") {to=7}
    3958 |  # node_Reshape_3958
            %"view_178"<FLOAT,[512,48,431]> ⬅️ ::Reshape(%"permute_31", %"val_2892") {allowzero=0}
    3959 |  # node_Conv_3959
            %"convolution_89"<FLOAT,[512,6,431]> ⬅️ ::Conv(%"view_178", %"decoder.3.dconv.layers.0.0.weight", %"decoder.3.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    3960 |  # node_Constant_3960
            %"val_2893"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3961 |  # node_Reshape_3961
            %"val_2894"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2893") {allowzero=0}
    3962 |  # node_Constant_3962
            %"val_2895"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3963 |  # node_Concat_3963
            %"val_2896"<?,?> ⬅️ ::Concat(%"val_2895", %"val_2894", %"val_2893") {axis=0}
    3964 |  # node_Reshape_3964
            %"val_2897"<?,?> ⬅️ ::Reshape(%"convolution_89", %"val_2896") {allowzero=0}
    3965 |  # node_Constant_3965
            %"val_2898"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3966 |  # node_CastLike_3966
            %"val_2899"<?,?> ⬅️ ::CastLike(%"val_2898", %"convolution_89")
    3967 |  # node_Expand_3967
            %"val_2900"<?,?> ⬅️ ::Expand(%"val_2899", %"val_2894")
    3968 |  # node_Constant_3968
            %"val_2901"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3969 |  # node_CastLike_3969
            %"val_2902"<?,?> ⬅️ ::CastLike(%"val_2901", %"convolution_89")
    3970 |  # node_Expand_3970
            %"val_2903"<?,?> ⬅️ ::Expand(%"val_2902", %"val_2894")
    3971 |  # node_InstanceNormalization_3971
            %"val_2904"<?,?> ⬅️ ::InstanceNormalization(%"val_2897", %"val_2900", %"val_2903") {epsilon=1e-05}
    3972 |  # node_Shape_3972
            %"val_2905"<?,?> ⬅️ ::Shape(%"convolution_89") {start=0}
    3973 |  # node_Reshape_3973
            %"val_2906"<?,?> ⬅️ ::Reshape(%"val_2904", %"val_2905") {allowzero=0}
    3974 |  # node_Constant_3974
            %"val_2907"<?,?> ⬅️ ::Constant() {value_int=1}
    3975 |  # node_Sub_3975
            %"val_2908"<?,?> ⬅️ ::Sub(%"val_50", %"val_2907")
    3976 |  # node_Range_3976
            %"val_2909"<?,?> ⬅️ ::Range(%"val_2907", %"val_2908", %"val_2907")
    3977 |  # node_Unsqueeze_3977
            %"val_2910"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.0.1.weight", %"val_2909")
    3978 |  # node_Unsqueeze_3978
            %"val_2911"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.0.1.bias", %"val_2909")
    3979 |  # node_CastLike_3979
            %"val_2912"<?,?> ⬅️ ::CastLike(%"val_2910", %"val_2906")
    3980 |  # node_Mul_3980
            %"val_2913"<?,?> ⬅️ ::Mul(%"val_2906", %"val_2912")
    3981 |  # node_CastLike_3981
            %"val_2914"<?,?> ⬅️ ::CastLike(%"val_2911", %"val_2913")
    3982 |  # node_Add_3982
            %"group_norm_66"<FLOAT,[512,6,431]> ⬅️ ::Add(%"val_2913", %"val_2914")
    3983 |  # node__aten_gelu_approximate_none_3983
            %"gelu_52"<FLOAT,[512,6,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_66")
    3984 |  # node_Conv_3984
            %"convolution_90"<FLOAT,[512,96,431]> ⬅️ ::Conv(%"gelu_52", %"decoder.3.dconv.layers.0.3.weight", %"decoder.3.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    3985 |  # node_Constant_3985
            %"val_2915"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    3986 |  # node_Reshape_3986
            %"val_2916"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2915") {allowzero=0}
    3987 |  # node_Constant_3987
            %"val_2917"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    3988 |  # node_Concat_3988
            %"val_2918"<?,?> ⬅️ ::Concat(%"val_2917", %"val_2916", %"val_2915") {axis=0}
    3989 |  # node_Reshape_3989
            %"val_2919"<?,?> ⬅️ ::Reshape(%"convolution_90", %"val_2918") {allowzero=0}
    3990 |  # node_Constant_3990
            %"val_2920"<?,?> ⬅️ ::Constant() {value_float=1.0}
    3991 |  # node_CastLike_3991
            %"val_2921"<?,?> ⬅️ ::CastLike(%"val_2920", %"convolution_90")
    3992 |  # node_Expand_3992
            %"val_2922"<?,?> ⬅️ ::Expand(%"val_2921", %"val_2916")
    3993 |  # node_Constant_3993
            %"val_2923"<?,?> ⬅️ ::Constant() {value_float=0.0}
    3994 |  # node_CastLike_3994
            %"val_2924"<?,?> ⬅️ ::CastLike(%"val_2923", %"convolution_90")
    3995 |  # node_Expand_3995
            %"val_2925"<?,?> ⬅️ ::Expand(%"val_2924", %"val_2916")
    3996 |  # node_InstanceNormalization_3996
            %"val_2926"<?,?> ⬅️ ::InstanceNormalization(%"val_2919", %"val_2922", %"val_2925") {epsilon=1e-05}
    3997 |  # node_Shape_3997
            %"val_2927"<?,?> ⬅️ ::Shape(%"convolution_90") {start=0}
    3998 |  # node_Reshape_3998
            %"val_2928"<?,?> ⬅️ ::Reshape(%"val_2926", %"val_2927") {allowzero=0}
    3999 |  # node_Constant_3999
            %"val_2929"<?,?> ⬅️ ::Constant() {value_int=1}
    4000 |  # node_Sub_4000
            %"val_2930"<?,?> ⬅️ ::Sub(%"val_50", %"val_2929")
    4001 |  # node_Range_4001
            %"val_2931"<?,?> ⬅️ ::Range(%"val_2929", %"val_2930", %"val_2929")
    4002 |  # node_Unsqueeze_4002
            %"val_2932"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.0.4.weight", %"val_2931")
    4003 |  # node_Unsqueeze_4003
            %"val_2933"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.0.4.bias", %"val_2931")
    4004 |  # node_CastLike_4004
            %"val_2934"<?,?> ⬅️ ::CastLike(%"val_2932", %"val_2928")
    4005 |  # node_Mul_4005
            %"val_2935"<?,?> ⬅️ ::Mul(%"val_2928", %"val_2934")
    4006 |  # node_CastLike_4006
            %"val_2936"<?,?> ⬅️ ::CastLike(%"val_2933", %"val_2935")
    4007 |  # node_Add_4007
            %"group_norm_67"<FLOAT,[512,96,431]> ⬅️ ::Add(%"val_2935", %"val_2936")
    4008 |  # node_aten_glu_4008
            %"glu_43"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_67") {dim=1}
    4009 |  # node_Cast_4009
            %"val_2937"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4010 |  # node_Constant_4010
            %"val_2938"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4011 |  # node_Reshape_4011
            %"val_2939"<?,?> ⬅️ ::Reshape(%"val_2937", %"val_2938") {allowzero=0}
    4012 |  # node_Cast_4012
            %"val_2940"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4013 |  # node_Constant_4013
            %"val_2941"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4014 |  # node_Reshape_4014
            %"val_2942"<?,?> ⬅️ ::Reshape(%"val_2940", %"val_2941") {allowzero=0}
    4015 |  # node_Cast_4015
            %"val_2943"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4016 |  # node_Constant_4016
            %"val_2944"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4017 |  # node_Reshape_4017
            %"val_2945"<?,?> ⬅️ ::Reshape(%"val_2943", %"val_2944") {allowzero=0}
    4018 |  # node_Constant_4018
            %"val_2946"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4019 |  # node_Slice_4019
            %"slice_61"<FLOAT,[48]> ⬅️ ::Slice(%"decoder.3.dconv.layers.0.6.scale", %"val_2939", %"val_2942", %"val_2945", %"val_2946")
    4020 |  # node_aten_unsqueeze_4020
            %"unsqueeze_47"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_61") {dim=1}
    4021 |  # node_Mul_4021
            %"mul_63"<FLOAT,[512,48,431]> ⬅️ ::Mul(%"unsqueeze_47", %"glu_43")
    4022 |  # node_aten_add_4022
            %"add_71"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"view_178", %"mul_63") {alpha=1.0}
    4023 |  # node_Conv_4023
            %"convolution_91"<FLOAT,[512,6,431]> ⬅️ ::Conv(%"add_71", %"decoder.3.dconv.layers.1.0.weight", %"decoder.3.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    4024 |  # node_Constant_4024
            %"val_2947"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4025 |  # node_Reshape_4025
            %"val_2948"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2947") {allowzero=0}
    4026 |  # node_Constant_4026
            %"val_2949"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    4027 |  # node_Concat_4027
            %"val_2950"<?,?> ⬅️ ::Concat(%"val_2949", %"val_2948", %"val_2947") {axis=0}
    4028 |  # node_Reshape_4028
            %"val_2951"<?,?> ⬅️ ::Reshape(%"convolution_91", %"val_2950") {allowzero=0}
    4029 |  # node_Constant_4029
            %"val_2952"<?,?> ⬅️ ::Constant() {value_float=1.0}
    4030 |  # node_CastLike_4030
            %"val_2953"<?,?> ⬅️ ::CastLike(%"val_2952", %"convolution_91")
    4031 |  # node_Expand_4031
            %"val_2954"<?,?> ⬅️ ::Expand(%"val_2953", %"val_2948")
    4032 |  # node_Constant_4032
            %"val_2955"<?,?> ⬅️ ::Constant() {value_float=0.0}
    4033 |  # node_CastLike_4033
            %"val_2956"<?,?> ⬅️ ::CastLike(%"val_2955", %"convolution_91")
    4034 |  # node_Expand_4034
            %"val_2957"<?,?> ⬅️ ::Expand(%"val_2956", %"val_2948")
    4035 |  # node_InstanceNormalization_4035
            %"val_2958"<?,?> ⬅️ ::InstanceNormalization(%"val_2951", %"val_2954", %"val_2957") {epsilon=1e-05}
    4036 |  # node_Shape_4036
            %"val_2959"<?,?> ⬅️ ::Shape(%"convolution_91") {start=0}
    4037 |  # node_Reshape_4037
            %"val_2960"<?,?> ⬅️ ::Reshape(%"val_2958", %"val_2959") {allowzero=0}
    4038 |  # node_Constant_4038
            %"val_2961"<?,?> ⬅️ ::Constant() {value_int=1}
    4039 |  # node_Sub_4039
            %"val_2962"<?,?> ⬅️ ::Sub(%"val_50", %"val_2961")
    4040 |  # node_Range_4040
            %"val_2963"<?,?> ⬅️ ::Range(%"val_2961", %"val_2962", %"val_2961")
    4041 |  # node_Unsqueeze_4041
            %"val_2964"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.1.1.weight", %"val_2963")
    4042 |  # node_Unsqueeze_4042
            %"val_2965"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.1.1.bias", %"val_2963")
    4043 |  # node_CastLike_4043
            %"val_2966"<?,?> ⬅️ ::CastLike(%"val_2964", %"val_2960")
    4044 |  # node_Mul_4044
            %"val_2967"<?,?> ⬅️ ::Mul(%"val_2960", %"val_2966")
    4045 |  # node_CastLike_4045
            %"val_2968"<?,?> ⬅️ ::CastLike(%"val_2965", %"val_2967")
    4046 |  # node_Add_4046
            %"group_norm_68"<FLOAT,[512,6,431]> ⬅️ ::Add(%"val_2967", %"val_2968")
    4047 |  # node__aten_gelu_approximate_none_4047
            %"gelu_53"<FLOAT,[512,6,431]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_68")
    4048 |  # node_Conv_4048
            %"convolution_92"<FLOAT,[512,96,431]> ⬅️ ::Conv(%"gelu_53", %"decoder.3.dconv.layers.1.3.weight", %"decoder.3.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    4049 |  # node_Constant_4049
            %"val_2969"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4050 |  # node_Reshape_4050
            %"val_2970"<?,?> ⬅️ ::Reshape(%"val_35", %"val_2969") {allowzero=0}
    4051 |  # node_Constant_4051
            %"val_2971"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    4052 |  # node_Concat_4052
            %"val_2972"<?,?> ⬅️ ::Concat(%"val_2971", %"val_2970", %"val_2969") {axis=0}
    4053 |  # node_Reshape_4053
            %"val_2973"<?,?> ⬅️ ::Reshape(%"convolution_92", %"val_2972") {allowzero=0}
    4054 |  # node_Constant_4054
            %"val_2974"<?,?> ⬅️ ::Constant() {value_float=1.0}
    4055 |  # node_CastLike_4055
            %"val_2975"<?,?> ⬅️ ::CastLike(%"val_2974", %"convolution_92")
    4056 |  # node_Expand_4056
            %"val_2976"<?,?> ⬅️ ::Expand(%"val_2975", %"val_2970")
    4057 |  # node_Constant_4057
            %"val_2977"<?,?> ⬅️ ::Constant() {value_float=0.0}
    4058 |  # node_CastLike_4058
            %"val_2978"<?,?> ⬅️ ::CastLike(%"val_2977", %"convolution_92")
    4059 |  # node_Expand_4059
            %"val_2979"<?,?> ⬅️ ::Expand(%"val_2978", %"val_2970")
    4060 |  # node_InstanceNormalization_4060
            %"val_2980"<?,?> ⬅️ ::InstanceNormalization(%"val_2973", %"val_2976", %"val_2979") {epsilon=1e-05}
    4061 |  # node_Shape_4061
            %"val_2981"<?,?> ⬅️ ::Shape(%"convolution_92") {start=0}
    4062 |  # node_Reshape_4062
            %"val_2982"<?,?> ⬅️ ::Reshape(%"val_2980", %"val_2981") {allowzero=0}
    4063 |  # node_Constant_4063
            %"val_2983"<?,?> ⬅️ ::Constant() {value_int=1}
    4064 |  # node_Sub_4064
            %"val_2984"<?,?> ⬅️ ::Sub(%"val_50", %"val_2983")
    4065 |  # node_Range_4065
            %"val_2985"<?,?> ⬅️ ::Range(%"val_2983", %"val_2984", %"val_2983")
    4066 |  # node_Unsqueeze_4066
            %"val_2986"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.1.4.weight", %"val_2985")
    4067 |  # node_Unsqueeze_4067
            %"val_2987"<?,?> ⬅️ ::Unsqueeze(%"decoder.3.dconv.layers.1.4.bias", %"val_2985")
    4068 |  # node_CastLike_4068
            %"val_2988"<?,?> ⬅️ ::CastLike(%"val_2986", %"val_2982")
    4069 |  # node_Mul_4069
            %"val_2989"<?,?> ⬅️ ::Mul(%"val_2982", %"val_2988")
    4070 |  # node_CastLike_4070
            %"val_2990"<?,?> ⬅️ ::CastLike(%"val_2987", %"val_2989")
    4071 |  # node_Add_4071
            %"group_norm_69"<FLOAT,[512,96,431]> ⬅️ ::Add(%"val_2989", %"val_2990")
    4072 |  # node_aten_glu_4072
            %"glu_44"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_69") {dim=1}
    4073 |  # node_Cast_4073
            %"val_2991"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4074 |  # node_Constant_4074
            %"val_2992"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4075 |  # node_Reshape_4075
            %"val_2993"<?,?> ⬅️ ::Reshape(%"val_2991", %"val_2992") {allowzero=0}
    4076 |  # node_Cast_4076
            %"val_2994"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4077 |  # node_Constant_4077
            %"val_2995"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4078 |  # node_Reshape_4078
            %"val_2996"<?,?> ⬅️ ::Reshape(%"val_2994", %"val_2995") {allowzero=0}
    4079 |  # node_Cast_4079
            %"val_2997"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4080 |  # node_Constant_4080
            %"val_2998"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4081 |  # node_Reshape_4081
            %"val_2999"<?,?> ⬅️ ::Reshape(%"val_2997", %"val_2998") {allowzero=0}
    4082 |  # node_Constant_4082
            %"val_3000"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4083 |  # node_Slice_4083
            %"slice_62"<FLOAT,[48]> ⬅️ ::Slice(%"decoder.3.dconv.layers.1.6.scale", %"val_2993", %"val_2996", %"val_2999", %"val_3000")
    4084 |  # node_aten_unsqueeze_4084
            %"unsqueeze_48"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_62") {dim=1}
    4085 |  # node_Mul_4085
            %"mul_64"<FLOAT,[512,48,431]> ⬅️ ::Mul(%"unsqueeze_48", %"glu_44")
    4086 |  # node_aten_add_4086
            %"add_72"<FLOAT,[512,48,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_71", %"mul_64") {alpha=1.0}
    4087 |  # node_Cast_4087
            %"val_3001"<?,?> ⬅️ ::Cast(%"val_256") {to=7}
    4088 |  # node_Reshape_4088
            %"view_179"<FLOAT,[1,512,48,431]> ⬅️ ::Reshape(%"add_72", %"val_3001") {allowzero=0}
    4089 |  # node_Transpose_4089
            %"permute_32"<FLOAT,[1,48,512,431]> ⬅️ ::Transpose(%"view_179") {perm=[0, 2, 1, 3]}
    4090 |  # node_ConvTranspose_4090
            %"convolution_93"<FLOAT,[1,16,2052,431]> ⬅️ ::ConvTranspose(%"permute_32", %"decoder.3.conv_tr.weight", %"decoder.3.conv_tr.bias") {auto_pad=NOTSET, dilations=[1, 1], group=1, output_padding=[0, 0], pads=[0, 0, 0, 0], strides=[4, 1]}
    4091 |  # node_Cast_4091
            %"val_3002"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    4092 |  # node_Constant_4092
            %"val_3003"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4093 |  # node_Reshape_4093
            %"val_3004"<?,?> ⬅️ ::Reshape(%"val_3002", %"val_3003") {allowzero=0}
    4094 |  # node_Cast_4094
            %"val_3005"<?,?> ⬅️ ::Cast(%"val_2257") {to=7}
    4095 |  # node_Constant_4095
            %"val_3006"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4096 |  # node_Reshape_4096
            %"val_3007"<?,?> ⬅️ ::Reshape(%"val_3005", %"val_3006") {allowzero=0}
    4097 |  # node_Cast_4097
            %"val_3008"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    4098 |  # node_Constant_4098
            %"val_3009"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4099 |  # node_Reshape_4099
            %"val_3010"<?,?> ⬅️ ::Reshape(%"val_3008", %"val_3009") {allowzero=0}
    4100 |  # node_Constant_4100
            %"val_3011"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4101 |  # node_Slice_4101
            %"slice_63"<FLOAT,[1,16,2048,431]> ⬅️ ::Slice(%"convolution_93", %"val_3004", %"val_3007", %"val_3010", %"val_3011")
    4102 |  # node_Cast_4102
            %"val_3012"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4103 |  # node_Constant_4103
            %"val_3013"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4104 |  # node_Reshape_4104
            %"val_3014"<?,?> ⬅️ ::Reshape(%"val_3012", %"val_3013") {allowzero=0}
    4105 |  # node_Cast_4105
            %"val_3015"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4106 |  # node_Constant_4106
            %"val_3016"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4107 |  # node_Reshape_4107
            %"val_3017"<?,?> ⬅️ ::Reshape(%"val_3015", %"val_3016") {allowzero=0}
    4108 |  # node_Cast_4108
            %"val_3018"<?,?> ⬅️ ::Cast(%"val_50") {to=7}
    4109 |  # node_Constant_4109
            %"val_3019"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4110 |  # node_Reshape_4110
            %"val_3020"<?,?> ⬅️ ::Reshape(%"val_3018", %"val_3019") {allowzero=0}
    4111 |  # node_Constant_4111
            %"val_3021"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4112 |  # node_Slice_4112
            %"slice_64"<FLOAT,[1,16,2048,431]> ⬅️ ::Slice(%"slice_63", %"val_3014", %"val_3017", %"val_3020", %"val_3021")
    4113 |  # node_aten_add_4113
            %"add_73"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"gelu_51", %"glu_2") {alpha=1.0}
    4114 |  # node_Conv_4114
            %"convolution_94"<FLOAT,[1,96,110250]> ⬅️ ::Conv(%"add_73", %"tdecoder.3.rewrite.weight", %"tdecoder.3.rewrite.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    4115 |  # node_aten_glu_4115
            %"glu_45"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"convolution_94") {dim=1}
    4116 |  # node_Conv_4116
            %"convolution_95"<FLOAT,[1,6,110250]> ⬅️ ::Conv(%"glu_45", %"tdecoder.3.dconv.layers.0.0.weight", %"tdecoder.3.dconv.layers.0.0.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[1, 1], strides=[1]}
    4117 |  # node_Constant_4117
            %"val_3022"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4118 |  # node_Reshape_4118
            %"val_3023"<?,?> ⬅️ ::Reshape(%"val_35", %"val_3022") {allowzero=0}
    4119 |  # node_Constant_4119
            %"val_3024"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    4120 |  # node_Concat_4120
            %"val_3025"<?,?> ⬅️ ::Concat(%"val_3024", %"val_3023", %"val_3022") {axis=0}
    4121 |  # node_Reshape_4121
            %"val_3026"<?,?> ⬅️ ::Reshape(%"convolution_95", %"val_3025") {allowzero=0}
    4122 |  # node_Constant_4122
            %"val_3027"<?,?> ⬅️ ::Constant() {value_float=1.0}
    4123 |  # node_CastLike_4123
            %"val_3028"<?,?> ⬅️ ::CastLike(%"val_3027", %"convolution_95")
    4124 |  # node_Expand_4124
            %"val_3029"<?,?> ⬅️ ::Expand(%"val_3028", %"val_3023")
    4125 |  # node_Constant_4125
            %"val_3030"<?,?> ⬅️ ::Constant() {value_float=0.0}
    4126 |  # node_CastLike_4126
            %"val_3031"<?,?> ⬅️ ::CastLike(%"val_3030", %"convolution_95")
    4127 |  # node_Expand_4127
            %"val_3032"<?,?> ⬅️ ::Expand(%"val_3031", %"val_3023")
    4128 |  # node_InstanceNormalization_4128
            %"val_3033"<?,?> ⬅️ ::InstanceNormalization(%"val_3026", %"val_3029", %"val_3032") {epsilon=1e-05}
    4129 |  # node_Shape_4129
            %"val_3034"<?,?> ⬅️ ::Shape(%"convolution_95") {start=0}
    4130 |  # node_Reshape_4130
            %"val_3035"<?,?> ⬅️ ::Reshape(%"val_3033", %"val_3034") {allowzero=0}
    4131 |  # node_Constant_4131
            %"val_3036"<?,?> ⬅️ ::Constant() {value_int=1}
    4132 |  # node_Sub_4132
            %"val_3037"<?,?> ⬅️ ::Sub(%"val_50", %"val_3036")
    4133 |  # node_Range_4133
            %"val_3038"<?,?> ⬅️ ::Range(%"val_3036", %"val_3037", %"val_3036")
    4134 |  # node_Unsqueeze_4134
            %"val_3039"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.0.1.weight", %"val_3038")
    4135 |  # node_Unsqueeze_4135
            %"val_3040"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.0.1.bias", %"val_3038")
    4136 |  # node_CastLike_4136
            %"val_3041"<?,?> ⬅️ ::CastLike(%"val_3039", %"val_3035")
    4137 |  # node_Mul_4137
            %"val_3042"<?,?> ⬅️ ::Mul(%"val_3035", %"val_3041")
    4138 |  # node_CastLike_4138
            %"val_3043"<?,?> ⬅️ ::CastLike(%"val_3040", %"val_3042")
    4139 |  # node_Add_4139
            %"group_norm_70"<FLOAT,[1,6,110250]> ⬅️ ::Add(%"val_3042", %"val_3043")
    4140 |  # node__aten_gelu_approximate_none_4140
            %"gelu_54"<FLOAT,[1,6,110250]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_70")
    4141 |  # node_Conv_4141
            %"convolution_96"<FLOAT,[1,96,110250]> ⬅️ ::Conv(%"gelu_54", %"tdecoder.3.dconv.layers.0.3.weight", %"tdecoder.3.dconv.layers.0.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    4142 |  # node_Constant_4142
            %"val_3044"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4143 |  # node_Reshape_4143
            %"val_3045"<?,?> ⬅️ ::Reshape(%"val_35", %"val_3044") {allowzero=0}
    4144 |  # node_Constant_4144
            %"val_3046"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    4145 |  # node_Concat_4145
            %"val_3047"<?,?> ⬅️ ::Concat(%"val_3046", %"val_3045", %"val_3044") {axis=0}
    4146 |  # node_Reshape_4146
            %"val_3048"<?,?> ⬅️ ::Reshape(%"convolution_96", %"val_3047") {allowzero=0}
    4147 |  # node_Constant_4147
            %"val_3049"<?,?> ⬅️ ::Constant() {value_float=1.0}
    4148 |  # node_CastLike_4148
            %"val_3050"<?,?> ⬅️ ::CastLike(%"val_3049", %"convolution_96")
    4149 |  # node_Expand_4149
            %"val_3051"<?,?> ⬅️ ::Expand(%"val_3050", %"val_3045")
    4150 |  # node_Constant_4150
            %"val_3052"<?,?> ⬅️ ::Constant() {value_float=0.0}
    4151 |  # node_CastLike_4151
            %"val_3053"<?,?> ⬅️ ::CastLike(%"val_3052", %"convolution_96")
    4152 |  # node_Expand_4152
            %"val_3054"<?,?> ⬅️ ::Expand(%"val_3053", %"val_3045")
    4153 |  # node_InstanceNormalization_4153
            %"val_3055"<?,?> ⬅️ ::InstanceNormalization(%"val_3048", %"val_3051", %"val_3054") {epsilon=1e-05}
    4154 |  # node_Shape_4154
            %"val_3056"<?,?> ⬅️ ::Shape(%"convolution_96") {start=0}
    4155 |  # node_Reshape_4155
            %"val_3057"<?,?> ⬅️ ::Reshape(%"val_3055", %"val_3056") {allowzero=0}
    4156 |  # node_Constant_4156
            %"val_3058"<?,?> ⬅️ ::Constant() {value_int=1}
    4157 |  # node_Sub_4157
            %"val_3059"<?,?> ⬅️ ::Sub(%"val_50", %"val_3058")
    4158 |  # node_Range_4158
            %"val_3060"<?,?> ⬅️ ::Range(%"val_3058", %"val_3059", %"val_3058")
    4159 |  # node_Unsqueeze_4159
            %"val_3061"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.0.4.weight", %"val_3060")
    4160 |  # node_Unsqueeze_4160
            %"val_3062"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.0.4.bias", %"val_3060")
    4161 |  # node_CastLike_4161
            %"val_3063"<?,?> ⬅️ ::CastLike(%"val_3061", %"val_3057")
    4162 |  # node_Mul_4162
            %"val_3064"<?,?> ⬅️ ::Mul(%"val_3057", %"val_3063")
    4163 |  # node_CastLike_4163
            %"val_3065"<?,?> ⬅️ ::CastLike(%"val_3062", %"val_3064")
    4164 |  # node_Add_4164
            %"group_norm_71"<FLOAT,[1,96,110250]> ⬅️ ::Add(%"val_3064", %"val_3065")
    4165 |  # node_aten_glu_4165
            %"glu_46"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_71") {dim=1}
    4166 |  # node_Cast_4166
            %"val_3066"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4167 |  # node_Constant_4167
            %"val_3067"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4168 |  # node_Reshape_4168
            %"val_3068"<?,?> ⬅️ ::Reshape(%"val_3066", %"val_3067") {allowzero=0}
    4169 |  # node_Cast_4169
            %"val_3069"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4170 |  # node_Constant_4170
            %"val_3070"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4171 |  # node_Reshape_4171
            %"val_3071"<?,?> ⬅️ ::Reshape(%"val_3069", %"val_3070") {allowzero=0}
    4172 |  # node_Cast_4172
            %"val_3072"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4173 |  # node_Constant_4173
            %"val_3073"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4174 |  # node_Reshape_4174
            %"val_3074"<?,?> ⬅️ ::Reshape(%"val_3072", %"val_3073") {allowzero=0}
    4175 |  # node_Constant_4175
            %"val_3075"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4176 |  # node_Slice_4176
            %"slice_65"<FLOAT,[48]> ⬅️ ::Slice(%"tdecoder.3.dconv.layers.0.6.scale", %"val_3068", %"val_3071", %"val_3074", %"val_3075")
    4177 |  # node_aten_unsqueeze_4177
            %"unsqueeze_49"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_65") {dim=1}
    4178 |  # node_Mul_4178
            %"mul_65"<FLOAT,[1,48,110250]> ⬅️ ::Mul(%"unsqueeze_49", %"glu_46")
    4179 |  # node_aten_add_4179
            %"add_74"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"glu_45", %"mul_65") {alpha=1.0}
    4180 |  # node_Conv_4180
            %"convolution_97"<FLOAT,[1,6,110250]> ⬅️ ::Conv(%"add_74", %"tdecoder.3.dconv.layers.1.0.weight", %"tdecoder.3.dconv.layers.1.0.bias") {auto_pad=NOTSET, dilations=[2], group=1, pads=[2, 2], strides=[1]}
    4181 |  # node_Constant_4181
            %"val_3076"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4182 |  # node_Reshape_4182
            %"val_3077"<?,?> ⬅️ ::Reshape(%"val_35", %"val_3076") {allowzero=0}
    4183 |  # node_Constant_4183
            %"val_3078"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    4184 |  # node_Concat_4184
            %"val_3079"<?,?> ⬅️ ::Concat(%"val_3078", %"val_3077", %"val_3076") {axis=0}
    4185 |  # node_Reshape_4185
            %"val_3080"<?,?> ⬅️ ::Reshape(%"convolution_97", %"val_3079") {allowzero=0}
    4186 |  # node_Constant_4186
            %"val_3081"<?,?> ⬅️ ::Constant() {value_float=1.0}
    4187 |  # node_CastLike_4187
            %"val_3082"<?,?> ⬅️ ::CastLike(%"val_3081", %"convolution_97")
    4188 |  # node_Expand_4188
            %"val_3083"<?,?> ⬅️ ::Expand(%"val_3082", %"val_3077")
    4189 |  # node_Constant_4189
            %"val_3084"<?,?> ⬅️ ::Constant() {value_float=0.0}
    4190 |  # node_CastLike_4190
            %"val_3085"<?,?> ⬅️ ::CastLike(%"val_3084", %"convolution_97")
    4191 |  # node_Expand_4191
            %"val_3086"<?,?> ⬅️ ::Expand(%"val_3085", %"val_3077")
    4192 |  # node_InstanceNormalization_4192
            %"val_3087"<?,?> ⬅️ ::InstanceNormalization(%"val_3080", %"val_3083", %"val_3086") {epsilon=1e-05}
    4193 |  # node_Shape_4193
            %"val_3088"<?,?> ⬅️ ::Shape(%"convolution_97") {start=0}
    4194 |  # node_Reshape_4194
            %"val_3089"<?,?> ⬅️ ::Reshape(%"val_3087", %"val_3088") {allowzero=0}
    4195 |  # node_Constant_4195
            %"val_3090"<?,?> ⬅️ ::Constant() {value_int=1}
    4196 |  # node_Sub_4196
            %"val_3091"<?,?> ⬅️ ::Sub(%"val_50", %"val_3090")
    4197 |  # node_Range_4197
            %"val_3092"<?,?> ⬅️ ::Range(%"val_3090", %"val_3091", %"val_3090")
    4198 |  # node_Unsqueeze_4198
            %"val_3093"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.1.1.weight", %"val_3092")
    4199 |  # node_Unsqueeze_4199
            %"val_3094"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.1.1.bias", %"val_3092")
    4200 |  # node_CastLike_4200
            %"val_3095"<?,?> ⬅️ ::CastLike(%"val_3093", %"val_3089")
    4201 |  # node_Mul_4201
            %"val_3096"<?,?> ⬅️ ::Mul(%"val_3089", %"val_3095")
    4202 |  # node_CastLike_4202
            %"val_3097"<?,?> ⬅️ ::CastLike(%"val_3094", %"val_3096")
    4203 |  # node_Add_4203
            %"group_norm_72"<FLOAT,[1,6,110250]> ⬅️ ::Add(%"val_3096", %"val_3097")
    4204 |  # node__aten_gelu_approximate_none_4204
            %"gelu_55"<FLOAT,[1,6,110250]> ⬅️ pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(%"group_norm_72")
    4205 |  # node_Conv_4205
            %"convolution_98"<FLOAT,[1,96,110250]> ⬅️ ::Conv(%"gelu_55", %"tdecoder.3.dconv.layers.1.3.weight", %"tdecoder.3.dconv.layers.1.3.bias") {auto_pad=NOTSET, dilations=[1], group=1, pads=[0, 0], strides=[1]}
    4206 |  # node_Constant_4206
            %"val_3098"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4207 |  # node_Reshape_4207
            %"val_3099"<?,?> ⬅️ ::Reshape(%"val_35", %"val_3098") {allowzero=0}
    4208 |  # node_Constant_4208
            %"val_3100"<?,?> ⬅️ ::Constant() {value_ints=[0]}
    4209 |  # node_Concat_4209
            %"val_3101"<?,?> ⬅️ ::Concat(%"val_3100", %"val_3099", %"val_3098") {axis=0}
    4210 |  # node_Reshape_4210
            %"val_3102"<?,?> ⬅️ ::Reshape(%"convolution_98", %"val_3101") {allowzero=0}
    4211 |  # node_Constant_4211
            %"val_3103"<?,?> ⬅️ ::Constant() {value_float=1.0}
    4212 |  # node_CastLike_4212
            %"val_3104"<?,?> ⬅️ ::CastLike(%"val_3103", %"convolution_98")
    4213 |  # node_Expand_4213
            %"val_3105"<?,?> ⬅️ ::Expand(%"val_3104", %"val_3099")
    4214 |  # node_Constant_4214
            %"val_3106"<?,?> ⬅️ ::Constant() {value_float=0.0}
    4215 |  # node_CastLike_4215
            %"val_3107"<?,?> ⬅️ ::CastLike(%"val_3106", %"convolution_98")
    4216 |  # node_Expand_4216
            %"val_3108"<?,?> ⬅️ ::Expand(%"val_3107", %"val_3099")
    4217 |  # node_InstanceNormalization_4217
            %"val_3109"<?,?> ⬅️ ::InstanceNormalization(%"val_3102", %"val_3105", %"val_3108") {epsilon=1e-05}
    4218 |  # node_Shape_4218
            %"val_3110"<?,?> ⬅️ ::Shape(%"convolution_98") {start=0}
    4219 |  # node_Reshape_4219
            %"val_3111"<?,?> ⬅️ ::Reshape(%"val_3109", %"val_3110") {allowzero=0}
    4220 |  # node_Constant_4220
            %"val_3112"<?,?> ⬅️ ::Constant() {value_int=1}
    4221 |  # node_Sub_4221
            %"val_3113"<?,?> ⬅️ ::Sub(%"val_50", %"val_3112")
    4222 |  # node_Range_4222
            %"val_3114"<?,?> ⬅️ ::Range(%"val_3112", %"val_3113", %"val_3112")
    4223 |  # node_Unsqueeze_4223
            %"val_3115"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.1.4.weight", %"val_3114")
    4224 |  # node_Unsqueeze_4224
            %"val_3116"<?,?> ⬅️ ::Unsqueeze(%"tdecoder.3.dconv.layers.1.4.bias", %"val_3114")
    4225 |  # node_CastLike_4225
            %"val_3117"<?,?> ⬅️ ::CastLike(%"val_3115", %"val_3111")
    4226 |  # node_Mul_4226
            %"val_3118"<?,?> ⬅️ ::Mul(%"val_3111", %"val_3117")
    4227 |  # node_CastLike_4227
            %"val_3119"<?,?> ⬅️ ::CastLike(%"val_3116", %"val_3118")
    4228 |  # node_Add_4228
            %"group_norm_73"<FLOAT,[1,96,110250]> ⬅️ ::Add(%"val_3118", %"val_3119")
    4229 |  # node_aten_glu_4229
            %"glu_47"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_glu(%"group_norm_73") {dim=1}
    4230 |  # node_Cast_4230
            %"val_3120"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4231 |  # node_Constant_4231
            %"val_3121"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4232 |  # node_Reshape_4232
            %"val_3122"<?,?> ⬅️ ::Reshape(%"val_3120", %"val_3121") {allowzero=0}
    4233 |  # node_Cast_4233
            %"val_3123"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4234 |  # node_Constant_4234
            %"val_3124"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4235 |  # node_Reshape_4235
            %"val_3125"<?,?> ⬅️ ::Reshape(%"val_3123", %"val_3124") {allowzero=0}
    4236 |  # node_Cast_4236
            %"val_3126"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4237 |  # node_Constant_4237
            %"val_3127"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4238 |  # node_Reshape_4238
            %"val_3128"<?,?> ⬅️ ::Reshape(%"val_3126", %"val_3127") {allowzero=0}
    4239 |  # node_Constant_4239
            %"val_3129"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4240 |  # node_Slice_4240
            %"slice_66"<FLOAT,[48]> ⬅️ ::Slice(%"tdecoder.3.dconv.layers.1.6.scale", %"val_3122", %"val_3125", %"val_3128", %"val_3129")
    4241 |  # node_aten_unsqueeze_4241
            %"unsqueeze_50"<FLOAT,[48,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_66") {dim=1}
    4242 |  # node_Mul_4242
            %"mul_66"<FLOAT,[1,48,110250]> ⬅️ ::Mul(%"unsqueeze_50", %"glu_47")
    4243 |  # node_aten_add_4243
            %"add_75"<FLOAT,[1,48,110250]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"add_74", %"mul_66") {alpha=1.0}
    4244 |  # node_ConvTranspose_4244
            %"convolution_99"<FLOAT,[1,8,441004]> ⬅️ ::ConvTranspose(%"add_75", %"tdecoder.3.conv_tr.weight", %"tdecoder.3.conv_tr.bias") {auto_pad=NOTSET, dilations=[1], group=1, output_padding=[0], pads=[0, 0], strides=[4]}
    4245 |  # node_Cast_4245
            %"val_3130"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    4246 |  # node_Constant_4246
            %"val_3131"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4247 |  # node_Reshape_4247
            %"val_3132"<?,?> ⬅️ ::Reshape(%"val_3130", %"val_3131") {allowzero=0}
    4248 |  # node_Constant_4248
            %"val_3133"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(441002), name=None)}
    4249 |  # node_Cast_4249
            %"val_3134"<?,?> ⬅️ ::Cast(%"val_3133") {to=7}
    4250 |  # node_Constant_4250
            %"val_3135"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4251 |  # node_Reshape_4251
            %"val_3136"<?,?> ⬅️ ::Reshape(%"val_3134", %"val_3135") {allowzero=0}
    4252 |  # node_Cast_4252
            %"val_3137"<?,?> ⬅️ ::Cast(%"val_276") {to=7}
    4253 |  # node_Constant_4253
            %"val_3138"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4254 |  # node_Reshape_4254
            %"val_3139"<?,?> ⬅️ ::Reshape(%"val_3137", %"val_3138") {allowzero=0}
    4255 |  # node_Constant_4255
            %"val_3140"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4256 |  # node_Slice_4256
            %"slice_67"<FLOAT,[1,8,441000]> ⬅️ ::Slice(%"convolution_99", %"val_3132", %"val_3136", %"val_3139", %"val_3140")
    4257 |  # node_Constant_4257
            %"val_3141"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[5]>(array([   1,    4,   -1, 2048,  431]), name=None)}
    4258 |  # node_Cast_4258
            %"val_3142"<?,?> ⬅️ ::Cast(%"val_3141") {to=7}
    4259 |  # node_Reshape_4259
            %"view_180"<FLOAT,[1,4,4,2048,431]> ⬅️ ::Reshape(%"slice_64", %"val_3142") {allowzero=0}
    4260 |  # node_Cast_4260
            %"val_3143"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4261 |  # node_Constant_4261
            %"val_3144"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4262 |  # node_Reshape_4262
            %"val_3145"<?,?> ⬅️ ::Reshape(%"val_3143", %"val_3144") {allowzero=0}
    4263 |  # node_Cast_4263
            %"val_3146"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4264 |  # node_Constant_4264
            %"val_3147"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4265 |  # node_Reshape_4265
            %"val_3148"<?,?> ⬅️ ::Reshape(%"val_3146", %"val_3147") {allowzero=0}
    4266 |  # node_Cast_4266
            %"val_3149"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4267 |  # node_Constant_4267
            %"val_3150"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4268 |  # node_Reshape_4268
            %"val_3151"<?,?> ⬅️ ::Reshape(%"val_3149", %"val_3150") {allowzero=0}
    4269 |  # node_Constant_4269
            %"val_3152"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4270 |  # node_Slice_4270
            %"slice_68"<FLOAT,[1,1,1,1]> ⬅️ ::Slice(%"sqrt", %"val_3145", %"val_3148", %"val_3151", %"val_3152")
    4271 |  # node_aten_unsqueeze_4271
            %"unsqueeze_51"<FLOAT,[1,1,1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_68") {dim=1}
    4272 |  # node_Mul_4272
            %"mul_67"<FLOAT,[1,4,4,2048,431]> ⬅️ ::Mul(%"view_180", %"unsqueeze_51")
    4273 |  # node_Cast_4273
            %"val_3153"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4274 |  # node_Constant_4274
            %"val_3154"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4275 |  # node_Reshape_4275
            %"val_3155"<?,?> ⬅️ ::Reshape(%"val_3153", %"val_3154") {allowzero=0}
    4276 |  # node_Cast_4276
            %"val_3156"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4277 |  # node_Constant_4277
            %"val_3157"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4278 |  # node_Reshape_4278
            %"val_3158"<?,?> ⬅️ ::Reshape(%"val_3156", %"val_3157") {allowzero=0}
    4279 |  # node_Cast_4279
            %"val_3159"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4280 |  # node_Constant_4280
            %"val_3160"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4281 |  # node_Reshape_4281
            %"val_3161"<?,?> ⬅️ ::Reshape(%"val_3159", %"val_3160") {allowzero=0}
    4282 |  # node_Constant_4282
            %"val_3162"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4283 |  # node_Slice_4283
            %"slice_69"<FLOAT,[1,1,1,1]> ⬅️ ::Slice(%"mean", %"val_3155", %"val_3158", %"val_3161", %"val_3162")
    4284 |  # node_aten_unsqueeze_4284
            %"unsqueeze_52"<FLOAT,[1,1,1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_69") {dim=1}
    4285 |  # node_aten_add_4285
            %"add_76"<FLOAT,[1,4,4,2048,431]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"mul_67", %"unsqueeze_52") {alpha=1.0}
    4286 |  # node_Constant_4286
            %"val_3163"<?,?> ⬅️ ::Constant() {value=Tensor<INT64,[4]>(array([     1,      4,     -1, 441000]), name=None)}
    4287 |  # node_Cast_4287
            %"val_3164"<?,?> ⬅️ ::Cast(%"val_3163") {to=7}
    4288 |  # node_Reshape_4288
            %"view_181"<FLOAT,[1,4,2,441000]> ⬅️ ::Reshape(%"slice_67", %"val_3164") {allowzero=0}
    4289 |  # node_Cast_4289
            %"val_3165"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4290 |  # node_Constant_4290
            %"val_3166"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4291 |  # node_Reshape_4291
            %"val_3167"<?,?> ⬅️ ::Reshape(%"val_3165", %"val_3166") {allowzero=0}
    4292 |  # node_Cast_4292
            %"val_3168"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4293 |  # node_Constant_4293
            %"val_3169"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4294 |  # node_Reshape_4294
            %"val_3170"<?,?> ⬅️ ::Reshape(%"val_3168", %"val_3169") {allowzero=0}
    4295 |  # node_Cast_4295
            %"val_3171"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4296 |  # node_Constant_4296
            %"val_3172"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4297 |  # node_Reshape_4297
            %"val_3173"<?,?> ⬅️ ::Reshape(%"val_3171", %"val_3172") {allowzero=0}
    4298 |  # node_Constant_4298
            %"val_3174"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4299 |  # node_Slice_4299
            %"slice_70"<FLOAT,[1,1,1]> ⬅️ ::Slice(%"sqrt_1", %"val_3167", %"val_3170", %"val_3173", %"val_3174")
    4300 |  # node_aten_unsqueeze_4300
            %"unsqueeze_53"<FLOAT,[1,1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_70") {dim=1}
    4301 |  # node_Mul_4301
            %"mul_68"<FLOAT,[1,4,2,441000]> ⬅️ ::Mul(%"view_181", %"unsqueeze_53")
    4302 |  # node_Cast_4302
            %"val_3175"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4303 |  # node_Constant_4303
            %"val_3176"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4304 |  # node_Reshape_4304
            %"val_3177"<?,?> ⬅️ ::Reshape(%"val_3175", %"val_3176") {allowzero=0}
    4305 |  # node_Cast_4305
            %"val_3178"<?,?> ⬅️ ::Cast(%"val_84") {to=7}
    4306 |  # node_Constant_4306
            %"val_3179"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4307 |  # node_Reshape_4307
            %"val_3180"<?,?> ⬅️ ::Reshape(%"val_3178", %"val_3179") {allowzero=0}
    4308 |  # node_Cast_4308
            %"val_3181"<?,?> ⬅️ ::Cast(%"val_80") {to=7}
    4309 |  # node_Constant_4309
            %"val_3182"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
    4310 |  # node_Reshape_4310
            %"val_3183"<?,?> ⬅️ ::Reshape(%"val_3181", %"val_3182") {allowzero=0}
    4311 |  # node_Constant_4311
            %"val_3184"<?,?> ⬅️ ::Constant() {value_ints=[1]}
    4312 |  # node_Slice_4312
            %"slice_71"<FLOAT,[1,1,1]> ⬅️ ::Slice(%"mean_1", %"val_3177", %"val_3180", %"val_3183", %"val_3184")
    4313 |  # node_aten_unsqueeze_4313
            %"unsqueeze_54"<FLOAT,[1,1,1,1]> ⬅️ pkg.onnxscript.torch_lib::aten_unsqueeze(%"slice_71") {dim=1}
    4314 |  # node_aten_add_4314
            %"add_77"<FLOAT,[1,4,2,441000]> ⬅️ pkg.onnxscript.torch_lib::aten_add(%"mul_68", %"unsqueeze_54") {alpha=1.0}
    return %"add_76"<FLOAT,[1,4,4,2048,431]>, %"add_77"<FLOAT,[1,4,2,441000]>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sqrt(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Sqrt(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_sub(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Sub(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_add(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    attributes={
        alpha: FLOAT = 1.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"alpha"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='alpha')}
    1 |  # n1
         %"alpha_0"<?,?> ⬅️ ::CastLike(%"alpha", %"other")
    2 |  # n2
         %"other_1"<?,?> ⬅️ ::Mul(%"other", %"alpha_0")
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Add(%"self", %"other_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_div(
    inputs=(
        %"self"<?,?>,
        %"other"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Div(%"self", %"other")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::_aten_gelu_approximate_none(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"result"<?,?>
    ),
) {
     0 |  # n0
          %"const"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const')}
     1 |  # n1
          %"const_cast"<?,?> ⬅️ ::CastLike(%"const", %"self")
     2 |  # n2
          %"inner"<?,?> ⬅️ ::Div(%"self", %"const_cast")
     3 |  # n3
          %"erf"<?,?> ⬅️ ::Erf(%"inner")
     4 |  # n4
          %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
     5 |  # n5
          %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"erf")
     6 |  # n6
          %"inner_0"<?,?> ⬅️ ::Add(%"erf", %"int64_1_cast")
     7 |  # n7
          %"inner_1"<?,?> ⬅️ ::Mul(%"self", %"inner_0")
     8 |  # n8
          %"const_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<FLOAT,[]>(name='const_2')}
     9 |  # n9
          %"const_2_cast"<?,?> ⬅️ ::CastLike(%"const_2", %"inner_1")
    10 |  # n10
          %"result"<?,?> ⬅️ ::Mul(%"const_2_cast", %"inner_1")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_glu(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: INT = -1
    }
    outputs=(
        %"result"<?,?>
    ),
) {
    0 |  # n0
         %"first"<?,?>, %"second"<?,?> ⬅️ ::Split(%"self") {axis=RefAttr('axis', INT, ref_attr_name='dim'), num_outputs=2}
    1 |  # n1
         %"tmp"<?,?> ⬅️ ::Sigmoid(%"second")
    2 |  # n2
         %"result"<?,?> ⬅️ ::Mul(%"first", %"tmp")
    return %"result"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_unsqueeze(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
    1 |  # n1
         %"dim_0"<?,?> ⬅️ ::Cast(%"dim") {to=7}
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Unsqueeze(%"self", %"dim_0")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_expand(
    inputs=(
        %"self"<?,?>,
        %"size"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"size_0"<?,?> ⬅️ ::Cast(%"size") {to=7}
    1 |  # n1
         %"size_1"<?,?> ⬅️ ::Abs(%"size_0")
    2 |  # n2
         %"return_val"<?,?> ⬅️ ::Expand(%"self", %"size_1")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18, 'pkg.onnxscript.torch_lib.common': 1},
>
def pkg.onnxscript.torch_lib::aten_constant_pad_nd(
    inputs=(
        %"self"<?,?>,
        %"pad"<?,?>
    ),
    attributes={
        value: FLOAT = 0.0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
     0 |  # n0
          %"neg_1"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
     1 |  # n1
          %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
     2 |  # n2
          %"int64_2"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_2')}
     3 |  # n3
          %"int64_2_cast"<?,?> ⬅️ ::CastLike(%"int64_2", %"tmp")
     4 |  # n4
          %"tmp_0"<?,?> ⬅️ ::Mul(%"tmp", %"int64_2_cast")
     5 |  # n5
          %"tmp_1"<?,?> ⬅️ ::Size(%"pad")
     6 |  # n6
          %"zero_count"<?,?> ⬅️ ::Sub(%"tmp_0", %"tmp_1")
     7 |  # n7
          %"zero_count_2"<?,?> ⬅️ ::Reshape(%"zero_count", %"neg_1")
     8 |  # n8
          %"zero"<?,?> ⬅️ ::Constant() {value_ints=[0]}
     9 |  # n9
          %"zeros"<?,?> ⬅️ ::Expand(%"zero", %"zero_count_2")
    10 |  # n10
          %"torch_paddings"<?,?> ⬅️ ::Concat(%"pad", %"zeros") {axis=0}
    11 |  # n11
          %"size_d"<?,?> ⬅️ ::Size(%"torch_paddings")
    12 |  # n12
          %"steps"<?,?> ⬅️ ::Constant() {value_ints=[-2]}
    13 |  # n13
          %"ends"<?,?> ⬅️ ::Sub(%"steps", %"size_d")
    14 |  # n14
          %"odd_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"steps", %"ends", %"zero", %"steps")
    15 |  # n15
          %"ends_3"<?,?> ⬅️ ::Sub(%"neg_1", %"size_d")
    16 |  # n16
          %"even_elements"<?,?> ⬅️ ::Slice(%"torch_paddings", %"neg_1", %"ends_3", %"zero", %"steps")
    17 |  # n17
          %"onnx_padding"<?,?> ⬅️ ::Concat(%"odd_elements", %"even_elements") {axis=0}
    18 |  # n18
          %"value"<?,?> ⬅️ ::Constant() {value_float=RefAttr('value_float', FLOAT, ref_attr_name='value')}
    19 |  # n19
          %"value_cast"<?,?> ⬅️ ::CastLike(%"value", %"self")
    20 |  # n20
          %"return_val"<?,?> ⬅️ ::Pad(%"self", %"onnx_padding", %"value_cast")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_exp(
    inputs=(
        %"self"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Exp(%"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_repeat(
    inputs=(
        %"self"<?,?>,
        %"repeats"<?,?>
    ),
    outputs=(
        %"result_2"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Size(%"repeats")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Equal(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_2"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result"<?,?> ⬅️ ::Identity(%"self")
                 return %"result"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_5,
                 inputs=(

                 ),
                 outputs=(
                     %"result_1"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"repeats_0"<?,?> ⬅️ ::Cast(%"repeats") {to=7}
                 1 |  # n1
                      %"one"<?,?> ⬅️ ::Constant() {value_int=1}
                 2 |  # n2
                      %"repeats_shape"<?,?> ⬅️ ::Shape(%"repeats_0")
                 3 |  # n3
                      %"shape"<?,?> ⬅️ ::Expand(%"one", %"repeats_shape")
                 4 |  # n4
                      %"self_expanded"<?,?> ⬅️ ::Expand(%"self", %"shape")
                 5 |  # n5
                      %"result_1"<?,?> ⬅️ ::Tile(%"self_expanded", %"repeats_0")
                 return %"result_1"<?,?>
             }}
    return %"result_2"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_copy(
    inputs=(
        %"self"<?,?>,
        %"src"<?,?>
    ),
    attributes={
        non_blocking: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::CastLike(%"src", %"self")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_pow(
    inputs=(
        %"self"<?,?>,
        %"exponent"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::Pow(%"self", %"exponent")
    return %"return_val"<?,?>
},

<
    opset_imports={'pkg.onnxscript.torch_lib.common': 1, '': 18},
>
def pkg.onnxscript.torch_lib::aten_squeeze_dim(
    inputs=(
        %"self"<?,?>
    ),
    attributes={
        dim: UNDEFINED
    }
    outputs=(
        %"result_6"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ pkg.onnxscript.torch_lib.common::Rank(%"self")
    1 |  # n1
         %"int64_0"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_0')}
    2 |  # n2
         %"int64_0_cast"<?,?> ⬅️ ::CastLike(%"int64_0", %"tmp")
    3 |  # n3
         %"cond"<?,?> ⬅️ ::Greater(%"tmp", %"int64_0_cast")
    4 |  # n4
         %"result_6"<?,?> ⬅️ ::If(%"cond") {then_branch=
             graph(
                 name=thenGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_4"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"shape"<?,?> ⬅️ ::Shape(%"self")
                 1 |  # n1
                      %"dim"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                 2 |  # n2
                      %"dim_size"<?,?> ⬅️ ::Gather(%"shape", %"dim") {axis=0}
                 3 |  # n3
                      %"int64_1"<?,?> ⬅️ ::Constant() {value=TensorProtoTensor<INT64,[]>(name='int64_1')}
                 4 |  # n4
                      %"int64_1_cast"<?,?> ⬅️ ::CastLike(%"int64_1", %"dim_size")
                 5 |  # n5
                      %"cond_0"<?,?> ⬅️ ::Equal(%"dim_size", %"int64_1_cast")
                 6 |  # n6
                      %"result_4"<?,?> ⬅️ ::If(%"cond_0") {then_branch=
                          graph(
                              name=thenGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"dim_1"<?,?> ⬅️ ::Constant() {value_int=RefAttr('value_int', INT, ref_attr_name='dim')}
                              1 |  # n1
                                   %"tmp_2"<?,?> ⬅️ ::Constant() {value_ints=[-1]}
                              2 |  # n2
                                   %"dims"<?,?> ⬅️ ::Reshape(%"dim_1", %"tmp_2")
                              3 |  # n3
                                   %"result"<?,?> ⬅️ ::Squeeze(%"self", %"dims")
                              return %"result"<?,?>
                          }, else_branch=
                          graph(
                              name=elseGraph_8,
                              inputs=(

                              ),
                              outputs=(
                                  %"result_3"<?,?>
                              ),
                          ) {
                              0 |  # n0
                                   %"result_3"<?,?> ⬅️ ::Identity(%"self")
                              return %"result_3"<?,?>
                          }}
                 return %"result_4"<?,?>
             }, else_branch=
             graph(
                 name=elseGraph_4,
                 inputs=(

                 ),
                 outputs=(
                     %"result_5"<?,?>
                 ),
             ) {
                 0 |  # n0
                      %"result_5"<?,?> ⬅️ ::Identity(%"self")
                 return %"result_5"<?,?>
             }}
    return %"result_6"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_split_with_sizes(
    inputs=(
        %"self"<?,?>,
        %"split_sizes"<?,?>
    ),
    attributes={
        dim: INT = 0
    }
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SplitToSequence(%"self", %"split_sizes") {axis=RefAttr('axis', INT, ref_attr_name='dim')}
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib::aten_getitem(
    inputs=(
        %"self"<?,?>,
        %"i"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"return_val"<?,?> ⬅️ ::SequenceAt(%"self", %"i")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::Rank(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"return_val"<?,?> ⬅️ ::Size(%"tmp")
    return %"return_val"<?,?>
},

<
    opset_imports={'': 18},
>
def pkg.onnxscript.torch_lib.common::IsScalar(
    inputs=(
        %"input"<?,?>
    ),
    outputs=(
        %"return_val"<?,?>
    ),
) {
    0 |  # n0
         %"tmp"<?,?> ⬅️ ::Shape(%"input")
    1 |  # n1
         %"tmp_0"<?,?> ⬅️ ::Size(%"tmp")
    2 |  # n2
         %"tmp_1"<?,?> ⬅️ ::Constant() {value_int=0}
    3 |  # n3
         %"return_val"<?,?> ⬅️ ::Equal(%"tmp_0", %"tmp_1")
    return %"return_val"<?,?>
}
```

## Analysis

PyTorch ONNX Conversion Analysis

## Model Information

The model has 41984456 parameters and 0 buffers (non-trainable parameters).
Number of parameters per dtype:
```python
defaultdict(<class 'int'>, {torch.float32: 41984456})
```
Number of buffers per dtype:
```python
defaultdict(<class 'int'>, {})
```

Inputs:
- `mix`: `TensorMetadata(shape=torch.Size([1, 2, 441000]), dtype=torch.float32, requires_grad=False, stride=(882000, 441000, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`
- `spec`: `TensorMetadata(shape=torch.Size([1, 2, 2048, 431]), dtype=torch.complex64, requires_grad=False, stride=(2*s3, s3, 1, s1), memory_format=None, is_quantized=False, qparams={})`

Outputs:
- `add_76`: `TensorMetadata(shape=torch.Size([1, 4, 4, 2048, 431]), dtype=torch.float32, requires_grad=False, stride=(14123008, 3530752, 882688, 431, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`
- `add_77`: `TensorMetadata(shape=torch.Size([1, 4, 2, 441000]), dtype=torch.float32, requires_grad=False, stride=(3528000, 882000, 441000, 1), memory_format=torch.contiguous_format, is_quantized=False, qparams={})`

The FX graph has 1702 nodes in total. Number of FX nodes per op:
- `placeholder`: 535
- `call_function`: 1166
- `output`: 1


Of the call_function nodes, the counts of operators used are:

- `aten.view.default`: 182
- `aten.convolution.default`: 100
- `aten.transpose.int`: 88
- `aten.group_norm.default`: 74
- `aten.slice.Tensor`: 71
- `aten.add.Tensor`: 68
- `aten.mul.Tensor`: 63
- `aten.gelu.default`: 56
- `aten.unsqueeze.default`: 55
- `<built-in function getitem>`: 52
- `aten.glu.default`: 48
- `aten.t.default`: 45
- `aten.clone.default`: 44
- `aten.addmm.default`: 44
- `aten.permute.default`: 33
- `aten.native_layer_norm.default`: 26
- `aten.select.int`: 26
- `aten.slice_scatter.default`: 12
- `aten.squeeze.dim`: 10
- `aten._scaled_dot_product_flash_attention_for_cpu.default`: 10
- `aten.split_with_sizes.default`: 8
- `aten._unsafe_view.default`: 4
- `aten.div.Tensor`: 4
- `aten.repeat.default`: 4
- `aten.copy.default`: 4
- `aten.arange.default`: 3
- `aten.constant_pad_nd.default`: 3
- `aten.sin.default`: 3
- `aten.cos.default`: 3
- `aten.mean.dim`: 2
- `aten.var.correction`: 2
- `aten.sqrt.default`: 2
- `aten.sub.Tensor`: 2
- `aten.scalar_tensor.default`: 2
- `aten.arange.start`: 2
- `prims.convert_element_type.default`: 2
- `aten.view_as_real.default`: 1
- `aten.embedding.default`: 1
- `aten.expand.default`: 1
- `aten.zeros.default`: 1
- `aten.arange.start_step`: 1
- `aten.exp.default`: 1
- `aten._to_copy.default`: 1
- `aten.pow.Scalar`: 1
- `aten.cat.default`: 1

## ONNX Conversion Information

All operators in the model have registered ONNX decompositions.

## Decomposition comparison

Ops exist only in the ExportedProgram before decomposition: `['aten.conv1d.default', 'aten.conv2d.default', 'aten.conv_transpose1d.default', 'aten.conv_transpose2d.input', 'aten.dropout.default', 'aten.layer_norm.default', 'aten.linear.default', 'aten.pad.default', 'aten.scaled_dot_product_attention.default', 'aten.std.dim']`

Ops exist only in the ExportedProgram after decomposition: `['aten._scaled_dot_product_flash_attention_for_cpu.default', 'aten.addmm.default', 'aten.constant_pad_nd.default', 'aten.convolution.default', 'aten.native_layer_norm.default', 'aten.scalar_tensor.default', 'aten.sqrt.default', 'aten.var.correction', 'prims.convert_element_type.default']`


## Verification results

`add_76`: `max_abs_diff=2.128993e+00`, `max_rel_diff=3.286579e+05`, `abs_diff_hist=torch.return_types.histogram(
hist=tensor([8.9600e+02, 7.7020e+03, 7.8810e+04, 7.8380e+05, 5.1025e+06, 6.3164e+06,
        1.8287e+06, 4.2360e+03]),
bin_edges=tensor([0.0000e+00, 1.0000e-06, 1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02,
        1.0000e-01, 1.0000e+00, 1.0000e+01]))`, `rel_diff_hist=torch.return_types.histogram(
hist=tensor([2.6000e+01, 2.1000e+02, 2.1810e+03, 2.1820e+04, 2.1967e+05, 2.0787e+06,
        8.9185e+06, 2.5950e+06]),
bin_edges=tensor([0.0000e+00, 1.0000e-06, 1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02,
        1.0000e-01, 1.0000e+00, 1.0000e+01]))`
`add_77`: `max_abs_diff=1.667315e-02`, `max_rel_diff=5.604292e+03`, `abs_diff_hist=torch.return_types.histogram(
hist=tensor([2.7778e+04, 2.4731e+05, 1.4571e+06, 1.6223e+06, 1.7346e+05, 1.1000e+01,
        0.0000e+00, 0.0000e+00]),
bin_edges=tensor([0.0000e+00, 1.0000e-06, 1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02,
        1.0000e-01, 1.0000e+00, 1.0000e+01]))`, `rel_diff_hist=torch.return_types.histogram(
hist=tensor([6.7800e+02, 6.0070e+03, 6.0851e+04, 5.7854e+05, 1.8395e+06, 8.6203e+05,
        1.6205e+05, 1.6560e+04]),
bin_edges=tensor([0.0000e+00, 1.0000e-06, 1.0000e-05, 1.0000e-04, 1.0000e-03, 1.0000e-02,
        1.0000e-01, 1.0000e+00, 1.0000e+01]))`
